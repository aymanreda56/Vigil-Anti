# -*- coding: utf-8 -*-
"""eda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ep0fYHywtGEx1p7uZEux7rWwsl4-Dhke
"""

import os
import json
import pandas as pd
import numpy as np
import statistics
from helpers import *
import re
import pickle

json_file_path = r"D:\ClassWork\anti_virus\Vigil-Anti\EXE_Dataset\ember2018"
dataset_filenames= ['train_features_0.jsonl', 'train_features_1.jsonl', 'train_features_2.jsonl', 'train_features_3.jsonl', 'train_features_4.jsonl', 'train_features_5.jsonl']

for dataset_index, dataset_name in enumerate(dataset_filenames):

    json_file = os.path.join(json_file_path, dataset_name)

    with open(json_file, 'r') as f:
        json_ds_list = list(f)

    DataSet = []
    for i,ds in enumerate(json_ds_list):
        # if( i > 15000):
        #    break
        DataSet.append(json.loads(ds))

    # to free some of the precious memory
    del json_ds_list



    # Saving the most common section names

    with open(os.path.join(os.getcwd(), 'assets', 'common_section_names.txt'), 'r') as f:
        Common_section_names = f.readlines()

    Common_section_names = [re.sub(r'\n', '', i) for i in Common_section_names]



    new_Dataset = []

    for simple_ds in tqdm(DataSet, desc='cleansing the dataset'):
        try:
            # add reduced features of byteentropy distribution
            simple_ds.update(Interpret_Histogram(simple_ds['byteentropy'], 'byteentropy'))

            # add reduced features of byte histogram distribution
            simple_ds.update(Interpret_Histogram(simple_ds['histogram'], 'bytehistogram'))

            # reduce strings field
            simple_ds = extract_subfields_from_fields(simple_ds, 'strings', normalize_names=True, delete_field=True)

            # flatten the strings printables distribution field
            simple_ds = flatten_strings_printable_distribution(simple_ds, delete_field=True)

            # reduce general field
            simple_ds = extract_subfields_from_fields(simple_ds, 'general', normalize_names=True, delete_field=True)

            # reduce header field
            simple_ds = extract_subfields_from_fields(simple_ds, 'header', normalize_names=True, delete_field=True)
            simple_ds = extract_subfields_from_fields(simple_ds, 'header_optional', normalize_names=False, delete_field=True)
            simple_ds = extract_subfields_from_fields(simple_ds, 'header_coff', normalize_names=False, delete_field=True)


            # handle data directories field
            simple_ds = handle_data_directories_field(simple_ds)


            # handle sections fields
            simple_ds = handle_section_names(simple_ds, Common_section_names, delete_field=True)

            # handle imports fields
            simple_ds = handle_DLL_imports(simple_ds, delete_field=False)

            # Remove the useless columns for now (they are not entirely useless but they will make the training process very complex for me :(( )
            useless_columns = ['sha256'
                ,'md5'
                ,'appeared'
                ,'avclass'
                ,'histogram'
                ,'byteentropy'
                ,'imports'
                ,'exports'
                ,'dll_characteristics'
                ,'characteristics']

            for useless_col in useless_columns:
                del simple_ds[useless_col]

            new_Dataset.append(simple_ds)
        except:
            continue


    # Finally, free the original dataset from our precious memory
    del DataSet

    all_keys = set().union(*new_Dataset)

    merged_dict = {}

    for d in tqdm(new_Dataset, desc="contructing a very big Dictionary"):
        for key in all_keys:
            if key in d.keys():
                if key in merged_dict:
                    merged_dict[key].append(d[key])
                else:
                    merged_dict[key] = [d[key]]
            else:
                if key not in merged_dict:
                    merged_dict[key] = []

    df = pd.DataFrame().from_dict(merged_dict, orient='index').transpose()


    df.fillna(0, inplace=True)
    df.to_csv(f'Dataset_{dataset_index}.csv')
    df.describe()

    with open(os.path.join(os.getcwd(), 'assets', 'suspicious_imports.txt'), 'r') as f:
        sus_imports = f.readlines()
    sus_imports = [re.sub(r'\n', '', i) for i in sus_imports]

    boolean_columns = sus_imports + []
    categorical_columns = ["subsystem", "magic", "machine"]


    for col in df.columns:
        if col in boolean_columns:
            df[col] = df[col].astype(bool)
            df[col].fillna(False)
            continue

        if col in categorical_columns:
            df[col].replace(0, 'UNKNOWN', inplace=True)
            continue
        df[col].fillna(0)
        df[col] = df[col].astype(np.int64)
        df[col].fillna(0)

    for col in df.columns:
        print(f"{col}:        {df[col].dtype}")


    df.to_csv(f'Dataset_{dataset_index}.csv', index=False)




Dataset_CSVs = ['Dataset_1.csv', 'Dataset_2.csv', 'Dataset_3.csv', 'Dataset_4.csv', 'Dataset_5.csv']
BigDataset_DF = pd.read_csv('Dataset_0.csv')
for Dataset_name in Dataset_CSVs:
    BigDataset_DF = pd.concat([BigDataset_DF, pd.read_csv(Dataset_name)], axis=0, join='outer', ignore_index=False)

BigDataset_DF.to_csv('Dataset_Big', index=False)







'''
# Save our feature list
feature_columns = list(df.columns)
feature_columns.pop(feature_columns.index('label'))
print(f"total number of Features: {len(feature_columns)}")
with open(os.path.join(os.getcwd(), 'assets', 'features.pkl'), 'wb') as f:
    pickle.dump(feature_columns, f)

"""# Remove the -1 tuples!"""

df = df[df['label'] != -1]

"""# Encoding Categorical Columns"""

from sklearn.preprocessing import LabelEncoder

df_train = df.copy()
array_of_Label_Encoders = []
for col in categorical_columns:
    new_LE = LabelEncoder().fit(df_train[col])
    df_train[col] = new_LE.transform(df_train[col])
    array_of_Label_Encoders.append(new_LE)

with open(os.path.join(os.getcwd(), 'models', 'enc.pkl'), 'wb') as f:
    pickle.dump(array_of_Label_Encoders, f)

"""# Let's make our classifier"""

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


df_train_1 = df_train.copy()

feature_columns = list(df_train_1.columns)
feature_columns.pop(feature_columns.index("label"))
if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):
    feature_columns.pop(0)


x_train, x_test, y_train, y_test = train_test_split(df_train_1[feature_columns], df_train_1['label'], test_size=0.3, shuffle=True)

svm_model = SVC(kernel='poly', degree= 3, verbose=True).fit(x_train, y_train)

y_pred = svm_model.predict(x_test)
print(classification_report(y_test, y_pred))


with open(os.path.join(os.getcwd(), 'models', 'svm.pkl'), 'wb') as f:
    pickle.dump(svm_model, f)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

df_train_2 = df.copy()

for col in categorical_columns:
    df_train_2[col] = LabelEncoder().fit_transform(df_train_2[col])

feature_columns = list(df_train_2.columns)
feature_columns.pop(feature_columns.index("label"))

if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):
    feature_columns.pop(0)


x_train, x_test, y_train, y_test = train_test_split(df_train_2[feature_columns], df_train_2['label'], test_size=0.3, shuffle=True)

rf_model = RandomForestClassifier().fit(x_train, y_train)

y_pred = rf_model.predict(x_test)
print(classification_report(y_test, y_pred))


with open(os.path.join(os.getcwd(), 'models', 'rf.pkl'), 'wb') as f:
    pickle.dump(rf_model, f)



"""# Pytorch's Neural Network"""

import torch
import torch.nn as nn


class MyNet(nn.Module):
    def __init__(self, num_features = 120):
        super(MyNet, self).__init__()

        self.batch_norm1 = nn.BatchNorm1d(num_features)
        self.dense1 = nn.Linear(num_features, 512)
        self.batch_norm2 = nn.BatchNorm1d(512)
        self.dense2 = nn.Linear(512, 128)
        self.batch_norm3 = nn.BatchNorm1d(128)
        self.dense4 = nn.Linear(128, 128)
        self.batch_norm4 = nn.BatchNorm1d(128)
        self.dense5 = nn.Linear(128, 8)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.batch_norm1(x.float())
        x = torch.tanh(self.dense1(x))
        x = self.batch_norm2(x.float())
        x = torch.tanh(self.dense2(x))
        x = self.batch_norm3(x.float())
        #x = torch.tanh(self.dense3(x))
        x = torch.tanh(self.dense4(x))
        x = self.batch_norm4(x.float())
        x = torch.tanh(self.dense5(x))
        x = self.softmax(x)
        return x

# Create an instance of the network
net = MyNet()

# Print the network architecture
print(net)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd

# Define a custom dataset class
class MyDataset(Dataset):
    def __init__(self, dataframe_train, dataframe_labels):
        self.data = dataframe_train
        self.labels = dataframe_labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        x = self.data[index, :]
        y = self.labels[index]
        return x, y

# Load the data from a pandas DataFrame
#df = pd.read_csv('Dataset_1.csv')


df_train_3 = df.copy()

for i, col in enumerate(categorical_columns):
    df_train_3[col] = LabelEncoder().fit_transform(df_train_3[col]).astype(int)

feature_columns = list(df_train_3.columns)
feature_columns.pop(feature_columns.index("label"))

if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):
    feature_columns.pop(0)

num_features = len(feature_columns)


x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)


# Split the data into features and labels
X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))
#X = torch.tensor(torch.from_numpy(np.asarray(x_train, dtype=np.int64)), dtype=torch.int64)
#y = torch.tensor(torch.from_numpy(np.asarray(y_train, dtype=bool)), dtype=torch.int64)
y = torch.tensor(torch.from_numpy(np.asarray(y_train, dtype=bool)), dtype= torch.int64)


# Split the data into features and labels
X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))
y_val = torch.tensor(torch.from_numpy(np.asarray(y_val, dtype=bool)), dtype=torch.int64)


# Create instances of the dataset and data loader
dataset = MyDataset(X, y)
dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)


val_dataset = MyDataset(X_val, y_val)
val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)


# Create an instance of the network
net = MyNet(num_features)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.0001)

# Training loop
for epoch in range(130):  # Replace 10 with the desired number of epochs
    running_loss = 0.0
    running_corrects = 0  # Counter for correct predictions

    for inputs, labels in dataloader:
        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = net(inputs)

        # Compute the loss
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Accumulate the loss
        running_loss += loss.item()

        # Calculate the predictions and accuracy
        _, preds = torch.max(outputs, 1)  # Get the predicted labels
        corrects = torch.sum(preds == labels)  # Count the number of correct predictions
        running_corrects += corrects.item()

    # Calculate the running accuracy on the training set
    train_accuracy = running_corrects / len(dataset)

    # Calculate the validation accuracy
    val_corrects = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_dataloader:
            outputs = net(inputs)
            _, preds = torch.max(outputs, 1)
            val_corrects += torch.sum(preds == labels).item()
            val_total += labels.size(0)
    val_accuracy = val_corrects / val_total

    # Print the average loss and accuracy for the epoch
    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')

# Save the trained model
torch.save(net.state_dict(), 'trained_model.pt')

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# Define the testing function
def test_model(model, dataloader):
    model.eval()  # Set the model to evaluation mode
    device = next(model.parameters()).device  # Get the device of the model

    all_labels = []
    all_preds = []

    with torch.no_grad():
        for inputs, labels in dataloader:

            # Forward pass
            outputs = model(inputs)
            # Get the predicted labels
            #preds = torch.round(torch.sigmoid(outputs))
            _, preds = torch.max(outputs, 1)  # Get the predicted labels

            # Collect the labels and predictions
            all_labels += list(labels.numpy().reshape((-1,1)))
            all_preds += list(preds.numpy().reshape((-1,1)))

    return np.asarray(all_labels), np.asarray(all_preds)

# Create an instance of the network
net = MyNet(num_features)

# Load the trained weights
net.load_state_dict(torch.load('trained_model.pt'))

# Set the model to evaluation mode
net.eval()

# Create the testing dataset and data loader
X = torch.from_numpy(np.asarray(x_test, dtype=np.int64))
y = torch.from_numpy(np.asarray(y_test, dtype=np.int64))
test_dataset = MyDataset(X, y)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Test the model
true_labels, predicted_labels = test_model(net, test_dataloader)

# Import the necessary libraries for classification report
from sklearn.metrics import classification_report

# Print the classification report
target_names = ['class_0', 'class_1']  # Replace with appropriate class names
print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))

"""# End-to-End Testing"""

from subprocess import run
Good_file = r"D:\win32diskimager-1.0.0-install.exe"
Bad_file = r"D:\hackSF\filtered_dataset\Win32_EXE\186"
end_script = r"D:\ClassWork\anti_virus\Vigil-Anti\Source\EXEs\run.py"
model_path= r"D:\ClassWork\anti_virus\Vigil-Anti\Source\EXEs\models\rf.pkl"

result1 = run(['python', end_script, Good_file, model_path], capture_output=True)
resutl2 = run(['python', end_script, Bad_file, model_path], capture_output=True)
print(result1.stdout)
print(resutl2.stdout)

'''