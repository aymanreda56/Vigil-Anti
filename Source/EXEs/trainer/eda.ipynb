{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from helpers import *\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sha256': '2ef9a92ee6c955364564b0df75ee3753473014b2ba162b9df90afe6df9dbb256', 'md5': '7e39aeea7bc21d16b8652516a150b282', 'appeared': '2018-01', 'label': 1, 'avclass': 'sivis', 'histogram': [60782, 5895, 2020, 1487, 2075, 1367, 1145, 856, 2037, 725, 2027, 716, 1418, 903, 672, 1014, 1605, 652, 702, 691, 1048, 927, 641, 599, 795, 636, 598, 598, 677, 629, 597, 571, 8564, 738, 921, 600, 1253, 835, 645, 565, 1015, 919, 958, 868, 917, 784, 1435, 1307, 1470, 1081, 903, 1380, 913, 914, 872, 823, 1013, 1048, 1001, 1289, 1063, 1261, 792, 771, 1852, 3074, 928, 1346, 1238, 1786, 1036, 857, 1028, 1149, 902, 749, 1003, 1101, 1014, 883, 2012, 1152, 1374, 1468, 1242, 1374, 1312, 1447, 975, 848, 716, 1067, 940, 1566, 1298, 1468, 897, 3196, 1406, 2574, 2206, 5376, 2771, 1455, 2052, 2923, 1401, 908, 2522, 1562, 3768, 3473, 2336, 813, 3879, 2968, 5270, 2441, 1323, 1398, 1176, 1245, 843, 944, 984, 1172, 878, 851, 1168, 1116, 1029, 2612, 900, 1471, 827, 767, 953, 1479, 908, 4228, 772, 1342, 753, 719, 828, 761, 745, 667, 729, 686, 713, 639, 737, 713, 666, 672, 705, 725, 678, 685, 830, 987, 702, 760, 765, 719, 679, 645, 780, 748, 680, 685, 716, 685, 621, 669, 816, 653, 658, 659, 678, 658, 618, 628, 903, 654, 659, 629, 654, 667, 612, 740, 1437, 848, 819, 1296, 1546, 564, 831, 996, 689, 721, 643, 583, 3203, 570, 617, 603, 719, 594, 601, 548, 576, 541, 548, 547, 731, 557, 553, 589, 663, 543, 610, 562, 710, 552, 583, 580, 599, 812, 518, 600, 1907, 648, 515, 828, 993, 545, 527, 577, 841, 550, 571, 629, 667, 565, 750, 723, 1097, 631, 546, 581, 864, 605, 859, 4201], 'byteentropy': [24434, 11, 18, 5, 9, 8, 35, 21, 3, 4, 8, 4, 2, 3, 7, 4, 3921, 32, 41, 22, 21, 4, 15, 7, 22, 0, 0, 4, 0, 0, 4, 3, 7561, 18, 51, 73, 51, 40, 104, 71, 116, 13, 25, 9, 10, 19, 13, 18, 5508, 58, 71, 53, 75, 49, 86, 58, 39, 15, 9, 16, 20, 9, 10, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11511, 101, 126, 342, 292, 209, 2287, 1039, 128, 18, 10, 39, 105, 49, 55, 73, 10708, 79, 164, 153, 171, 123, 5559, 3045, 47, 46, 77, 66, 45, 98, 46, 53, 6544, 90, 493, 123, 406, 153, 2676, 1645, 16, 13, 10, 9, 18, 9, 23, 60, 7743, 288, 14879, 1670, 2485, 2220, 19097, 10241, 86, 94, 104, 72, 78, 97, 92, 146, 12536, 518, 4286, 917, 2292, 1775, 8007, 4676, 521, 112, 150, 156, 203, 139, 214, 362, 9395, 855, 1791, 1529, 2377, 1044, 4336, 2572, 453, 250, 210, 235, 342, 224, 236, 775, 11736, 1103, 1073, 5260, 1820, 1455, 2484, 2296, 1637, 698, 741, 691, 1121, 684, 784, 1233, 3145, 294, 554, 638, 544, 577, 973, 1039, 700, 334, 308, 203, 212, 131, 246, 342, 29295, 4407, 2827, 3128, 8848, 10365, 5312, 5061, 15488, 874, 1533, 1870, 11498, 1445, 5335, 9450, 6830, 2164, 3620, 5074, 6207, 7724, 9480, 9261, 8770, 6310, 6425, 4435, 4128, 2301, 2210, 3125, 16372, 13900, 14628, 14176, 14254, 14749, 16462, 15966, 14060, 13914, 13731, 13957, 14147, 13756, 13708, 13644], 'strings': {'numstrings': 3863, 'avlength': 17.643541289153507, 'printabledist': [7399, 89, 373, 105, 116, 114, 101, 94, 430, 406, 359, 173, 325, 243, 829, 797, 479, 350, 283, 236, 214, 214, 236, 226, 213, 241, 345, 498, 258, 415, 188, 164, 198, 422, 274, 724, 394, 386, 311, 243, 304, 550, 138, 198, 352, 261, 343, 294, 469, 208, 517, 646, 526, 290, 261, 452, 242, 212, 174, 254, 252, 237, 176, 550, 210, 2508, 825, 1746, 1558, 4547, 941, 883, 1064, 2222, 317, 347, 1798, 938, 3013, 2686, 1543, 219, 3052, 2222, 3465, 1030, 653, 651, 457, 573, 234, 307, 201, 236, 146, 194], 'printables': 68157, 'entropy': 5.683163905800377, 'paths': 6, 'urls': 22, 'registry': 0, 'MZ': 7}, 'general': {'size': 349811, 'vsize': 28672, 'has_debug': 0, 'exports': 0, 'imports': 55, 'has_relocations': 0, 'has_resources': 1, 'has_signature': 0, 'has_tls': 0, 'symbols': 0}, 'header': {'coff': {'timestamp': 1301832471, 'machine': 'I386', 'characteristics': ['CHARA_32BIT_MACHINE', 'RELOCS_STRIPPED', 'EXECUTABLE_IMAGE', 'LINE_NUMS_STRIPPED', 'LOCAL_SYMS_STRIPPED']}, 'optional': {'subsystem': 'WINDOWS_GUI', 'dll_characteristics': [], 'magic': 'PE32', 'major_image_version': 0, 'minor_image_version': 0, 'major_linker_version': 2, 'minor_linker_version': 50, 'major_operating_system_version': 4, 'minor_operating_system_version': 0, 'major_subsystem_version': 4, 'minor_subsystem_version': 0, 'sizeof_code': 8704, 'sizeof_headers': 1024, 'sizeof_heap_commit': 4096}}, 'section': {'entry': '.code', 'sections': [{'name': '.code', 'size': 2048, 'entropy': 5.322405635106704, 'vsize': 1841, 'props': ['CNT_CODE', 'MEM_EXECUTE', 'MEM_READ']}, {'name': '.text', 'size': 6656, 'entropy': 6.191552965438026, 'vsize': 6552, 'props': ['CNT_CODE', 'MEM_EXECUTE', 'MEM_READ']}, {'name': '.rdata', 'size': 512, 'entropy': 1.7695459925589745, 'vsize': 28, 'props': ['CNT_INITIALIZED_DATA', 'MEM_READ']}, {'name': '.data', 'size': 2048, 'entropy': 4.516154676301226, 'vsize': 1960, 'props': ['CNT_INITIALIZED_DATA', 'MEM_READ', 'MEM_WRITE']}, {'name': '.rsrc', 'size': 1024, 'entropy': 5.06744528418907, 'vsize': 700, 'props': ['CNT_INITIALIZED_DATA', 'MEM_READ']}]}, 'imports': {'MSVCRT.dll': ['memset', 'memcpy', '_stricmp', 'strncmp', '_strnicmp', 'strcmp', 'memmove', 'strlen', 'strcpy', 'strcat', 'strncpy'], 'KERNEL32.dll': ['GetModuleHandleA', 'HeapCreate', 'HeapDestroy', 'ExitProcess', 'GetCurrentThreadId', 'GetTickCount', 'HeapAlloc', 'HeapFree', 'WriteFile', 'CloseHandle', 'CreateFileA', 'GetFileSize', 'ReadFile', 'SetFilePointer', 'InitializeCriticalSection', 'GetModuleFileNameA', 'GetCurrentProcess', 'DuplicateHandle', 'CreatePipe', 'GetStdHandle', 'CreateProcessA', 'WaitForSingleObject', 'EnterCriticalSection', 'LeaveCriticalSection', 'GetCurrentProcessId', 'GetDriveTypeA', 'FindFirstFileA', 'FindClose', 'GetFileAttributesA', 'CreateDirectoryA', 'GetLastError', 'FindNextFileA', 'SetFileAttributesA', 'HeapReAlloc'], 'COMCTL32.DLL': ['InitCommonControls'], 'USER32.DLL': ['MessageBoxA', 'GetWindowThreadProcessId', 'IsWindowVisible', 'IsWindowEnabled', 'GetForegroundWindow', 'EnableWindow', 'EnumWindows'], 'SHELL32.DLL': ['ShellExecuteExA'], 'OLE32.DLL': ['CoInitialize']}, 'exports': [], 'datadirectories': [{'name': 'EXPORT_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'IMPORT_TABLE', 'size': 140, 'virtual_address': 20604}, {'name': 'RESOURCE_TABLE', 'size': 700, 'virtual_address': 24576}, {'name': 'EXCEPTION_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'CERTIFICATE_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'BASE_RELOCATION_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'DEBUG', 'size': 0, 'virtual_address': 0}, {'name': 'ARCHITECTURE', 'size': 0, 'virtual_address': 0}, {'name': 'GLOBAL_PTR', 'size': 0, 'virtual_address': 0}, {'name': 'TLS_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'LOAD_CONFIG_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'BOUND_IMPORT', 'size': 0, 'virtual_address': 0}, {'name': 'IAT', 'size': 244, 'virtual_address': 20988}, {'name': 'DELAY_IMPORT_DESCRIPTOR', 'size': 0, 'virtual_address': 0}, {'name': 'CLR_RUNTIME_HEADER', 'size': 0, 'virtual_address': 0}]}\n"
     ]
    }
   ],
   "source": [
    "json_file = r\"D:\\ClassWork\\anti_virus\\Vigil-Anti\\EXE_Dataset\\ember2018\\train_features_1.jsonl\"\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    json_ds_list = list(f)\n",
    "\n",
    "DataSet = []\n",
    "for i,ds in enumerate(json_ds_list):\n",
    "    if( i > 15000):\n",
    "        break\n",
    "    DataSet.append(json.loads(ds))\n",
    "\n",
    "# to free some of the precious memory\n",
    "del json_ds_list\n",
    "\n",
    "print (DataSet[0])\n",
    "\n",
    "simple_ds = DataSet[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the unique section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_sectionNames = set()\\nfor ds_obj in DataSet:\\n    for dic_elm in ds_obj[\\'section\\'][\\'sections\\']:\\n        all_sectionNames.add(dic_elm[\\'name\\'])\\n\\nwith open(\\'sectionNames.txt\\', \\'w\\') as f:\\n    f.write(\\'\\n\\'.join(all_sectionNames))\\n\\ncorrect_sec_names = []\\nfor n in all_sectionNames:\\n    if(n and n[0] == \".\"):\\n        correct_sec_names.append(n)\\n\\nwith open(\\'sectionNames_correct.txt\\', \\'w\\') as f:\\n    f.write(\\'\\n\\'.join(correct_sec_names))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "all_sectionNames = set()\n",
    "for ds_obj in DataSet:\n",
    "    for dic_elm in ds_obj['section']['sections']:\n",
    "        all_sectionNames.add(dic_elm['name'])\n",
    "\n",
    "with open('sectionNames.txt', 'w') as f:\n",
    "    f.write('\\n'.join(all_sectionNames))\n",
    "\n",
    "correct_sec_names = []\n",
    "for n in all_sectionNames:\n",
    "    if(n and n[0] == \".\"):\n",
    "        correct_sec_names.append(n)\n",
    "\n",
    "with open('sectionNames_correct.txt', 'w') as f:\n",
    "    f.write('\\n'.join(correct_sec_names))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spoiler: there are lots of malicious section names\n",
    "#### so I just extracted the most common and correct section names and then wrote them into \"common_section_names.txt\"\n",
    "#### any other section names will be considered \"UNKNOWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the most common section names\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'common_section_names.txt'), 'r') as f:\n",
    "    Common_section_names = f.readlines()\n",
    "\n",
    "Common_section_names = [re.sub(r'\\n', '', i) for i in Common_section_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore all the possible imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tqdm import tqdm\\nall_imports = set()\\nfor obj in tqdm(DataSet):\\n    import_DLL_dict = obj['imports']\\n    DLL_list = list(import_DLL_dict.keys())\\n    for elm in DLL_list:\\n        if(elm.endswith('.dll')):\\n            all_imports.add(elm)\\n    #all_imports = set(all_imports)\\n\\nwith open('all_imports_cleansed.txt', 'w') as f:\\n    f.write('\\n'.join(all_imports))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "all_imports = set()\n",
    "for obj in tqdm(DataSet):\n",
    "    import_DLL_dict = obj['imports']\n",
    "    DLL_list = list(import_DLL_dict.keys())\n",
    "    for elm in DLL_list:\n",
    "        if(elm.endswith('.dll')):\n",
    "            all_imports.add(elm)\n",
    "    #all_imports = set(all_imports)\n",
    "\n",
    "with open('all_imports_cleansed.txt', 'w') as f:\n",
    "    f.write('\\n'.join(all_imports))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Same problem with DLL imports, there are numerous different DLLs\n",
    "### and I cannot really filter all of them, so I will just grab the most common DLLs that are associated with most malwares\n",
    "### and another feature which will be the number of imported DLLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's just cleanse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleansing the dataset:   1%|          | 112/15001 [00:00<00:13, 1108.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleansing the dataset:  50%|████▉     | 7448/15001 [00:05<00:06, 1123.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inside handle_DLL_imports()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleansing the dataset:  91%|█████████▏| 13690/15001 [00:10<00:01, 1196.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inside handle_DLL_imports()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleansing the dataset: 100%|██████████| 15001/15001 [00:12<00:00, 1247.59it/s]\n"
     ]
    }
   ],
   "source": [
    "new_Dataset = []\n",
    "\n",
    "for simple_ds in tqdm(DataSet, desc='cleansing the dataset'):\n",
    "    try:\n",
    "        # add reduced features of byteentropy distribution\n",
    "        simple_ds.update(Interpret_Histogram(simple_ds['byteentropy'], 'byteentropy'))\n",
    "\n",
    "        # add reduced features of byte histogram distribution\n",
    "        simple_ds.update(Interpret_Histogram(simple_ds['histogram'], 'bytehistogram'))\n",
    "\n",
    "        # reduce strings field\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'strings', normalize_names=True, delete_field=True)\n",
    "\n",
    "        # flatten the strings printables distribution field\n",
    "        simple_ds = flatten_strings_printable_distribution(simple_ds, delete_field=True)\n",
    "\n",
    "        # reduce general field\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'general', normalize_names=True, delete_field=True)\n",
    "\n",
    "        # reduce header field\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'header', normalize_names=True, delete_field=True)\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'header_optional', normalize_names=False, delete_field=True)\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'header_coff', normalize_names=False, delete_field=True)\n",
    "\n",
    "\n",
    "        # handle data directories field\n",
    "        simple_ds = handle_data_directories_field(simple_ds)\n",
    "\n",
    "\n",
    "        # handle sections fields\n",
    "        simple_ds = handle_section_names(simple_ds, Common_section_names, delete_field=True)\n",
    "\n",
    "        # handle imports fields\n",
    "        simple_ds = handle_DLL_imports(simple_ds, delete_field=False)\n",
    "\n",
    "        # Remove the useless columns for now (they are not entirely useless but they will make the training process very complex for me :(( )\n",
    "        useless_columns = ['sha256'\n",
    "            ,'md5'\n",
    "            ,'appeared'\n",
    "            ,'avclass'\n",
    "            ,'histogram'\n",
    "            ,'byteentropy'\n",
    "            ,'imports'\n",
    "            ,'exports'\n",
    "            ,'dll_characteristics'\n",
    "            ,'characteristics']\n",
    "\n",
    "        for useless_col in useless_columns:\n",
    "            del simple_ds[useless_col]\n",
    "        \n",
    "        new_Dataset.append(simple_ds)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Finally, free the original dataset from our precious memory\n",
    "del DataSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's prepare our Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= pd.DataFrame()\n",
    "# i = 0\n",
    "# for dic in new_Dataset:\n",
    "#     df = pd.concat([df, pd.DataFrame([0]*len(df.columns))], axis=0)\n",
    "#     for k in dic.keys():\n",
    "#         if k in df.columns:\n",
    "#             try:\n",
    "#                 df.loc[i, k] = dic[k]\n",
    "#             except:\n",
    "#                 print(k)\n",
    "#                 print(df)\n",
    "#         else:\n",
    "#             dummy_list = pd.DataFrame([0]*len(df) if len(df) > 0 else [0])\n",
    "#             df.insert(0, k, dummy_list)\n",
    "#             #print(df.columns)\n",
    "#             df.loc[i, k] = dic[k]\n",
    "    \n",
    "#     #print(df.head())\n",
    "#     i+=1\n",
    "\n",
    "\n",
    "# df.fillna(0)\n",
    "# print(df)\n",
    "\n",
    "# df.to_csv('lol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "contructing a very big Dictionary:   0%|          | 0/15001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "contructing a very big Dictionary: 100%|██████████| 15001/15001 [00:01<00:00, 11253.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.idata_size</th>\n",
       "      <th>strings_printabledist_25</th>\n",
       "      <th>.aspack_vsize</th>\n",
       "      <th>sizeof_headers</th>\n",
       "      <th>strings_printabledist_9</th>\n",
       "      <th>strings_printabledist_82</th>\n",
       "      <th>strings_printabledist_44</th>\n",
       "      <th>strings_printabledist_95</th>\n",
       "      <th>general_imports</th>\n",
       "      <th>strings_registry</th>\n",
       "      <th>...</th>\n",
       "      <th>strings_printabledist_51</th>\n",
       "      <th>.idat_vsize</th>\n",
       "      <th>CLR_RUNTIME_HEADER_virtual_address</th>\n",
       "      <th>.xdata_size</th>\n",
       "      <th>strings_printabledist_14</th>\n",
       "      <th>.textbss_vsize</th>\n",
       "      <th>strings_avlength</th>\n",
       "      <th>.xdata_props_len</th>\n",
       "      <th>mean_of_first_tertile_byteentropy</th>\n",
       "      <th>strings_numstrings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15001.000000</td>\n",
       "      <td>1.500100e+04</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>1.500100e+04</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>1.500100e+04</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>1.500100e+04</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>15001.000000</td>\n",
       "      <td>1.500100e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373.918205</td>\n",
       "      <td>2.108124e+03</td>\n",
       "      <td>126.421439</td>\n",
       "      <td>1856.527432</td>\n",
       "      <td>390.415706</td>\n",
       "      <td>2.481147e+03</td>\n",
       "      <td>803.101860</td>\n",
       "      <td>357.667889</td>\n",
       "      <td>114.269249</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>...</td>\n",
       "      <td>1737.138257</td>\n",
       "      <td>73.888407</td>\n",
       "      <td>9.965269e+04</td>\n",
       "      <td>485.454836</td>\n",
       "      <td>958.848077</td>\n",
       "      <td>7.492201e+02</td>\n",
       "      <td>36.876985</td>\n",
       "      <td>0.108259</td>\n",
       "      <td>3121.380347</td>\n",
       "      <td>8.029616e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6502.073311</td>\n",
       "      <td>2.198231e+04</td>\n",
       "      <td>6694.649364</td>\n",
       "      <td>3649.037775</td>\n",
       "      <td>1632.070865</td>\n",
       "      <td>1.338122e+04</td>\n",
       "      <td>3351.127516</td>\n",
       "      <td>1610.162114</td>\n",
       "      <td>201.690491</td>\n",
       "      <td>6.564754</td>\n",
       "      <td>...</td>\n",
       "      <td>5231.488967</td>\n",
       "      <td>1874.958170</td>\n",
       "      <td>1.144656e+07</td>\n",
       "      <td>8252.815492</td>\n",
       "      <td>7626.328708</td>\n",
       "      <td>3.747466e+04</td>\n",
       "      <td>378.607564</td>\n",
       "      <td>0.995061</td>\n",
       "      <td>13167.978979</td>\n",
       "      <td>2.855483e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.460000e+02</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.697155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.576471</td>\n",
       "      <td>6.450000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.970000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>7.430000e+02</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>368.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>12.005911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>162.576471</td>\n",
       "      <td>2.659000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.660000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4096.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>2.184000e+03</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>802.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>17.862069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1145.882353</td>\n",
       "      <td>6.624000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>433664.000000</td>\n",
       "      <td>1.179314e+06</td>\n",
       "      <td>774144.000000</td>\n",
       "      <td>207360.000000</td>\n",
       "      <td>56245.000000</td>\n",
       "      <td>1.050041e+06</td>\n",
       "      <td>149959.000000</td>\n",
       "      <td>61444.000000</td>\n",
       "      <td>9024.000000</td>\n",
       "      <td>710.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>234986.000000</td>\n",
       "      <td>51200.000000</td>\n",
       "      <td>1.401809e+09</td>\n",
       "      <td>502784.000000</td>\n",
       "      <td>672431.000000</td>\n",
       "      <td>3.489435e+06</td>\n",
       "      <td>35348.421466</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>559351.576471</td>\n",
       "      <td>1.158922e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         .idata_size  strings_printabledist_25  .aspack_vsize  sizeof_headers   \n",
       "count   15001.000000              1.500100e+04   15001.000000    15001.000000  \\\n",
       "mean     1373.918205              2.108124e+03     126.421439     1856.527432   \n",
       "std      6502.073311              2.198231e+04    6694.649364     3649.037775   \n",
       "min         0.000000              0.000000e+00       0.000000      512.000000   \n",
       "25%         0.000000              3.400000e+01       0.000000     1024.000000   \n",
       "50%         0.000000              1.970000e+02       0.000000     1024.000000   \n",
       "75%         0.000000              6.660000e+02       0.000000     4096.000000   \n",
       "max    433664.000000              1.179314e+06  774144.000000   207360.000000   \n",
       "\n",
       "       strings_printabledist_9  strings_printabledist_82   \n",
       "count             15001.000000              1.500100e+04  \\\n",
       "mean                390.415706              2.481147e+03   \n",
       "std                1632.070865              1.338122e+04   \n",
       "min                   0.000000              0.000000e+00   \n",
       "25%                  14.000000              2.460000e+02   \n",
       "50%                  72.000000              7.430000e+02   \n",
       "75%                 226.000000              2.184000e+03   \n",
       "max               56245.000000              1.050041e+06   \n",
       "\n",
       "       strings_printabledist_44  strings_printabledist_95  general_imports   \n",
       "count              15001.000000              15001.000000     15001.000000  \\\n",
       "mean                 803.101860                357.667889       114.269249   \n",
       "std                 3351.127516               1610.162114       201.690491   \n",
       "min                    0.000000                  0.000000         0.000000   \n",
       "25%                   57.000000                  9.000000         8.000000   \n",
       "50%                  241.000000                 66.000000        67.000000   \n",
       "75%                  680.000000                210.000000       163.000000   \n",
       "max               149959.000000              61444.000000      9024.000000   \n",
       "\n",
       "       strings_registry  ...  strings_printabledist_51   .idat_vsize   \n",
       "count      15001.000000  ...              15001.000000  15001.000000  \\\n",
       "mean           0.302646  ...               1737.138257     73.888407   \n",
       "std            6.564754  ...               5231.488967   1874.958170   \n",
       "min            0.000000  ...                  0.000000      0.000000   \n",
       "25%            0.000000  ...                112.000000      0.000000   \n",
       "50%            0.000000  ...                368.000000      0.000000   \n",
       "75%            0.000000  ...               1143.000000      0.000000   \n",
       "max          710.000000  ...             234986.000000  51200.000000   \n",
       "\n",
       "       CLR_RUNTIME_HEADER_virtual_address    .xdata_size   \n",
       "count                        1.500100e+04   15001.000000  \\\n",
       "mean                         9.965269e+04     485.454836   \n",
       "std                          1.144656e+07    8252.815492   \n",
       "min                          0.000000e+00       0.000000   \n",
       "25%                          0.000000e+00       0.000000   \n",
       "50%                          0.000000e+00       0.000000   \n",
       "75%                          0.000000e+00       0.000000   \n",
       "max                          1.401809e+09  502784.000000   \n",
       "\n",
       "       strings_printabledist_14  .textbss_vsize  strings_avlength   \n",
       "count              15001.000000    1.500100e+04      15001.000000  \\\n",
       "mean                 958.848077    7.492201e+02         36.876985   \n",
       "std                 7626.328708    3.747466e+04        378.607564   \n",
       "min                    0.000000    0.000000e+00          0.000000   \n",
       "25%                   65.000000    0.000000e+00          7.697155   \n",
       "50%                  205.000000    0.000000e+00         12.005911   \n",
       "75%                  802.000000    0.000000e+00         17.862069   \n",
       "max               672431.000000    3.489435e+06      35348.421466   \n",
       "\n",
       "       .xdata_props_len  mean_of_first_tertile_byteentropy  strings_numstrings  \n",
       "count      15001.000000                       15001.000000        1.500100e+04  \n",
       "mean           0.108259                        3121.380347        8.029616e+03  \n",
       "std            0.995061                       13167.978979        2.855483e+04  \n",
       "min            0.000000                           0.000000        0.000000e+00  \n",
       "25%            0.000000                          20.576471        6.450000e+02  \n",
       "50%            0.000000                         162.576471        2.659000e+03  \n",
       "75%            0.000000                        1145.882353        6.624000e+03  \n",
       "max           14.000000                      559351.576471        1.158922e+06  \n",
       "\n",
       "[8 rows x 281 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just make a very big dictionary by joining keys together\n",
    "# then just casting this very big dictionary to a very big dataframe :)\n",
    "\n",
    "all_keys = set().union(*new_Dataset)\n",
    "\n",
    "merged_dict = {}\n",
    "\n",
    "for d in tqdm(new_Dataset, desc=\"contructing a very big Dictionary\"):\n",
    "    for key in all_keys:\n",
    "        if key in d.keys():\n",
    "            if key in merged_dict:\n",
    "                merged_dict[key].append(d[key])\n",
    "            else:\n",
    "                merged_dict[key] = [d[key]]\n",
    "        else:\n",
    "            if key not in merged_dict:\n",
    "                merged_dict[key] = []\n",
    "\n",
    "df = pd.DataFrame().from_dict(merged_dict, orient='index').transpose()\n",
    "\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "df.to_csv('Dataset.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".idata_size:        int64\n",
      "strings_printabledist_25:        int64\n",
      ".aspack_vsize:        int64\n",
      "sizeof_headers:        int64\n",
      "strings_printabledist_9:        int64\n",
      "strings_printabledist_82:        int64\n",
      "strings_printabledist_44:        int64\n",
      "strings_printabledist_95:        int64\n",
      "general_imports:        int64\n",
      "strings_registry:        int64\n",
      ".code_props_len:        int64\n",
      ".code_entropy:        int64\n",
      "strings_printabledist_89:        int64\n",
      "minor_subsystem_version:        int64\n",
      "strings_printabledist_10:        int64\n",
      "strings_printabledist_88:        int64\n",
      "strings_printabledist_32:        int64\n",
      "general_symbols:        int64\n",
      ".data_vsize:        int64\n",
      ".pdata_props_len:        int64\n",
      "strings_printabledist_17:        int64\n",
      "mean_of_second_tertile_bytehistogram:        int64\n",
      "RESOURCE_TABLE_size:        int64\n",
      "strings_printabledist_56:        int64\n",
      "strings_printabledist_15:        int64\n",
      "strings_printabledist_42:        int64\n",
      ".aspack_props_len:        int64\n",
      "strings_printabledist_3:        int64\n",
      "strings_printabledist_36:        int64\n",
      ".code_size:        int64\n",
      "strings_printabledist_57:        int64\n",
      "Ws2_32.dll:        bool\n",
      "strings_printables:        int64\n",
      "strings_printabledist_23:        int64\n",
      "strings_printabledist_76:        int64\n",
      ".idat_entropy:        int64\n",
      "strings_printabledist_60:        int64\n",
      ".reloc_size:        int64\n",
      "strings_printabledist_7:        int64\n",
      "total_bytes_byteentropy:        int64\n",
      "strings_printabledist_61:        int64\n",
      "strings_printabledist_91:        int64\n",
      "BOUND_IMPORT_size:        int64\n",
      ".edata_entropy:        int64\n",
      ".UPX_vsize:        int64\n",
      ".upx_vsize:        int64\n",
      "IMPORT_TABLE_size:        int64\n",
      "strings_printabledist_92:        int64\n",
      "total_num_of_imported_funcs:        int64\n",
      "strings_printabledist_78:        int64\n",
      "BASE_RELOCATION_TABLE_size:        int64\n",
      "strings_printabledist_70:        int64\n",
      "ncrypt.dll:        bool\n",
      "Rpcrt4.dll_num_funcs:        int64\n",
      "strings_printabledist_93:        int64\n",
      "Shell32.dll_num_funcs:        int64\n",
      "USER32.dll:        bool\n",
      "Dbgcore.dll_num_funcs:        int64\n",
      "Iphlpapi.dll:        bool\n",
      ".data_entropy:        int64\n",
      "UNKNOWN_SECTION_props_len:        int64\n",
      "TLS_TABLE_size:        int64\n",
      "ARCHITECTURE_virtual_address:        int64\n",
      "DEBUG_size:        int64\n",
      "TLS_TABLE_virtual_address:        int64\n",
      "RESOURCE_TABLE_virtual_address:        int64\n",
      "strings_printabledist_43:        int64\n",
      "strings_printabledist_87:        int64\n",
      "BOUND_IMPORT_virtual_address:        int64\n",
      ".text_props_len:        int64\n",
      ".reloc_props_len:        int64\n",
      ".aspack_entropy:        int64\n",
      "minor_linker_version:        int64\n",
      ".UPX_size:        int64\n",
      "zero_bytes_byteentropy:        int64\n",
      "strings_printabledist_47:        int64\n",
      "strings_printabledist_21:        int64\n",
      ".bss_size:        int64\n",
      "full_bytes_bytehistogram:        int64\n",
      "major_linker_version:        int64\n",
      "strings_printabledist_5:        int64\n",
      "Rpcrt4.dll:        bool\n",
      "strings_printabledist_85:        int64\n",
      "strings_printabledist_22:        int64\n",
      "strings_printabledist_38:        int64\n",
      "sizeof_code:        int64\n",
      "strings_printabledist_63:        int64\n",
      "CERTIFICATE_TABLE_virtual_address:        int64\n",
      "CERTIFICATE_TABLE_size:        int64\n",
      "Dbghelp.dll_num_funcs:        int64\n",
      "strings_printabledist_37:        int64\n",
      "strings_printabledist_1:        int64\n",
      "OLEAUT32.dll_num_funcs:        int64\n",
      ".pdata_vsize:        int64\n",
      ".edata_props_len:        int64\n",
      "USER32.dll_num_funcs:        int64\n",
      ".debug_props_len:        int64\n",
      "strings_printabledist_58:        int64\n",
      ".tls_size:        int64\n",
      "Ole32.dll_num_funcs:        int64\n",
      ".xdata_entropy:        int64\n",
      "strings_entropy:        int64\n",
      "Advapi32.dll:        bool\n",
      ".reloc_entropy:        int64\n",
      "Ws2_32.dll_num_funcs:        int64\n",
      ".UPX_entropy:        int64\n",
      ".pdata_size:        int64\n",
      ".edata_vsize:        int64\n",
      ".text_vsize:        int64\n",
      "major_operating_system_version:        int64\n",
      ".sxdata_entropy:        int64\n",
      ".idat_props_len:        int64\n",
      ".tls_props_len:        int64\n",
      "strings_printabledist_84:        int64\n",
      "general_exports:        int64\n",
      ".upx_entropy:        int64\n",
      ".edata_size:        int64\n",
      "strings_printabledist_39:        int64\n",
      "DELAY_IMPORT_DESCRIPTOR_size:        int64\n",
      ".UPX_props_len:        int64\n",
      "DELAY_IMPORT_DESCRIPTOR_virtual_address:        int64\n",
      "strings_printabledist_66:        int64\n",
      ".data_size:        int64\n",
      "Wininet.dll_num_funcs:        int64\n",
      "strings_printabledist_73:        int64\n",
      "strings_printabledist_31:        int64\n",
      ".debug_entropy:        int64\n",
      "EXPORT_TABLE_size:        int64\n",
      "strings_printabledist_65:        int64\n",
      ".CRT_size:        int64\n",
      ".aspack_size:        int64\n",
      ".code_vsize:        int64\n",
      "major_subsystem_version:        int64\n",
      "strings_printabledist_19:        int64\n",
      "Winmm.dll_num_funcs:        int64\n",
      "ncrypt.dll_num_funcs:        int64\n",
      "strings_printabledist_2:        int64\n",
      "strings_printabledist_35:        int64\n",
      "EXPORT_TABLE_virtual_address:        int64\n",
      "strings_printabledist_40:        int64\n",
      "EXCEPTION_TABLE_virtual_address:        int64\n",
      ".upx_size:        int64\n",
      "mean_of_first_tertile_bytehistogram:        int64\n",
      ".rsrc_vsize:        int64\n",
      ".text_entropy:        int64\n",
      "strings_printabledist_50:        int64\n",
      ".bss_props_len:        int64\n",
      "mean_of_bytes_byteentropy:        int64\n",
      "strings_printabledist_8:        int64\n",
      "Dbghelp.dll:        bool\n",
      "Sensapi.dll:        bool\n",
      "strings_printabledist_94:        int64\n",
      "GDI32.dll_num_funcs:        int64\n",
      ".rsrc_props_len:        int64\n",
      ".rdata_vsize:        int64\n",
      "strings_printabledist_59:        int64\n",
      "strings_printabledist_49:        int64\n",
      "strings_printabledist_16:        int64\n",
      "subsystem:        object\n",
      "mean_of_bytes_bytehistogram:        int64\n",
      ".rdata_props_len:        int64\n",
      "User32.dll:        bool\n",
      ".CRT_vsize:        int64\n",
      ".sxdata_size:        int64\n",
      "Crypt32.dll:        bool\n",
      "Psapi.dll_num_funcs:        int64\n",
      "Advapi32.dll_num_funcs:        int64\n",
      "strings_printabledist_54:        int64\n",
      "UNKNOWN_SECTION_vsize:        int64\n",
      ".tls_entropy:        int64\n",
      "strings_printabledist_18:        int64\n",
      ".idata_vsize:        int64\n",
      "strings_MZ:        int64\n",
      "general_has_tls:        int64\n",
      ".rdata_size:        int64\n",
      "strings_printabledist_46:        int64\n",
      ".bss_entropy:        int64\n",
      "Sensapi.dll_num_funcs:        int64\n",
      "GLOBAL_PTR_size:        int64\n",
      "Bcrypt.dll:        bool\n",
      "strings_printabledist_86:        int64\n",
      ".bss_vsize:        int64\n",
      "standard_dev_bytehistogram:        int64\n",
      "strings_printabledist_52:        int64\n",
      "strings_printabledist_6:        int64\n",
      ".xdata_vsize:        int64\n",
      "Msvcr.dll:        bool\n",
      "strings_printabledist_34:        int64\n",
      "OLEAUT32.dll:        bool\n",
      "strings_printabledist_83:        int64\n",
      "general_has_resources:        int64\n",
      "Msvcr.dll_num_funcs:        int64\n",
      ".rsrc_size:        int64\n",
      "strings_printabledist_0:        int64\n",
      "strings_printabledist_80:        int64\n",
      "IAT_size:        int64\n",
      "strings_printabledist_33:        int64\n",
      ".data_props_len:        int64\n",
      "strings_printabledist_13:        int64\n",
      "label:        int64\n",
      "Kernel32.dll:        bool\n",
      "strings_printabledist_12:        int64\n",
      ".debug_vsize:        int64\n",
      "strings_printabledist_11:        int64\n",
      "strings_printabledist_64:        int64\n",
      "strings_printabledist_79:        int64\n",
      "strings_printabledist_74:        int64\n",
      "Urlmon.dll_num_funcs:        int64\n",
      "magic:        object\n",
      "Shell32.dll:        bool\n",
      "Winmm.dll:        bool\n",
      "general_has_debug:        int64\n",
      "strings_printabledist_55:        int64\n",
      "Kernel32.dll_num_funcs:        int64\n",
      "strings_printabledist_48:        int64\n",
      "GDI32.dll:        bool\n",
      "Ole32.dll:        bool\n",
      ".rsrc_entropy:        int64\n",
      "full_bytes_byteentropy:        int64\n",
      "strings_printabledist_45:        int64\n",
      ".rdata_entropy:        int64\n",
      "strings_printabledist_72:        int64\n",
      "general_size:        int64\n",
      "Iphlpapi.dll_num_funcs:        int64\n",
      "User32.dll_num_funcs:        int64\n",
      "strings_printabledist_67:        int64\n",
      "DEBUG_virtual_address:        int64\n",
      "minor_operating_system_version:        int64\n",
      ".textbss_entropy:        int64\n",
      ".idata_props_len:        int64\n",
      "general_has_signature:        int64\n",
      "timestamp:        int64\n",
      "standard_dev_byteentropy:        int64\n",
      "EXCEPTION_TABLE_size:        int64\n",
      "mean_of_third_tertile_bytehistogram:        int64\n",
      "strings_urls:        int64\n",
      "IMPORT_TABLE_virtual_address:        int64\n",
      "LOAD_CONFIG_TABLE_virtual_address:        int64\n",
      "general_has_relocations:        int64\n",
      "strings_printabledist_62:        int64\n",
      ".reloc_vsize:        int64\n",
      ".sxdata_vsize:        int64\n",
      "strings_printabledist_41:        int64\n",
      "Psapi.dll:        bool\n",
      ".idat_size:        int64\n",
      ".pdata_entropy:        int64\n",
      "strings_paths:        int64\n",
      "strings_printabledist_30:        int64\n",
      "total_bytes_bytehistogram:        int64\n",
      "IAT_virtual_address:        int64\n",
      "CLR_RUNTIME_HEADER_size:        int64\n",
      ".CRT_entropy:        int64\n",
      ".idata_entropy:        int64\n",
      ".sxdata_props_len:        int64\n",
      "UNKNOWN_SECTION_entropy:        int64\n",
      "sizeof_heap_commit:        int64\n",
      "strings_printabledist_28:        int64\n",
      "Bcrypt.dll_num_funcs:        int64\n",
      "strings_printabledist_4:        int64\n",
      "general_vsize:        int64\n",
      "UNKNOWN_SECTION_size:        int64\n",
      "minor_image_version:        int64\n",
      "number_of_DLLs:        int64\n",
      ".textbss_size:        int64\n",
      "strings_printabledist_77:        int64\n",
      "GLOBAL_PTR_virtual_address:        int64\n",
      "zero_bytes_bytehistogram:        int64\n",
      "mean_of_third_tertile_byteentropy:        int64\n",
      ".text_size:        int64\n",
      "Crypt32.dll_num_funcs:        int64\n",
      ".textbss_props_len:        int64\n",
      "strings_printabledist_69:        int64\n",
      "strings_printabledist_81:        int64\n",
      "BASE_RELOCATION_TABLE_virtual_address:        int64\n",
      "strings_printabledist_29:        int64\n",
      "mean_of_second_tertile_byteentropy:        int64\n",
      "ARCHITECTURE_size:        int64\n",
      "strings_printabledist_68:        int64\n",
      "strings_printabledist_90:        int64\n",
      "strings_printabledist_20:        int64\n",
      ".tls_vsize:        int64\n",
      "Wininet.dll:        bool\n",
      "strings_printabledist_24:        int64\n",
      "strings_printabledist_53:        int64\n",
      "major_image_version:        int64\n",
      "strings_printabledist_26:        int64\n",
      "LOAD_CONFIG_TABLE_size:        int64\n",
      ".upx_props_len:        int64\n",
      "strings_printabledist_75:        int64\n",
      "strings_printabledist_27:        int64\n",
      "machine:        object\n",
      "Dbgcore.dll:        bool\n",
      ".CRT_props_len:        int64\n",
      "strings_printabledist_71:        int64\n",
      ".debug_size:        int64\n",
      "strings_printabledist_51:        int64\n",
      ".idat_vsize:        int64\n",
      "CLR_RUNTIME_HEADER_virtual_address:        int64\n",
      ".xdata_size:        int64\n",
      "strings_printabledist_14:        int64\n",
      ".textbss_vsize:        int64\n",
      "strings_avlength:        int64\n",
      ".xdata_props_len:        int64\n",
      "mean_of_first_tertile_byteentropy:        int64\n",
      "strings_numstrings:        int64\n",
      "Urlmon.dll:        bool\n",
      "total number of Features: 305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'suspicious_imports.txt'), 'r') as f:\n",
    "    sus_imports = f.readlines()\n",
    "sus_imports = [re.sub(r'\\n', '', i) for i in sus_imports]\n",
    "\n",
    "boolean_columns = sus_imports + []\n",
    "categorical_columns = [\"subsystem\", \"magic\", \"machine\"]\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in boolean_columns:\n",
    "        df[col] = df[col].astype(bool)\n",
    "        df[col].fillna(False)\n",
    "        continue\n",
    "\n",
    "    if col in categorical_columns:\n",
    "        df[col].replace(0, 'UNKNOWN', inplace=True)\n",
    "        continue\n",
    "    df[col].fillna(0)\n",
    "    df[col] = df[col].astype(np.int64)\n",
    "    df[col].fillna(0)\n",
    "\n",
    "for col in df.columns:\n",
    "    print(f\"{col}:        {df[col].dtype}\")\n",
    "\n",
    "\n",
    "df.to_csv('Dataset_big.csv', index=False)\n",
    "\n",
    "# Save our feature list\n",
    "feature_columns = list(df.columns)\n",
    "feature_columns.pop(feature_columns.index('label'))\n",
    "print(f\"total number of Features: {len(feature_columns)}\")\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'features.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_columns, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the -1 tuples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_train = df.copy()\n",
    "array_of_Label_Encoders = []\n",
    "for col in categorical_columns:\n",
    "    new_LE = LabelEncoder().fit(df_train[col])\n",
    "    df_train[col] = new_LE.transform(df_train[col])\n",
    "    array_of_Label_Encoders.append(new_LE)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'enc.pkl'), 'wb') as f:\n",
    "    pickle.dump(array_of_Label_Encoders, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.01      0.01      1526\n",
      "           1       0.53      1.00      0.70      1747\n",
      "\n",
      "    accuracy                           0.54      3273\n",
      "   macro avg       0.65      0.50      0.35      3273\n",
      "weighted avg       0.64      0.54      0.38      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "df_train_1 = df_train.copy()\n",
    "\n",
    "feature_columns = list(df_train_1.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_1[feature_columns], df_train_1['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "svm_model = SVC(kernel='poly', degree= 3, verbose=True).fit(x_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'svm.pkl'), 'wb') as f:\n",
    "    pickle.dump(svm_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93      1553\n",
      "           1       0.95      0.91      0.93      1720\n",
      "\n",
      "    accuracy                           0.93      3273\n",
      "   macro avg       0.93      0.93      0.93      3273\n",
      "weighted avg       0.93      0.93      0.93      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_train_2 = df.copy()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_train_2[col] = LabelEncoder().fit_transform(df_train_2[col])\n",
    "\n",
    "feature_columns = list(df_train_2.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_2[feature_columns], df_train_2['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "rf_model = RandomForestClassifier().fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'rf.pkl'), 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (batch_norm1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (batch_norm4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense5): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dense4 = nn.Linear(128, 128)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(128)\n",
    "        self.dense5 = nn.Linear(128, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x.float())\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = self.batch_norm2(x.float())\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = self.batch_norm3(x.float())\n",
    "        #x = torch.tanh(self.dense3(x))\n",
    "        x = torch.tanh(self.dense4(x))\n",
    "        x = self.batch_norm4(x.float())\n",
    "        x = torch.tanh(self.dense5(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swak\\AppData\\Local\\Temp\\ipykernel_17024\\3912236282.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(torch.from_numpy(np.asarray(y_train, dtype=bool)), dtype= torch.int64)\n",
      "C:\\Users\\swak\\AppData\\Local\\Temp\\ipykernel_17024\\3912236282.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(torch.from_numpy(np.asarray(y_val, dtype=bool)), dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.9762, Train Accuracy = 0.5331, Val Accuracy = 0.5969\n",
      "Epoch 2: Loss = 1.8950, Train Accuracy = 0.6431, Val Accuracy = 0.6545\n",
      "Epoch 3: Loss = 1.8472, Train Accuracy = 0.7103, Val Accuracy = 0.7238\n",
      "Epoch 4: Loss = 1.8177, Train Accuracy = 0.7595, Val Accuracy = 0.7762\n",
      "Epoch 5: Loss = 1.8041, Train Accuracy = 0.7742, Val Accuracy = 0.7565\n",
      "Epoch 6: Loss = 1.7977, Train Accuracy = 0.7778, Val Accuracy = 0.7932\n",
      "Epoch 7: Loss = 1.7928, Train Accuracy = 0.7908, Val Accuracy = 0.7997\n",
      "Epoch 8: Loss = 1.7945, Train Accuracy = 0.7825, Val Accuracy = 0.7919\n",
      "Epoch 9: Loss = 1.7881, Train Accuracy = 0.8024, Val Accuracy = 0.8010\n",
      "Epoch 10: Loss = 1.7865, Train Accuracy = 0.8010, Val Accuracy = 0.7971\n",
      "Epoch 11: Loss = 1.7847, Train Accuracy = 0.8050, Val Accuracy = 0.8050\n",
      "Epoch 12: Loss = 1.7835, Train Accuracy = 0.8082, Val Accuracy = 0.7958\n",
      "Epoch 13: Loss = 1.7840, Train Accuracy = 0.8116, Val Accuracy = 0.7919\n",
      "Epoch 14: Loss = 1.7843, Train Accuracy = 0.8063, Val Accuracy = 0.7919\n",
      "Epoch 15: Loss = 1.7843, Train Accuracy = 0.8034, Val Accuracy = 0.8076\n",
      "Epoch 16: Loss = 1.7829, Train Accuracy = 0.8148, Val Accuracy = 0.8115\n",
      "Epoch 17: Loss = 1.7807, Train Accuracy = 0.8136, Val Accuracy = 0.8128\n",
      "Epoch 18: Loss = 1.7788, Train Accuracy = 0.8151, Val Accuracy = 0.8154\n",
      "Epoch 19: Loss = 1.7790, Train Accuracy = 0.8136, Val Accuracy = 0.8168\n",
      "Epoch 20: Loss = 1.7806, Train Accuracy = 0.8125, Val Accuracy = 0.8102\n",
      "Epoch 21: Loss = 1.7795, Train Accuracy = 0.8117, Val Accuracy = 0.8128\n",
      "Epoch 22: Loss = 1.7800, Train Accuracy = 0.8149, Val Accuracy = 0.8128\n",
      "Epoch 23: Loss = 1.7768, Train Accuracy = 0.8142, Val Accuracy = 0.8115\n",
      "Epoch 24: Loss = 1.7790, Train Accuracy = 0.8122, Val Accuracy = 0.7945\n",
      "Epoch 25: Loss = 1.7776, Train Accuracy = 0.8181, Val Accuracy = 0.8168\n",
      "Epoch 26: Loss = 1.7752, Train Accuracy = 0.8178, Val Accuracy = 0.8207\n",
      "Epoch 27: Loss = 1.7740, Train Accuracy = 0.8235, Val Accuracy = 0.8194\n",
      "Epoch 28: Loss = 1.7742, Train Accuracy = 0.8264, Val Accuracy = 0.8246\n",
      "Epoch 29: Loss = 1.7735, Train Accuracy = 0.8247, Val Accuracy = 0.8364\n",
      "Epoch 30: Loss = 1.7730, Train Accuracy = 0.8250, Val Accuracy = 0.8403\n",
      "Epoch 31: Loss = 1.7725, Train Accuracy = 0.8295, Val Accuracy = 0.8272\n",
      "Epoch 32: Loss = 1.7717, Train Accuracy = 0.8257, Val Accuracy = 0.8154\n",
      "Epoch 33: Loss = 1.7704, Train Accuracy = 0.8373, Val Accuracy = 0.8442\n",
      "Epoch 34: Loss = 1.7698, Train Accuracy = 0.8346, Val Accuracy = 0.8482\n",
      "Epoch 35: Loss = 1.7679, Train Accuracy = 0.8423, Val Accuracy = 0.8325\n",
      "Epoch 36: Loss = 1.7666, Train Accuracy = 0.8478, Val Accuracy = 0.8312\n",
      "Epoch 37: Loss = 1.7654, Train Accuracy = 0.8530, Val Accuracy = 0.8351\n",
      "Epoch 38: Loss = 1.7660, Train Accuracy = 0.8525, Val Accuracy = 0.8521\n",
      "Epoch 39: Loss = 1.7652, Train Accuracy = 0.8507, Val Accuracy = 0.8390\n",
      "Epoch 40: Loss = 1.7612, Train Accuracy = 0.8619, Val Accuracy = 0.8482\n",
      "Epoch 41: Loss = 1.7611, Train Accuracy = 0.8672, Val Accuracy = 0.8547\n",
      "Epoch 42: Loss = 1.7614, Train Accuracy = 0.8637, Val Accuracy = 0.8469\n",
      "Epoch 43: Loss = 1.7592, Train Accuracy = 0.8698, Val Accuracy = 0.8495\n",
      "Epoch 44: Loss = 1.7579, Train Accuracy = 0.8712, Val Accuracy = 0.8521\n",
      "Epoch 45: Loss = 1.7555, Train Accuracy = 0.8771, Val Accuracy = 0.8586\n",
      "Epoch 46: Loss = 1.7571, Train Accuracy = 0.8747, Val Accuracy = 0.8521\n",
      "Epoch 47: Loss = 1.7587, Train Accuracy = 0.8724, Val Accuracy = 0.8429\n",
      "Epoch 48: Loss = 1.7571, Train Accuracy = 0.8725, Val Accuracy = 0.8416\n",
      "Epoch 49: Loss = 1.7535, Train Accuracy = 0.8837, Val Accuracy = 0.8469\n",
      "Epoch 50: Loss = 1.7535, Train Accuracy = 0.8826, Val Accuracy = 0.8325\n",
      "Epoch 51: Loss = 1.7542, Train Accuracy = 0.8829, Val Accuracy = 0.8586\n",
      "Epoch 52: Loss = 1.7548, Train Accuracy = 0.8816, Val Accuracy = 0.8547\n",
      "Epoch 53: Loss = 1.7548, Train Accuracy = 0.8792, Val Accuracy = 0.8455\n",
      "Epoch 54: Loss = 1.7542, Train Accuracy = 0.8803, Val Accuracy = 0.8534\n",
      "Epoch 55: Loss = 1.7508, Train Accuracy = 0.8888, Val Accuracy = 0.8482\n",
      "Epoch 56: Loss = 1.7497, Train Accuracy = 0.8931, Val Accuracy = 0.8403\n",
      "Epoch 57: Loss = 1.7529, Train Accuracy = 0.8852, Val Accuracy = 0.8469\n",
      "Epoch 58: Loss = 1.7526, Train Accuracy = 0.8856, Val Accuracy = 0.8482\n",
      "Epoch 59: Loss = 1.7488, Train Accuracy = 0.8944, Val Accuracy = 0.8626\n",
      "Epoch 60: Loss = 1.7514, Train Accuracy = 0.8888, Val Accuracy = 0.8495\n",
      "Epoch 61: Loss = 1.7494, Train Accuracy = 0.8961, Val Accuracy = 0.8416\n",
      "Epoch 62: Loss = 1.7472, Train Accuracy = 0.8983, Val Accuracy = 0.8547\n",
      "Epoch 63: Loss = 1.7464, Train Accuracy = 0.9000, Val Accuracy = 0.8521\n",
      "Epoch 64: Loss = 1.7462, Train Accuracy = 0.9014, Val Accuracy = 0.8377\n",
      "Epoch 65: Loss = 1.7483, Train Accuracy = 0.8951, Val Accuracy = 0.8416\n",
      "Epoch 66: Loss = 1.7486, Train Accuracy = 0.8954, Val Accuracy = 0.8469\n",
      "Epoch 67: Loss = 1.7477, Train Accuracy = 0.8984, Val Accuracy = 0.8547\n",
      "Epoch 68: Loss = 1.7465, Train Accuracy = 0.9014, Val Accuracy = 0.8560\n",
      "Epoch 69: Loss = 1.7448, Train Accuracy = 0.9030, Val Accuracy = 0.8521\n",
      "Epoch 70: Loss = 1.7430, Train Accuracy = 0.9078, Val Accuracy = 0.8586\n",
      "Epoch 71: Loss = 1.7435, Train Accuracy = 0.9072, Val Accuracy = 0.8495\n",
      "Epoch 72: Loss = 1.7443, Train Accuracy = 0.9060, Val Accuracy = 0.8442\n",
      "Epoch 73: Loss = 1.7425, Train Accuracy = 0.9088, Val Accuracy = 0.8560\n",
      "Epoch 74: Loss = 1.7438, Train Accuracy = 0.9073, Val Accuracy = 0.8599\n",
      "Epoch 75: Loss = 1.7433, Train Accuracy = 0.9080, Val Accuracy = 0.8586\n",
      "Epoch 76: Loss = 1.7458, Train Accuracy = 0.9008, Val Accuracy = 0.8521\n",
      "Epoch 77: Loss = 1.7440, Train Accuracy = 0.9070, Val Accuracy = 0.8547\n",
      "Epoch 78: Loss = 1.7424, Train Accuracy = 0.9091, Val Accuracy = 0.8678\n",
      "Epoch 79: Loss = 1.7423, Train Accuracy = 0.9107, Val Accuracy = 0.8482\n",
      "Epoch 80: Loss = 1.7415, Train Accuracy = 0.9127, Val Accuracy = 0.8560\n",
      "Epoch 81: Loss = 1.7412, Train Accuracy = 0.9131, Val Accuracy = 0.8495\n",
      "Epoch 82: Loss = 1.7408, Train Accuracy = 0.9150, Val Accuracy = 0.8534\n",
      "Epoch 83: Loss = 1.7421, Train Accuracy = 0.9114, Val Accuracy = 0.8678\n",
      "Epoch 84: Loss = 1.7416, Train Accuracy = 0.9118, Val Accuracy = 0.8547\n",
      "Epoch 85: Loss = 1.7394, Train Accuracy = 0.9165, Val Accuracy = 0.8599\n",
      "Epoch 86: Loss = 1.7406, Train Accuracy = 0.9137, Val Accuracy = 0.8560\n",
      "Epoch 87: Loss = 1.7387, Train Accuracy = 0.9211, Val Accuracy = 0.8599\n",
      "Epoch 88: Loss = 1.7401, Train Accuracy = 0.9153, Val Accuracy = 0.8573\n",
      "Epoch 89: Loss = 1.7412, Train Accuracy = 0.9130, Val Accuracy = 0.8534\n",
      "Epoch 90: Loss = 1.7396, Train Accuracy = 0.9159, Val Accuracy = 0.8613\n",
      "Epoch 91: Loss = 1.7404, Train Accuracy = 0.9155, Val Accuracy = 0.8586\n",
      "Epoch 92: Loss = 1.7394, Train Accuracy = 0.9162, Val Accuracy = 0.8573\n",
      "Epoch 93: Loss = 1.7410, Train Accuracy = 0.9143, Val Accuracy = 0.8613\n",
      "Epoch 94: Loss = 1.7408, Train Accuracy = 0.9142, Val Accuracy = 0.8573\n",
      "Epoch 95: Loss = 1.7395, Train Accuracy = 0.9175, Val Accuracy = 0.8573\n",
      "Epoch 96: Loss = 1.7383, Train Accuracy = 0.9203, Val Accuracy = 0.8586\n",
      "Epoch 97: Loss = 1.7362, Train Accuracy = 0.9258, Val Accuracy = 0.8613\n",
      "Epoch 98: Loss = 1.7409, Train Accuracy = 0.9156, Val Accuracy = 0.8599\n",
      "Epoch 99: Loss = 1.7407, Train Accuracy = 0.9133, Val Accuracy = 0.8455\n",
      "Epoch 100: Loss = 1.7406, Train Accuracy = 0.9150, Val Accuracy = 0.8613\n",
      "Epoch 101: Loss = 1.7375, Train Accuracy = 0.9219, Val Accuracy = 0.8495\n",
      "Epoch 102: Loss = 1.7377, Train Accuracy = 0.9223, Val Accuracy = 0.8455\n",
      "Epoch 103: Loss = 1.7384, Train Accuracy = 0.9188, Val Accuracy = 0.8560\n",
      "Epoch 104: Loss = 1.7391, Train Accuracy = 0.9184, Val Accuracy = 0.8560\n",
      "Epoch 105: Loss = 1.7385, Train Accuracy = 0.9200, Val Accuracy = 0.8508\n",
      "Epoch 106: Loss = 1.7370, Train Accuracy = 0.9220, Val Accuracy = 0.8599\n",
      "Epoch 107: Loss = 1.7374, Train Accuracy = 0.9225, Val Accuracy = 0.8573\n",
      "Epoch 108: Loss = 1.7367, Train Accuracy = 0.9257, Val Accuracy = 0.8573\n",
      "Epoch 109: Loss = 1.7368, Train Accuracy = 0.9235, Val Accuracy = 0.8560\n",
      "Epoch 110: Loss = 1.7393, Train Accuracy = 0.9163, Val Accuracy = 0.8691\n",
      "Epoch 111: Loss = 1.7402, Train Accuracy = 0.9137, Val Accuracy = 0.8691\n",
      "Epoch 112: Loss = 1.7383, Train Accuracy = 0.9207, Val Accuracy = 0.8508\n",
      "Epoch 113: Loss = 1.7413, Train Accuracy = 0.9114, Val Accuracy = 0.8521\n",
      "Epoch 114: Loss = 1.7400, Train Accuracy = 0.9150, Val Accuracy = 0.8626\n",
      "Epoch 115: Loss = 1.7395, Train Accuracy = 0.9156, Val Accuracy = 0.8547\n",
      "Epoch 116: Loss = 1.7374, Train Accuracy = 0.9208, Val Accuracy = 0.8455\n",
      "Epoch 117: Loss = 1.7347, Train Accuracy = 0.9273, Val Accuracy = 0.8560\n",
      "Epoch 118: Loss = 1.7355, Train Accuracy = 0.9251, Val Accuracy = 0.8586\n",
      "Epoch 119: Loss = 1.7366, Train Accuracy = 0.9229, Val Accuracy = 0.8469\n",
      "Epoch 120: Loss = 1.7352, Train Accuracy = 0.9255, Val Accuracy = 0.8495\n",
      "Epoch 121: Loss = 1.7352, Train Accuracy = 0.9270, Val Accuracy = 0.8469\n",
      "Epoch 122: Loss = 1.7348, Train Accuracy = 0.9267, Val Accuracy = 0.8560\n",
      "Epoch 123: Loss = 1.7347, Train Accuracy = 0.9270, Val Accuracy = 0.8560\n",
      "Epoch 124: Loss = 1.7338, Train Accuracy = 0.9291, Val Accuracy = 0.8599\n",
      "Epoch 125: Loss = 1.7324, Train Accuracy = 0.9344, Val Accuracy = 0.8599\n",
      "Epoch 126: Loss = 1.7325, Train Accuracy = 0.9325, Val Accuracy = 0.8560\n",
      "Epoch 127: Loss = 1.7359, Train Accuracy = 0.9236, Val Accuracy = 0.8429\n",
      "Epoch 128: Loss = 1.7331, Train Accuracy = 0.9331, Val Accuracy = 0.8521\n",
      "Epoch 129: Loss = 1.7323, Train Accuracy = 0.9334, Val Accuracy = 0.8482\n",
      "Epoch 130: Loss = 1.7332, Train Accuracy = 0.9322, Val Accuracy = 0.8573\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "# Load the data from a pandas DataFrame\n",
    "#df = pd.read_csv('Dataset_1.csv') \n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    df_train_3[col] = LabelEncoder().fit_transform(df_train_3[col]).astype(int)\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "#X = torch.tensor(torch.from_numpy(np.asarray(x_train, dtype=np.int64)), dtype=torch.int64)\n",
    "#y = torch.tensor(torch.from_numpy(np.asarray(y_train, dtype=bool)), dtype=torch.int64)\n",
    "y = torch.tensor(torch.from_numpy(np.asarray(y_train, dtype=bool)), dtype= torch.int64)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.tensor(torch.from_numpy(np.asarray(y_val, dtype=bool)), dtype=torch.int64)\n",
    "\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(130):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), 'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.84      1553\n",
      "           1       0.84      0.90      0.87      1720\n",
      "\n",
      "    accuracy                           0.86      3273\n",
      "   macro avg       0.86      0.85      0.86      3273\n",
      "weighted avg       0.86      0.86      0.86      3273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the testing function\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Get the predicted labels\n",
    "            #preds = torch.round(torch.sigmoid(outputs))\n",
    "            _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "            \n",
    "            # Collect the labels and predictions\n",
    "            all_labels += list(labels.numpy().reshape((-1,1)))\n",
    "            all_preds += list(preds.numpy().reshape((-1,1)))\n",
    "    \n",
    "    return np.asarray(all_labels), np.asarray(all_preds)\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features)\n",
    "\n",
    "# Load the trained weights\n",
    "net.load_state_dict(torch.load('trained_model.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# Create the testing dataset and data loader\n",
    "X = torch.from_numpy(np.asarray(x_test, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_test, dtype=np.int64))\n",
    "test_dataset = MyDataset(X, y)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net, test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'bruh\\r\\nCLASSIFICATION IS =============================================================\\r\\n[0]\\r\\n'\n",
      "b'bruh\\r\\nCLASSIFICATION IS =============================================================\\r\\n[1]\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "from subprocess import run\n",
    "Good_file = r\"D:\\win32diskimager-1.0.0-install.exe\"\n",
    "Bad_file = r\"D:\\hackSF\\filtered_dataset\\Win32_EXE\\186\"\n",
    "end_script = r\"D:\\ClassWork\\anti_virus\\Vigil-Anti\\Source\\EXEs\\run.py\"\n",
    "model_path= r\"D:\\ClassWork\\anti_virus\\Vigil-Anti\\Source\\EXEs\\models\\rf.pkl\"\n",
    "\n",
    "result1 = run(['python', end_script, Good_file, model_path], capture_output=True)\n",
    "resutl2 = run(['python', end_script, Bad_file, model_path], capture_output=True)\n",
    "print(result1.stdout)\n",
    "print(resutl2.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks suck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.neural_network import MLPClassifier\\n\\nMLP_classifier = MLPClassifier(hidden_layer_sizes=[120, 120, 30], solver=\\'sgd\\', alpha=1, random_state=1)\\n\\ndf_train_3 = df.copy()\\n\\nfor col in categorical_columns:\\n    df_train_3[col] = LabelEncoder().fit_transform(df_train_3[col])\\n\\nfeature_columns = list(df_train_3.columns)\\nfeature_columns.pop(feature_columns.index(\"label\"))\\n\\nx_train, x_test, y_train, y_test = train_test_split(df_train_3[feature_columns], df_train_3[\\'label\\'], test_size=0.3, shuffle=True)\\n\\nfor i in range(10):\\n    MLP_classifier.fit(x_train, y_train)\\n\\ny_pred = MLP_classifier.predict(x_test)\\nprint(classification_report(y_test, y_pred, zero_division=1))\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_classifier = MLPClassifier(hidden_layer_sizes=[120, 120, 30], solver='sgd', alpha=1, random_state=1)\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_train_3[col] = LabelEncoder().fit_transform(df_train_3[col])\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3[feature_columns], df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "for i in range(10):\n",
    "    MLP_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = MLP_classifier.predict(x_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
