{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from helpers import *\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swak\\AppData\\Local\\Temp\\ipykernel_9404\\52725572.py:1: DtypeWarning: Columns (228,302,307,309,316) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r'Dataset_Big.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMPORT_TABLE_size</th>\n",
       "      <th>strings_printabledist_31</th>\n",
       "      <th>strings_printabledist_88</th>\n",
       "      <th>.tls_vsize</th>\n",
       "      <th>subsystem</th>\n",
       "      <th>minor_image_version</th>\n",
       "      <th>RESOURCE_TABLE_virtual_address</th>\n",
       "      <th>.idata_vsize</th>\n",
       "      <th>.textbss_size</th>\n",
       "      <th>Dbgcore.dll</th>\n",
       "      <th>...</th>\n",
       "      <th>.idat_size</th>\n",
       "      <th>Libcrypto_num_funcs</th>\n",
       "      <th>mbedTLS</th>\n",
       "      <th>.txt_entropy</th>\n",
       "      <th>.UPX_props_len</th>\n",
       "      <th>CryptoAPI_num_funcs</th>\n",
       "      <th>.idat_props_len</th>\n",
       "      <th>.idat_vsize</th>\n",
       "      <th>.txt_size</th>\n",
       "      <th>.txt_vsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>858</td>\n",
       "      <td>965</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>0</td>\n",
       "      <td>352256</td>\n",
       "      <td>2370</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420</td>\n",
       "      <td>48</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>20512</td>\n",
       "      <td>483328</td>\n",
       "      <td>8192</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>116</td>\n",
       "      <td>152</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>0</td>\n",
       "      <td>90112</td>\n",
       "      <td>2242</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2370</td>\n",
       "      <td>613</td>\n",
       "      <td>653</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>0</td>\n",
       "      <td>65536</td>\n",
       "      <td>2384</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140</td>\n",
       "      <td>288</td>\n",
       "      <td>323</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>0</td>\n",
       "      <td>49152</td>\n",
       "      <td>2384</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IMPORT_TABLE_size  strings_printabledist_31  strings_printabledist_88   \n",
       "0                200                       858                       965  \\\n",
       "1                420                        48                       170   \n",
       "2                216                       116                       152   \n",
       "3               2370                       613                       653   \n",
       "4                140                       288                       323   \n",
       "\n",
       "   .tls_vsize    subsystem  minor_image_version   \n",
       "0           8  WINDOWS_GUI                    0  \\\n",
       "1           8  WINDOWS_GUI                20512   \n",
       "2           8  WINDOWS_GUI                    0   \n",
       "3           8  WINDOWS_GUI                    0   \n",
       "4           8  WINDOWS_GUI                    0   \n",
       "\n",
       "   RESOURCE_TABLE_virtual_address  .idata_vsize  .textbss_size  Dbgcore.dll   \n",
       "0                          352256          2370              0         True  \\\n",
       "1                          483328          8192              0         True   \n",
       "2                           90112          2242              0         True   \n",
       "3                           65536          2384              0         True   \n",
       "4                           49152          2384              0         True   \n",
       "\n",
       "   ...  .idat_size  Libcrypto_num_funcs  mbedTLS  .txt_entropy   \n",
       "0  ...         0.0                  0.0        0           0.0  \\\n",
       "1  ...         0.0                  0.0        0           0.0   \n",
       "2  ...         0.0                  0.0        0           0.0   \n",
       "3  ...         0.0                  0.0        0           0.0   \n",
       "4  ...         0.0                  0.0        0           0.0   \n",
       "\n",
       "   .UPX_props_len  CryptoAPI_num_funcs  .idat_props_len  .idat_vsize   \n",
       "0             0.0                  0.0              0.0          0.0  \\\n",
       "1             0.0                  0.0              0.0          0.0   \n",
       "2             0.0                  0.0              0.0          0.0   \n",
       "3             0.0                  0.0              0.0          0.0   \n",
       "4             0.0                  0.0              0.0          0.0   \n",
       "\n",
       "   .txt_size  .txt_vsize  \n",
       "0        0.0         0.0  \n",
       "1        0.0         0.0  \n",
       "2        0.0         0.0  \n",
       "3        0.0         0.0  \n",
       "4        0.0         0.0  \n",
       "\n",
       "[5 rows x 324 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'Dataset_Big.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of Features: 323\n"
     ]
    }
   ],
   "source": [
    "# Save our feature list\n",
    "feature_columns = list(df.columns)\n",
    "feature_columns.pop(feature_columns.index('label'))\n",
    "print(f\"total number of Features: {len(feature_columns)}\")\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'features.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_columns, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discard -1 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'suspicious_imports.txt'), 'r') as f:\n",
    "        sus_imports = f.readlines()\n",
    "sus_imports = [re.sub(r'\\n', '', i) for i in sus_imports]\n",
    "boolean_columns = sus_imports + []\n",
    "categorical_columns = [\"subsystem\", \"magic\", \"machine\"]\n",
    "\n",
    "array_of_Label_Encoders = []\n",
    "for col in categorical_columns:\n",
    "    new_LE = LabelEncoder().fit(df[col])\n",
    "    df[col] = new_LE.transform(df[col])\n",
    "    array_of_Label_Encoders.append(new_LE)\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "        if col in boolean_columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "            df[col].fillna(False)\n",
    "            continue\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'enc.pkl'), 'wb') as f:\n",
    "    pickle.dump(array_of_Label_Encoders, f)\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(),'models','enc.pkl'), 'rb') as f:\n",
    "        array_of_Label_Encoders = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   10.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   16.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  9.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     89740\n",
      "           1       0.97      0.95      0.96     90260\n",
      "\n",
      "    accuracy                           0.96    180000\n",
      "   macro avg       0.96      0.96      0.96    180000\n",
      "weighted avg       0.96      0.96      0.96    180000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    6.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  3.8min finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_train_2 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_2.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_2[feature_columns], df_train_2['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, verbose=4).fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "rf_model.fit(x_test, y_test)\n",
    "\n",
    "y_pred = rf_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'rf.pkl'), 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "df_train_1 = df.copy()\n",
    "\n",
    "feature_columns = list(df_train_1.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_1[feature_columns], df_train_1['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "svm_model = SVC(kernel='poly', degree= 3, verbose=3).fit(x_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "svm_model = SVC(kernel='poly', degree= 3, verbose=3).fit(x_test, y_test)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'svm.pkl'), 'wb') as f:\n",
    "    pickle.dump(svm_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (batch_norm1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense3): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x.float())\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = self.batch_norm2(x.float())\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = self.batch_norm3(x.float())\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "# Define the testing function\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs.to(device=device))\n",
    "            # Get the predicted labels\n",
    "            #preds = torch.round(torch.sigmoid(outputs))\n",
    "            _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "            \n",
    "            # Collect the labels and predictions\n",
    "            all_labels += list(labels.numpy().reshape((-1,1)))\n",
    "            all_preds += list(preds.numpy().reshape((-1,1)))\n",
    "    \n",
    "    return np.asarray(all_labels), np.asarray(all_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.9132, Train Accuracy = 0.6080, Val Accuracy = 0.6587\n",
      "Epoch 2: Loss = 1.8472, Train Accuracy = 0.6919, Val Accuracy = 0.7053\n",
      "Epoch 3: Loss = 1.8261, Train Accuracy = 0.7149, Val Accuracy = 0.7291\n",
      "Epoch 4: Loss = 1.8144, Train Accuracy = 0.7318, Val Accuracy = 0.7411\n",
      "Epoch 5: Loss = 1.8083, Train Accuracy = 0.7431, Val Accuracy = 0.7658\n",
      "Epoch 6: Loss = 1.7969, Train Accuracy = 0.7646, Val Accuracy = 0.7733\n",
      "Epoch 7: Loss = 1.7935, Train Accuracy = 0.7716, Val Accuracy = 0.7801\n",
      "Epoch 8: Loss = 1.7909, Train Accuracy = 0.7788, Val Accuracy = 0.7848\n",
      "Epoch 9: Loss = 1.7884, Train Accuracy = 0.7855, Val Accuracy = 0.7880\n",
      "Epoch 10: Loss = 1.7861, Train Accuracy = 0.7915, Val Accuracy = 0.8018\n",
      "Epoch 11: Loss = 1.7843, Train Accuracy = 0.8000, Val Accuracy = 0.8047\n",
      "Epoch 12: Loss = 1.7801, Train Accuracy = 0.8168, Val Accuracy = 0.8169\n",
      "Epoch 13: Loss = 1.7777, Train Accuracy = 0.8224, Val Accuracy = 0.8216\n",
      "Epoch 14: Loss = 1.7757, Train Accuracy = 0.8266, Val Accuracy = 0.8282\n",
      "Epoch 15: Loss = 1.7738, Train Accuracy = 0.8349, Val Accuracy = 0.8327\n",
      "Epoch 16: Loss = 1.7720, Train Accuracy = 0.8402, Val Accuracy = 0.8419\n",
      "Epoch 17: Loss = 1.7672, Train Accuracy = 0.8510, Val Accuracy = 0.8470\n",
      "Epoch 18: Loss = 1.7653, Train Accuracy = 0.8564, Val Accuracy = 0.8484\n",
      "Epoch 19: Loss = 1.7643, Train Accuracy = 0.8575, Val Accuracy = 0.8531\n",
      "Epoch 20: Loss = 1.7629, Train Accuracy = 0.8616, Val Accuracy = 0.8648\n",
      "Epoch 21: Loss = 1.7580, Train Accuracy = 0.8693, Val Accuracy = 0.8665\n",
      "Epoch 22: Loss = 1.7569, Train Accuracy = 0.8719, Val Accuracy = 0.8692\n",
      "Epoch 23: Loss = 1.7561, Train Accuracy = 0.8743, Val Accuracy = 0.8710\n",
      "Epoch 24: Loss = 1.7553, Train Accuracy = 0.8759, Val Accuracy = 0.8725\n",
      "Epoch 25: Loss = 1.7542, Train Accuracy = 0.8792, Val Accuracy = 0.8746\n",
      "Epoch 26: Loss = 1.7535, Train Accuracy = 0.8812, Val Accuracy = 0.8753\n",
      "Epoch 27: Loss = 1.7526, Train Accuracy = 0.8837, Val Accuracy = 0.8785\n",
      "Epoch 28: Loss = 1.7519, Train Accuracy = 0.8852, Val Accuracy = 0.8798\n",
      "Epoch 29: Loss = 1.7514, Train Accuracy = 0.8868, Val Accuracy = 0.8812\n",
      "Epoch 30: Loss = 1.7503, Train Accuracy = 0.8901, Val Accuracy = 0.8837\n",
      "Epoch 31: Loss = 1.7494, Train Accuracy = 0.8923, Val Accuracy = 0.8868\n",
      "Epoch 32: Loss = 1.7488, Train Accuracy = 0.8943, Val Accuracy = 0.8864\n",
      "Epoch 33: Loss = 1.7480, Train Accuracy = 0.8959, Val Accuracy = 0.8883\n",
      "Epoch 34: Loss = 1.7474, Train Accuracy = 0.8974, Val Accuracy = 0.8899\n",
      "Epoch 35: Loss = 1.7469, Train Accuracy = 0.8987, Val Accuracy = 0.8902\n",
      "Epoch 36: Loss = 1.7464, Train Accuracy = 0.8993, Val Accuracy = 0.8918\n",
      "Epoch 37: Loss = 1.7457, Train Accuracy = 0.9015, Val Accuracy = 0.8939\n",
      "Epoch 38: Loss = 1.7453, Train Accuracy = 0.9020, Val Accuracy = 0.8935\n",
      "Epoch 39: Loss = 1.7447, Train Accuracy = 0.9038, Val Accuracy = 0.8952\n",
      "Epoch 40: Loss = 1.7443, Train Accuracy = 0.9047, Val Accuracy = 0.8957\n",
      "Epoch 41: Loss = 1.7438, Train Accuracy = 0.9058, Val Accuracy = 0.8974\n",
      "Epoch 42: Loss = 1.7434, Train Accuracy = 0.9064, Val Accuracy = 0.8974\n",
      "Epoch 43: Loss = 1.7428, Train Accuracy = 0.9085, Val Accuracy = 0.8991\n",
      "Epoch 44: Loss = 1.7425, Train Accuracy = 0.9093, Val Accuracy = 0.8989\n",
      "Epoch 45: Loss = 1.7420, Train Accuracy = 0.9103, Val Accuracy = 0.8997\n",
      "Epoch 46: Loss = 1.7417, Train Accuracy = 0.9110, Val Accuracy = 0.9005\n",
      "Epoch 47: Loss = 1.7413, Train Accuracy = 0.9118, Val Accuracy = 0.9013\n",
      "Epoch 48: Loss = 1.7410, Train Accuracy = 0.9125, Val Accuracy = 0.9020\n",
      "Epoch 49: Loss = 1.7407, Train Accuracy = 0.9135, Val Accuracy = 0.9025\n",
      "Epoch 50: Loss = 1.7405, Train Accuracy = 0.9141, Val Accuracy = 0.9015\n",
      "Epoch 51: Loss = 1.7400, Train Accuracy = 0.9149, Val Accuracy = 0.9042\n",
      "Epoch 52: Loss = 1.7397, Train Accuracy = 0.9161, Val Accuracy = 0.9046\n",
      "Epoch 53: Loss = 1.7394, Train Accuracy = 0.9165, Val Accuracy = 0.9045\n",
      "Epoch 54: Loss = 1.7393, Train Accuracy = 0.9169, Val Accuracy = 0.9052\n",
      "Epoch 55: Loss = 1.7390, Train Accuracy = 0.9176, Val Accuracy = 0.9067\n",
      "Epoch 56: Loss = 1.7388, Train Accuracy = 0.9183, Val Accuracy = 0.9063\n",
      "Epoch 57: Loss = 1.7384, Train Accuracy = 0.9189, Val Accuracy = 0.9073\n",
      "Epoch 58: Loss = 1.7380, Train Accuracy = 0.9200, Val Accuracy = 0.9079\n",
      "Epoch 59: Loss = 1.7378, Train Accuracy = 0.9204, Val Accuracy = 0.9088\n",
      "Epoch 60: Loss = 1.7374, Train Accuracy = 0.9215, Val Accuracy = 0.9083\n",
      "Epoch 61: Loss = 1.7372, Train Accuracy = 0.9220, Val Accuracy = 0.9099\n",
      "Epoch 62: Loss = 1.7366, Train Accuracy = 0.9232, Val Accuracy = 0.9096\n",
      "Epoch 63: Loss = 1.7367, Train Accuracy = 0.9235, Val Accuracy = 0.9103\n",
      "Epoch 64: Loss = 1.7364, Train Accuracy = 0.9240, Val Accuracy = 0.9100\n",
      "Epoch 65: Loss = 1.7361, Train Accuracy = 0.9246, Val Accuracy = 0.9103\n",
      "Epoch 66: Loss = 1.7359, Train Accuracy = 0.9254, Val Accuracy = 0.9116\n",
      "Epoch 67: Loss = 1.7358, Train Accuracy = 0.9253, Val Accuracy = 0.9111\n",
      "Epoch 68: Loss = 1.7355, Train Accuracy = 0.9258, Val Accuracy = 0.9118\n",
      "Epoch 69: Loss = 1.7353, Train Accuracy = 0.9268, Val Accuracy = 0.9115\n",
      "Epoch 70: Loss = 1.7352, Train Accuracy = 0.9266, Val Accuracy = 0.9135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.81     89986\n",
      "           1       0.78      0.92      0.84     90014\n",
      "\n",
      "    accuracy                           0.83    180000\n",
      "   macro avg       0.84      0.83      0.82    180000\n",
      "weighted avg       0.84      0.83      0.82    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)#, weight_decay=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(70):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), 'trained_model.pt')\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN1.pkl'), 'wb') as f:\n",
    "     pickle.dump(net.cpu(), f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_test = torch.from_numpy(np.asarray(x_test, dtype=np.int64))\n",
    "Y_test = torch.from_numpy(np.asarray(y_test, dtype=bool)).type(torch.int64)\n",
    "\n",
    "\n",
    "test_dataset = MyDataset(X_test, Y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net.to(device=device), test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN2\n",
    "### same as base NN model, but without any batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (dense3): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.dense2 = nn.Linear(512, 128)\n",
    "        self.dense3 = nn.Linear(128, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.8663, Train Accuracy = 0.5118, Val Accuracy = 0.5036\n",
      "Epoch 2: Loss = 1.8545, Train Accuracy = 0.5008, Val Accuracy = 0.5037\n",
      "Epoch 3: Loss = 1.8537, Train Accuracy = 0.5008, Val Accuracy = 0.5037\n",
      "Epoch 4: Loss = 1.8534, Train Accuracy = 0.5009, Val Accuracy = 0.5038\n",
      "Epoch 5: Loss = 1.8533, Train Accuracy = 0.5010, Val Accuracy = 0.5039\n",
      "Epoch 6: Loss = 1.8532, Train Accuracy = 0.5280, Val Accuracy = 0.5337\n",
      "Epoch 7: Loss = 1.8532, Train Accuracy = 0.5356, Val Accuracy = 0.5332\n",
      "Epoch 8: Loss = 1.8531, Train Accuracy = 0.5354, Val Accuracy = 0.5332\n",
      "Epoch 9: Loss = 1.8531, Train Accuracy = 0.5355, Val Accuracy = 0.5333\n",
      "Epoch 10: Loss = 1.8530, Train Accuracy = 0.5355, Val Accuracy = 0.5333\n",
      "Epoch 11: Loss = 1.8530, Train Accuracy = 0.5356, Val Accuracy = 0.5333\n",
      "Epoch 12: Loss = 1.8530, Train Accuracy = 0.5356, Val Accuracy = 0.5334\n",
      "Epoch 13: Loss = 1.8530, Train Accuracy = 0.5356, Val Accuracy = 0.5334\n",
      "Epoch 14: Loss = 1.8530, Train Accuracy = 0.5356, Val Accuracy = 0.5333\n",
      "Epoch 15: Loss = 1.8530, Train Accuracy = 0.5356, Val Accuracy = 0.5334\n",
      "Epoch 16: Loss = 1.8529, Train Accuracy = 0.5356, Val Accuracy = 0.5334\n",
      "Epoch 17: Loss = 1.8529, Train Accuracy = 0.5357, Val Accuracy = 0.5334\n",
      "Epoch 18: Loss = 1.8529, Train Accuracy = 0.5357, Val Accuracy = 0.5334\n",
      "Epoch 19: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5334\n",
      "Epoch 20: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 21: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 22: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 23: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 24: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 25: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 26: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 27: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 28: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 29: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 30: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 31: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5335\n",
      "Epoch 32: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 33: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 34: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 35: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 36: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 37: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 38: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 39: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 40: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 41: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 42: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 43: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 44: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 45: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 46: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 47: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 48: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 49: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 50: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 51: Loss = 1.8528, Train Accuracy = 0.5359, Val Accuracy = 0.5336\n",
      "Epoch 52: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 53: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 54: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 55: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 56: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 57: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 58: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 59: Loss = 1.8528, Train Accuracy = 0.5359, Val Accuracy = 0.5336\n",
      "Epoch 60: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 61: Loss = 1.8528, Train Accuracy = 0.5359, Val Accuracy = 0.5336\n",
      "Epoch 62: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 63: Loss = 1.8528, Train Accuracy = 0.5359, Val Accuracy = 0.5336\n",
      "Epoch 64: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 65: Loss = 1.8528, Train Accuracy = 0.5359, Val Accuracy = 0.5336\n",
      "Epoch 66: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 67: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 68: Loss = 1.8528, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 69: Loss = 1.8529, Train Accuracy = 0.5358, Val Accuracy = 0.5336\n",
      "Epoch 70: Loss = 1.8528, Train Accuracy = 0.5359, Val Accuracy = 0.5336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.08      0.14     89984\n",
      "           1       0.52      0.99      0.68     90016\n",
      "\n",
      "    accuracy                           0.54    180000\n",
      "   macro avg       0.72      0.54      0.41    180000\n",
      "weighted avg       0.72      0.54      0.41    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64)).type(torch.float)\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64)).type(torch.float)\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)#, weight_decay=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(70):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN2.pkl'), 'wb') as f:\n",
    "     pickle.dump(net.cpu(), f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_test = torch.from_numpy(np.asarray(x_test, dtype=np.int64)).type(torch.float)\n",
    "Y_test = torch.from_numpy(np.asarray(y_test, dtype=bool)).type(torch.int64)\n",
    "\n",
    "\n",
    "test_dataset = MyDataset(X_test, Y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net.to(device=device), test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN3\n",
    "### same as base model, but with Leaky Relu instead of the tanh activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (batch_norm1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (lrelu1): LeakyReLU(negative_slope=0.01)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (lrelu2): LeakyReLU(negative_slope=0.01)\n",
      "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense3): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (lrelu3): LeakyReLU(negative_slope=0.01)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.lrelu1 = nn.LeakyReLU()\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 128)\n",
    "        self.lrelu2 = nn.LeakyReLU()\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, 8)\n",
    "        self.lrelu3 = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x.float())\n",
    "        x = self.lrelu1(self.dense1(x))\n",
    "        x = self.batch_norm2(x.float())\n",
    "        x = self.lrelu2(self.dense2(x))\n",
    "        x = self.batch_norm3(x.float())\n",
    "        x = self.lrelu3(self.dense3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.6602, Train Accuracy = 0.6768, Val Accuracy = 0.7649\n",
      "Epoch 2: Loss = 1.5088, Train Accuracy = 0.7807, Val Accuracy = 0.7947\n",
      "Epoch 3: Loss = 1.4764, Train Accuracy = 0.8043, Val Accuracy = 0.8059\n",
      "Epoch 4: Loss = 1.4607, Train Accuracy = 0.8155, Val Accuracy = 0.8147\n",
      "Epoch 5: Loss = 1.4512, Train Accuracy = 0.8227, Val Accuracy = 0.8183\n",
      "Epoch 6: Loss = 1.4452, Train Accuracy = 0.8275, Val Accuracy = 0.8230\n",
      "Epoch 7: Loss = 1.4406, Train Accuracy = 0.8314, Val Accuracy = 0.8251\n",
      "Epoch 8: Loss = 1.4365, Train Accuracy = 0.8349, Val Accuracy = 0.8290\n",
      "Epoch 9: Loss = 1.4293, Train Accuracy = 0.8433, Val Accuracy = 0.8506\n",
      "Epoch 10: Loss = 1.4135, Train Accuracy = 0.8616, Val Accuracy = 0.8542\n",
      "Epoch 11: Loss = 1.4105, Train Accuracy = 0.8637, Val Accuracy = 0.8558\n",
      "Epoch 12: Loss = 1.4088, Train Accuracy = 0.8657, Val Accuracy = 0.8573\n",
      "Epoch 13: Loss = 1.4062, Train Accuracy = 0.8683, Val Accuracy = 0.8599\n",
      "Epoch 14: Loss = 1.4039, Train Accuracy = 0.8705, Val Accuracy = 0.8609\n",
      "Epoch 15: Loss = 1.4018, Train Accuracy = 0.8724, Val Accuracy = 0.8633\n",
      "Epoch 16: Loss = 1.4000, Train Accuracy = 0.8745, Val Accuracy = 0.8638\n",
      "Epoch 17: Loss = 1.3986, Train Accuracy = 0.8756, Val Accuracy = 0.8635\n",
      "Epoch 18: Loss = 1.3967, Train Accuracy = 0.8774, Val Accuracy = 0.8657\n",
      "Epoch 19: Loss = 1.3953, Train Accuracy = 0.8789, Val Accuracy = 0.8683\n",
      "Epoch 20: Loss = 1.3939, Train Accuracy = 0.8804, Val Accuracy = 0.8692\n",
      "Epoch 21: Loss = 1.3922, Train Accuracy = 0.8821, Val Accuracy = 0.8713\n",
      "Epoch 22: Loss = 1.3906, Train Accuracy = 0.8836, Val Accuracy = 0.8714\n",
      "Epoch 23: Loss = 1.3892, Train Accuracy = 0.8850, Val Accuracy = 0.8723\n",
      "Epoch 24: Loss = 1.3882, Train Accuracy = 0.8860, Val Accuracy = 0.8731\n",
      "Epoch 25: Loss = 1.3873, Train Accuracy = 0.8870, Val Accuracy = 0.8714\n",
      "Epoch 26: Loss = 1.3858, Train Accuracy = 0.8886, Val Accuracy = 0.8715\n",
      "Epoch 27: Loss = 1.3843, Train Accuracy = 0.8900, Val Accuracy = 0.8744\n",
      "Epoch 28: Loss = 1.3827, Train Accuracy = 0.8919, Val Accuracy = 0.8754\n",
      "Epoch 29: Loss = 1.3818, Train Accuracy = 0.8925, Val Accuracy = 0.8781\n",
      "Epoch 30: Loss = 1.3807, Train Accuracy = 0.8936, Val Accuracy = 0.8769\n",
      "Epoch 31: Loss = 1.3792, Train Accuracy = 0.8953, Val Accuracy = 0.8796\n",
      "Epoch 32: Loss = 1.3786, Train Accuracy = 0.8958, Val Accuracy = 0.8807\n",
      "Epoch 33: Loss = 1.3779, Train Accuracy = 0.8965, Val Accuracy = 0.8796\n",
      "Epoch 34: Loss = 1.3761, Train Accuracy = 0.8983, Val Accuracy = 0.8812\n",
      "Epoch 35: Loss = 1.3752, Train Accuracy = 0.8994, Val Accuracy = 0.8814\n",
      "Epoch 36: Loss = 1.3745, Train Accuracy = 0.9000, Val Accuracy = 0.8822\n",
      "Epoch 37: Loss = 1.3730, Train Accuracy = 0.9017, Val Accuracy = 0.8848\n",
      "Epoch 38: Loss = 1.3728, Train Accuracy = 0.9018, Val Accuracy = 0.8849\n",
      "Epoch 39: Loss = 1.3719, Train Accuracy = 0.9025, Val Accuracy = 0.8847\n",
      "Epoch 40: Loss = 1.3707, Train Accuracy = 0.9035, Val Accuracy = 0.8850\n",
      "Epoch 41: Loss = 1.3705, Train Accuracy = 0.9040, Val Accuracy = 0.8860\n",
      "Epoch 42: Loss = 1.3694, Train Accuracy = 0.9051, Val Accuracy = 0.8882\n",
      "Epoch 43: Loss = 1.3687, Train Accuracy = 0.9058, Val Accuracy = 0.8868\n",
      "Epoch 44: Loss = 1.3676, Train Accuracy = 0.9070, Val Accuracy = 0.8880\n",
      "Epoch 45: Loss = 1.3667, Train Accuracy = 0.9079, Val Accuracy = 0.8876\n",
      "Epoch 46: Loss = 1.3659, Train Accuracy = 0.9087, Val Accuracy = 0.8878\n",
      "Epoch 47: Loss = 1.3653, Train Accuracy = 0.9092, Val Accuracy = 0.8895\n",
      "Epoch 48: Loss = 1.3652, Train Accuracy = 0.9093, Val Accuracy = 0.8887\n",
      "Epoch 49: Loss = 1.3638, Train Accuracy = 0.9110, Val Accuracy = 0.8905\n",
      "Epoch 50: Loss = 1.3632, Train Accuracy = 0.9114, Val Accuracy = 0.8916\n",
      "Epoch 51: Loss = 1.3633, Train Accuracy = 0.9114, Val Accuracy = 0.8907\n",
      "Epoch 52: Loss = 1.3623, Train Accuracy = 0.9123, Val Accuracy = 0.8911\n",
      "Epoch 53: Loss = 1.3615, Train Accuracy = 0.9132, Val Accuracy = 0.8916\n",
      "Epoch 54: Loss = 1.3609, Train Accuracy = 0.9140, Val Accuracy = 0.8927\n",
      "Epoch 55: Loss = 1.3600, Train Accuracy = 0.9146, Val Accuracy = 0.8923\n",
      "Epoch 56: Loss = 1.3596, Train Accuracy = 0.9150, Val Accuracy = 0.8931\n",
      "Epoch 57: Loss = 1.3591, Train Accuracy = 0.9155, Val Accuracy = 0.8927\n",
      "Epoch 58: Loss = 1.3583, Train Accuracy = 0.9164, Val Accuracy = 0.8890\n",
      "Epoch 59: Loss = 1.3580, Train Accuracy = 0.9167, Val Accuracy = 0.8939\n",
      "Epoch 60: Loss = 1.3572, Train Accuracy = 0.9175, Val Accuracy = 0.8936\n",
      "Epoch 61: Loss = 1.3573, Train Accuracy = 0.9172, Val Accuracy = 0.8953\n",
      "Epoch 62: Loss = 1.3564, Train Accuracy = 0.9183, Val Accuracy = 0.8957\n",
      "Epoch 63: Loss = 1.3562, Train Accuracy = 0.9185, Val Accuracy = 0.8959\n",
      "Epoch 64: Loss = 1.3556, Train Accuracy = 0.9191, Val Accuracy = 0.8943\n",
      "Epoch 65: Loss = 1.3557, Train Accuracy = 0.9190, Val Accuracy = 0.8954\n",
      "Epoch 66: Loss = 1.3551, Train Accuracy = 0.9197, Val Accuracy = 0.8966\n",
      "Epoch 67: Loss = 1.3537, Train Accuracy = 0.9208, Val Accuracy = 0.8955\n",
      "Epoch 68: Loss = 1.3539, Train Accuracy = 0.9209, Val Accuracy = 0.8963\n",
      "Epoch 69: Loss = 1.3531, Train Accuracy = 0.9217, Val Accuracy = 0.8977\n",
      "Epoch 70: Loss = 1.3526, Train Accuracy = 0.9220, Val Accuracy = 0.8962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84     90157\n",
      "           1       0.82      0.90      0.86     89843\n",
      "\n",
      "   micro avg       0.85      0.85      0.85    180000\n",
      "   macro avg       0.85      0.85      0.85    180000\n",
      "weighted avg       0.85      0.85      0.85    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)#, weight_decay=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(70):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), 'trained_model.pt')\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN3.pkl'), 'wb') as f:\n",
    "     pickle.dump(net.cpu(), f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_test = torch.from_numpy(np.asarray(x_test, dtype=np.int64)).type(torch.float)\n",
    "Y_test = torch.from_numpy(np.asarray(y_test, dtype=bool)).type(torch.int64)\n",
    "\n",
    "\n",
    "test_dataset = MyDataset(X_test, Y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net.to(device=device), test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN 4\n",
    "### same base model but with an extra dense layer and weight decay = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (batch_norm1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (batch_norm3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batch_norm4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense4): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 512)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.dense3 = nn.Linear(512, 128)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(128)\n",
    "        self.dense4 = nn.Linear(128, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x.float())\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = self.batch_norm2(x.float())\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = self.batch_norm3(x.float())\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = self.batch_norm4(x.float())\n",
    "        x = torch.tanh(self.dense4(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.8330, Train Accuracy = 0.7044, Val Accuracy = 0.7471\n",
      "Epoch 2: Loss = 1.7979, Train Accuracy = 0.7688, Val Accuracy = 0.7876\n",
      "Epoch 3: Loss = 1.7748, Train Accuracy = 0.8228, Val Accuracy = 0.8368\n",
      "Epoch 4: Loss = 1.7629, Train Accuracy = 0.8575, Val Accuracy = 0.8610\n",
      "Epoch 5: Loss = 1.7569, Train Accuracy = 0.8720, Val Accuracy = 0.8718\n",
      "Epoch 6: Loss = 1.7525, Train Accuracy = 0.8826, Val Accuracy = 0.8816\n",
      "Epoch 7: Loss = 1.7485, Train Accuracy = 0.8915, Val Accuracy = 0.8854\n",
      "Epoch 8: Loss = 1.7460, Train Accuracy = 0.8975, Val Accuracy = 0.8955\n",
      "Epoch 9: Loss = 1.7442, Train Accuracy = 0.9021, Val Accuracy = 0.8980\n",
      "Epoch 10: Loss = 1.7424, Train Accuracy = 0.9062, Val Accuracy = 0.9013\n",
      "Epoch 11: Loss = 1.7413, Train Accuracy = 0.9088, Val Accuracy = 0.9013\n",
      "Epoch 12: Loss = 1.7397, Train Accuracy = 0.9129, Val Accuracy = 0.9074\n",
      "Epoch 13: Loss = 1.7387, Train Accuracy = 0.9154, Val Accuracy = 0.9079\n",
      "Epoch 14: Loss = 1.7373, Train Accuracy = 0.9193, Val Accuracy = 0.9102\n",
      "Epoch 15: Loss = 1.7365, Train Accuracy = 0.9209, Val Accuracy = 0.9111\n",
      "Epoch 16: Loss = 1.7355, Train Accuracy = 0.9228, Val Accuracy = 0.9138\n",
      "Epoch 17: Loss = 1.7351, Train Accuracy = 0.9242, Val Accuracy = 0.9157\n",
      "Epoch 18: Loss = 1.7340, Train Accuracy = 0.9267, Val Accuracy = 0.9165\n",
      "Epoch 19: Loss = 1.7335, Train Accuracy = 0.9279, Val Accuracy = 0.9156\n",
      "Epoch 20: Loss = 1.7331, Train Accuracy = 0.9289, Val Accuracy = 0.9192\n",
      "Epoch 21: Loss = 1.7325, Train Accuracy = 0.9302, Val Accuracy = 0.9189\n",
      "Epoch 22: Loss = 1.7319, Train Accuracy = 0.9319, Val Accuracy = 0.9209\n",
      "Epoch 23: Loss = 1.7314, Train Accuracy = 0.9333, Val Accuracy = 0.9217\n",
      "Epoch 24: Loss = 1.7311, Train Accuracy = 0.9338, Val Accuracy = 0.9229\n",
      "Epoch 25: Loss = 1.7305, Train Accuracy = 0.9349, Val Accuracy = 0.9245\n",
      "Epoch 26: Loss = 1.7304, Train Accuracy = 0.9354, Val Accuracy = 0.9231\n",
      "Epoch 27: Loss = 1.7301, Train Accuracy = 0.9362, Val Accuracy = 0.9251\n",
      "Epoch 28: Loss = 1.7295, Train Accuracy = 0.9375, Val Accuracy = 0.9249\n",
      "Epoch 29: Loss = 1.7294, Train Accuracy = 0.9382, Val Accuracy = 0.9241\n",
      "Epoch 30: Loss = 1.7290, Train Accuracy = 0.9389, Val Accuracy = 0.9248\n",
      "Epoch 31: Loss = 1.7287, Train Accuracy = 0.9397, Val Accuracy = 0.9258\n",
      "Epoch 32: Loss = 1.7282, Train Accuracy = 0.9406, Val Accuracy = 0.9267\n",
      "Epoch 33: Loss = 1.7282, Train Accuracy = 0.9411, Val Accuracy = 0.9276\n",
      "Epoch 34: Loss = 1.7275, Train Accuracy = 0.9425, Val Accuracy = 0.9278\n",
      "Epoch 35: Loss = 1.7271, Train Accuracy = 0.9431, Val Accuracy = 0.9282\n",
      "Epoch 36: Loss = 1.7267, Train Accuracy = 0.9442, Val Accuracy = 0.9301\n",
      "Epoch 37: Loss = 1.7266, Train Accuracy = 0.9446, Val Accuracy = 0.9284\n",
      "Epoch 38: Loss = 1.7263, Train Accuracy = 0.9453, Val Accuracy = 0.9272\n",
      "Epoch 39: Loss = 1.7261, Train Accuracy = 0.9461, Val Accuracy = 0.9298\n",
      "Epoch 40: Loss = 1.7261, Train Accuracy = 0.9460, Val Accuracy = 0.9297\n",
      "Epoch 41: Loss = 1.7259, Train Accuracy = 0.9464, Val Accuracy = 0.9296\n",
      "Epoch 42: Loss = 1.7257, Train Accuracy = 0.9469, Val Accuracy = 0.9290\n",
      "Epoch 43: Loss = 1.7255, Train Accuracy = 0.9476, Val Accuracy = 0.9320\n",
      "Epoch 44: Loss = 1.7256, Train Accuracy = 0.9472, Val Accuracy = 0.9311\n",
      "Epoch 45: Loss = 1.7251, Train Accuracy = 0.9482, Val Accuracy = 0.9316\n",
      "Epoch 46: Loss = 1.7249, Train Accuracy = 0.9490, Val Accuracy = 0.9320\n",
      "Epoch 47: Loss = 1.7249, Train Accuracy = 0.9487, Val Accuracy = 0.9315\n",
      "Epoch 48: Loss = 1.7248, Train Accuracy = 0.9490, Val Accuracy = 0.9313\n",
      "Epoch 49: Loss = 1.7243, Train Accuracy = 0.9503, Val Accuracy = 0.9335\n",
      "Epoch 50: Loss = 1.7240, Train Accuracy = 0.9509, Val Accuracy = 0.9319\n",
      "Epoch 51: Loss = 1.7241, Train Accuracy = 0.9506, Val Accuracy = 0.9324\n",
      "Epoch 52: Loss = 1.7238, Train Accuracy = 0.9517, Val Accuracy = 0.9336\n",
      "Epoch 53: Loss = 1.7238, Train Accuracy = 0.9514, Val Accuracy = 0.9317\n",
      "Epoch 54: Loss = 1.7236, Train Accuracy = 0.9519, Val Accuracy = 0.9327\n",
      "Epoch 55: Loss = 1.7234, Train Accuracy = 0.9523, Val Accuracy = 0.9316\n",
      "Epoch 56: Loss = 1.7237, Train Accuracy = 0.9516, Val Accuracy = 0.9320\n",
      "Epoch 57: Loss = 1.7234, Train Accuracy = 0.9523, Val Accuracy = 0.9318\n",
      "Epoch 58: Loss = 1.7230, Train Accuracy = 0.9533, Val Accuracy = 0.9353\n",
      "Epoch 59: Loss = 1.7232, Train Accuracy = 0.9527, Val Accuracy = 0.9332\n",
      "Epoch 60: Loss = 1.7229, Train Accuracy = 0.9538, Val Accuracy = 0.9339\n",
      "Epoch 61: Loss = 1.7228, Train Accuracy = 0.9539, Val Accuracy = 0.9336\n",
      "Epoch 62: Loss = 1.7227, Train Accuracy = 0.9544, Val Accuracy = 0.9328\n",
      "Epoch 63: Loss = 1.7226, Train Accuracy = 0.9543, Val Accuracy = 0.9318\n",
      "Epoch 64: Loss = 1.7224, Train Accuracy = 0.9547, Val Accuracy = 0.9348\n",
      "Epoch 65: Loss = 1.7221, Train Accuracy = 0.9555, Val Accuracy = 0.9320\n",
      "Epoch 66: Loss = 1.7221, Train Accuracy = 0.9555, Val Accuracy = 0.9341\n",
      "Epoch 67: Loss = 1.7220, Train Accuracy = 0.9558, Val Accuracy = 0.9359\n",
      "Epoch 68: Loss = 1.7221, Train Accuracy = 0.9553, Val Accuracy = 0.9338\n",
      "Epoch 69: Loss = 1.7218, Train Accuracy = 0.9563, Val Accuracy = 0.9354\n",
      "Epoch 70: Loss = 1.7219, Train Accuracy = 0.9560, Val Accuracy = 0.9337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.69      0.79     89886\n",
      "           1       0.75      0.94      0.84     90114\n",
      "\n",
      "    accuracy                           0.82    180000\n",
      "   macro avg       0.84      0.82      0.81    180000\n",
      "weighted avg       0.84      0.82      0.81    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.000001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(70):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), 'trained_model.pt')\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN4.pkl'), 'wb') as f:\n",
    "     pickle.dump(net.cpu(), f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_test = torch.from_numpy(np.asarray(x_test, dtype=np.int64)).type(torch.float)\n",
    "Y_test = torch.from_numpy(np.asarray(y_test, dtype=bool)).type(torch.int64)\n",
    "\n",
    "\n",
    "test_dataset = MyDataset(X_test, Y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net.to(device=device), test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN 5\n",
    "### same as NN 4 but with bigger learning rate, and with weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.8401, Train Accuracy = 0.6794, Val Accuracy = 0.7226\n",
      "Epoch 2: Loss = 1.7965, Train Accuracy = 0.7662, Val Accuracy = 0.7916\n",
      "Epoch 3: Loss = 1.7766, Train Accuracy = 0.8184, Val Accuracy = 0.8360\n",
      "Epoch 4: Loss = 1.7633, Train Accuracy = 0.8564, Val Accuracy = 0.8625\n",
      "Epoch 5: Loss = 1.7563, Train Accuracy = 0.8748, Val Accuracy = 0.8749\n",
      "Epoch 6: Loss = 1.7523, Train Accuracy = 0.8844, Val Accuracy = 0.8814\n",
      "Epoch 7: Loss = 1.7489, Train Accuracy = 0.8924, Val Accuracy = 0.8905\n",
      "Epoch 8: Loss = 1.7460, Train Accuracy = 0.8998, Val Accuracy = 0.8955\n",
      "Epoch 9: Loss = 1.7445, Train Accuracy = 0.9032, Val Accuracy = 0.8968\n",
      "Epoch 10: Loss = 1.7428, Train Accuracy = 0.9066, Val Accuracy = 0.9001\n",
      "Epoch 11: Loss = 1.7406, Train Accuracy = 0.9110, Val Accuracy = 0.9046\n",
      "Epoch 12: Loss = 1.7390, Train Accuracy = 0.9146, Val Accuracy = 0.9065\n",
      "Epoch 13: Loss = 1.7378, Train Accuracy = 0.9181, Val Accuracy = 0.9100\n",
      "Epoch 14: Loss = 1.7371, Train Accuracy = 0.9198, Val Accuracy = 0.9100\n",
      "Epoch 15: Loss = 1.7362, Train Accuracy = 0.9216, Val Accuracy = 0.9142\n",
      "Epoch 16: Loss = 1.7356, Train Accuracy = 0.9231, Val Accuracy = 0.9131\n",
      "Epoch 17: Loss = 1.7350, Train Accuracy = 0.9244, Val Accuracy = 0.9146\n",
      "Epoch 18: Loss = 1.7344, Train Accuracy = 0.9257, Val Accuracy = 0.9163\n",
      "Epoch 19: Loss = 1.7335, Train Accuracy = 0.9280, Val Accuracy = 0.9173\n",
      "Epoch 20: Loss = 1.7330, Train Accuracy = 0.9291, Val Accuracy = 0.9185\n",
      "Epoch 21: Loss = 1.7326, Train Accuracy = 0.9303, Val Accuracy = 0.9167\n",
      "Epoch 22: Loss = 1.7320, Train Accuracy = 0.9318, Val Accuracy = 0.9140\n",
      "Epoch 23: Loss = 1.7316, Train Accuracy = 0.9330, Val Accuracy = 0.9212\n",
      "Epoch 24: Loss = 1.7313, Train Accuracy = 0.9329, Val Accuracy = 0.9213\n",
      "Epoch 25: Loss = 1.7309, Train Accuracy = 0.9340, Val Accuracy = 0.9227\n",
      "Epoch 26: Loss = 1.7304, Train Accuracy = 0.9357, Val Accuracy = 0.9227\n",
      "Epoch 27: Loss = 1.7300, Train Accuracy = 0.9366, Val Accuracy = 0.9234\n",
      "Epoch 28: Loss = 1.7298, Train Accuracy = 0.9370, Val Accuracy = 0.9235\n",
      "Epoch 29: Loss = 1.7294, Train Accuracy = 0.9383, Val Accuracy = 0.9245\n",
      "Epoch 30: Loss = 1.7293, Train Accuracy = 0.9381, Val Accuracy = 0.9238\n",
      "Epoch 31: Loss = 1.7288, Train Accuracy = 0.9392, Val Accuracy = 0.9232\n",
      "Epoch 32: Loss = 1.7285, Train Accuracy = 0.9400, Val Accuracy = 0.9242\n",
      "Epoch 33: Loss = 1.7284, Train Accuracy = 0.9407, Val Accuracy = 0.9239\n",
      "Epoch 34: Loss = 1.7281, Train Accuracy = 0.9413, Val Accuracy = 0.9273\n",
      "Epoch 35: Loss = 1.7278, Train Accuracy = 0.9416, Val Accuracy = 0.9268\n",
      "Epoch 36: Loss = 1.7275, Train Accuracy = 0.9422, Val Accuracy = 0.9249\n",
      "Epoch 37: Loss = 1.7272, Train Accuracy = 0.9435, Val Accuracy = 0.9276\n",
      "Epoch 38: Loss = 1.7270, Train Accuracy = 0.9435, Val Accuracy = 0.9261\n",
      "Epoch 39: Loss = 1.7265, Train Accuracy = 0.9449, Val Accuracy = 0.9281\n",
      "Epoch 40: Loss = 1.7265, Train Accuracy = 0.9453, Val Accuracy = 0.9280\n",
      "Epoch 41: Loss = 1.7265, Train Accuracy = 0.9449, Val Accuracy = 0.9282\n",
      "Epoch 42: Loss = 1.7259, Train Accuracy = 0.9461, Val Accuracy = 0.9288\n",
      "Epoch 43: Loss = 1.7261, Train Accuracy = 0.9461, Val Accuracy = 0.9278\n",
      "Epoch 44: Loss = 1.7257, Train Accuracy = 0.9464, Val Accuracy = 0.9287\n",
      "Epoch 45: Loss = 1.7257, Train Accuracy = 0.9469, Val Accuracy = 0.9299\n",
      "Epoch 46: Loss = 1.7253, Train Accuracy = 0.9479, Val Accuracy = 0.9281\n",
      "Epoch 47: Loss = 1.7251, Train Accuracy = 0.9482, Val Accuracy = 0.9298\n",
      "Epoch 48: Loss = 1.7250, Train Accuracy = 0.9483, Val Accuracy = 0.9302\n",
      "Epoch 49: Loss = 1.7247, Train Accuracy = 0.9491, Val Accuracy = 0.9281\n",
      "Epoch 50: Loss = 1.7247, Train Accuracy = 0.9493, Val Accuracy = 0.9292\n",
      "Epoch 51: Loss = 1.7247, Train Accuracy = 0.9491, Val Accuracy = 0.9297\n",
      "Epoch 52: Loss = 1.7244, Train Accuracy = 0.9499, Val Accuracy = 0.9304\n",
      "Epoch 53: Loss = 1.7242, Train Accuracy = 0.9505, Val Accuracy = 0.9325\n",
      "Epoch 54: Loss = 1.7243, Train Accuracy = 0.9504, Val Accuracy = 0.9300\n",
      "Epoch 55: Loss = 1.7241, Train Accuracy = 0.9508, Val Accuracy = 0.9300\n",
      "Epoch 56: Loss = 1.7242, Train Accuracy = 0.9503, Val Accuracy = 0.9320\n",
      "Epoch 57: Loss = 1.7240, Train Accuracy = 0.9509, Val Accuracy = 0.9328\n",
      "Epoch 58: Loss = 1.7235, Train Accuracy = 0.9522, Val Accuracy = 0.9330\n",
      "Epoch 59: Loss = 1.7235, Train Accuracy = 0.9522, Val Accuracy = 0.9320\n",
      "Epoch 60: Loss = 1.7234, Train Accuracy = 0.9525, Val Accuracy = 0.9329\n",
      "Epoch 61: Loss = 1.7233, Train Accuracy = 0.9525, Val Accuracy = 0.9307\n",
      "Epoch 62: Loss = 1.7231, Train Accuracy = 0.9531, Val Accuracy = 0.9309\n",
      "Epoch 63: Loss = 1.7231, Train Accuracy = 0.9529, Val Accuracy = 0.9337\n",
      "Epoch 64: Loss = 1.7229, Train Accuracy = 0.9534, Val Accuracy = 0.9324\n",
      "Epoch 65: Loss = 1.7231, Train Accuracy = 0.9531, Val Accuracy = 0.9324\n",
      "Epoch 66: Loss = 1.7226, Train Accuracy = 0.9543, Val Accuracy = 0.9324\n",
      "Epoch 67: Loss = 1.7227, Train Accuracy = 0.9541, Val Accuracy = 0.9331\n",
      "Epoch 68: Loss = 1.7226, Train Accuracy = 0.9543, Val Accuracy = 0.9314\n",
      "Epoch 69: Loss = 1.7223, Train Accuracy = 0.9549, Val Accuracy = 0.9332\n",
      "Epoch 70: Loss = 1.7224, Train Accuracy = 0.9547, Val Accuracy = 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.78      0.83     90234\n",
      "           1       0.80      0.90      0.85     89766\n",
      "\n",
      "    accuracy                           0.84    180000\n",
      "   macro avg       0.85      0.84      0.84    180000\n",
      "weighted avg       0.85      0.84      0.84    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.000001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(70):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), 'trained_model.pt')\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN5.pkl'), 'wb') as f:\n",
    "     pickle.dump(net.cpu(), f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_test = torch.from_numpy(np.asarray(x_test, dtype=np.int64)).type(torch.float)\n",
    "Y_test = torch.from_numpy(np.asarray(y_test, dtype=bool)).type(torch.int64)\n",
    "\n",
    "\n",
    "test_dataset = MyDataset(X_test, Y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net.to(device=device), test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3424           85.44m\n",
      "         2           1.3418           85.66m\n",
      "         3           1.3411           84.88m\n",
      "         4           1.3405           83.99m\n",
      "         5           1.3399           83.88m\n",
      "         6           1.3393           83.56m\n",
      "         7           1.3386           82.23m\n",
      "         8           1.3380           81.24m\n",
      "         9           1.3374           80.36m\n",
      "        10           1.3368           79.67m\n",
      "        11           1.3362           79.12m\n",
      "        12           1.3355           78.43m\n",
      "        13           1.3349           77.83m\n",
      "        14           1.3343           77.34m\n",
      "        15           1.3337           77.19m\n",
      "        16           1.3331           77.03m\n",
      "        17           1.3325           76.94m\n",
      "        18           1.3319           76.78m\n",
      "        19           1.3312           76.69m\n",
      "        20           1.3306           76.32m\n",
      "        21           1.3300           76.22m\n",
      "        22           1.3294           76.10m\n",
      "        23           1.3288           75.92m\n",
      "        24           1.3282           75.68m\n",
      "        25           1.3276           75.49m\n",
      "        26           1.3270           75.28m\n",
      "        27           1.3264           75.15m\n",
      "        28           1.3258           74.94m\n",
      "        29           1.3252           74.82m\n",
      "        30           1.3246           74.65m\n",
      "        31           1.3240           74.36m\n",
      "        32           1.3234           74.05m\n",
      "        33           1.3228           73.78m\n",
      "        34           1.3222           73.60m\n",
      "        35           1.3216           73.40m\n",
      "        36           1.3211           73.10m\n",
      "        37           1.3205           72.86m\n",
      "        38           1.3199           72.56m\n",
      "        39           1.3193           72.25m\n",
      "        40           1.3187           72.00m\n",
      "        41           1.3181           71.65m\n",
      "        42           1.3175           71.38m\n",
      "        43           1.3170           71.02m\n",
      "        44           1.3164           70.67m\n",
      "        45           1.3158           70.39m\n",
      "        46           1.3152           70.13m\n",
      "        47           1.3146           69.89m\n",
      "        48           1.3141           69.56m\n",
      "        49           1.3135           69.20m\n",
      "        50           1.3129           68.93m\n",
      "        51           1.3123           68.64m\n",
      "        52           1.3118           68.31m\n",
      "        53           1.3112           68.03m\n",
      "        54           1.3106           67.71m\n",
      "        55           1.3100           67.42m\n",
      "        56           1.3095           67.17m\n",
      "        57           1.3089           66.89m\n",
      "        58           1.3083           66.69m\n",
      "        59           1.3078           66.45m\n",
      "        60           1.3072           66.16m\n",
      "        61           1.3067           65.91m\n",
      "        62           1.3061           65.65m\n",
      "        63           1.3055           65.35m\n",
      "        64           1.3050           65.09m\n",
      "        65           1.3044           64.74m\n",
      "        66           1.3039           64.43m\n",
      "        67           1.3033           64.12m\n",
      "        68           1.3027           63.81m\n",
      "        69           1.3022           63.48m\n",
      "        70           1.3016           63.18m\n",
      "        71           1.3011           62.87m\n",
      "        72           1.3005           62.56m\n",
      "        73           1.3000           62.27m\n",
      "        74           1.2994           61.99m\n",
      "        75           1.2989           61.70m\n",
      "        76           1.2983           61.35m\n",
      "        77           1.2978           61.05m\n",
      "        78           1.2972           60.70m\n",
      "        79           1.2967           60.38m\n",
      "        80           1.2961           60.07m\n",
      "        81           1.2956           59.75m\n",
      "        82           1.2950           59.47m\n",
      "        83           1.2945           59.19m\n",
      "        84           1.2940           58.89m\n",
      "        85           1.2934           58.59m\n",
      "        86           1.2929           58.26m\n",
      "        87           1.2923           57.93m\n",
      "        88           1.2918           57.62m\n",
      "        89           1.2912           57.34m\n",
      "        90           1.2907           57.01m\n",
      "        91           1.2902           56.74m\n",
      "        92           1.2896           56.45m\n",
      "        93           1.2891           56.16m\n",
      "        94           1.2886           55.87m\n",
      "        95           1.2880           55.61m\n",
      "        96           1.2875           55.32m\n",
      "        97           1.2870           55.07m\n",
      "        98           1.2864           54.78m\n",
      "        99           1.2859           54.47m\n",
      "       100           1.2854           54.19m\n",
      "       101           1.2848           53.91m\n",
      "       102           1.2843           53.62m\n",
      "       103           1.2838           53.34m\n",
      "       104           1.2832           53.04m\n",
      "       105           1.2827           52.75m\n",
      "       106           1.2822           52.46m\n",
      "       107           1.2817           52.17m\n",
      "       108           1.2811           51.89m\n",
      "       109           1.2806           51.62m\n",
      "       110           1.2801           51.35m\n",
      "       111           1.2796           51.07m\n",
      "       112           1.2791           50.80m\n",
      "       113           1.2785           50.50m\n",
      "       114           1.2780           50.23m\n",
      "       115           1.2775           49.94m\n",
      "       116           1.2770           49.65m\n",
      "       117           1.2765           49.37m\n",
      "       118           1.2760           49.07m\n",
      "       119           1.2754           48.80m\n",
      "       120           1.2749           48.49m\n",
      "       121           1.2744           48.18m\n",
      "       122           1.2739           47.87m\n",
      "       123           1.2734           47.58m\n",
      "       124           1.2729           47.32m\n",
      "       125           1.2724           47.02m\n",
      "       126           1.2719           46.71m\n",
      "       127           1.2714           46.40m\n",
      "       128           1.2708           46.09m\n",
      "       129           1.2703           45.79m\n",
      "       130           1.2698           45.50m\n",
      "       131           1.2693           45.20m\n",
      "       132           1.2688           44.90m\n",
      "       133           1.2683           44.61m\n",
      "       134           1.2678           44.31m\n",
      "       135           1.2673           44.01m\n",
      "       136           1.2668           43.71m\n",
      "       137           1.2663           43.41m\n",
      "       138           1.2658           43.13m\n",
      "       139           1.2653           42.85m\n",
      "       140           1.2648           42.55m\n",
      "       141           1.2643           42.26m\n",
      "       142           1.2638           41.97m\n",
      "       143           1.2633           41.68m\n",
      "       144           1.2628           41.39m\n",
      "       145           1.2624           41.10m\n",
      "       146           1.2619           40.81m\n",
      "       147           1.2614           40.51m\n",
      "       148           1.2609           40.22m\n",
      "       149           1.2604           39.92m\n",
      "       150           1.2599           39.62m\n",
      "       151           1.2594           39.32m\n",
      "       152           1.2589           39.02m\n",
      "       153           1.2584           38.72m\n",
      "       154           1.2579           38.42m\n",
      "       155           1.2574           38.12m\n",
      "       156           1.2569           37.82m\n",
      "       157           1.2564           37.52m\n",
      "       158           1.2560           37.26m\n",
      "       159           1.2555           36.98m\n",
      "       160           1.2550           36.70m\n",
      "       161           1.2545           36.44m\n",
      "       162           1.2540           36.16m\n",
      "       163           1.2535           35.89m\n",
      "       164           1.2530           35.62m\n",
      "       165           1.2525           35.34m\n",
      "       166           1.2521           35.06m\n",
      "       167           1.2516           34.77m\n",
      "       168           1.2511           34.49m\n",
      "       169           1.2506           34.22m\n",
      "       170           1.2501           33.94m\n",
      "       171           1.2497           33.67m\n",
      "       172           1.2492           33.40m\n",
      "       173           1.2487           33.12m\n",
      "       174           1.2482           32.84m\n",
      "       175           1.2477           32.57m\n",
      "       176           1.2473           32.30m\n",
      "       177           1.2468           32.02m\n",
      "       178           1.2463           31.75m\n",
      "       179           1.2458           31.48m\n",
      "       180           1.2454           31.21m\n",
      "       181           1.2449           30.94m\n",
      "       182           1.2444           30.67m\n",
      "       183           1.2439           30.40m\n",
      "       184           1.2435           30.12m\n",
      "       185           1.2430           29.85m\n",
      "       186           1.2425           29.57m\n",
      "       187           1.2421           29.29m\n",
      "       188           1.2416           29.02m\n",
      "       189           1.2411           28.75m\n",
      "       190           1.2407           28.48m\n",
      "       191           1.2402           28.21m\n",
      "       192           1.2397           27.95m\n",
      "       193           1.2393           27.68m\n",
      "       194           1.2388           27.41m\n",
      "       195           1.2384           27.13m\n",
      "       196           1.2379           26.86m\n",
      "       197           1.2374           26.59m\n",
      "       198           1.2370           26.31m\n",
      "       199           1.2365           26.05m\n",
      "       200           1.2361           25.79m\n",
      "       201           1.2356           25.52m\n",
      "       202           1.2351           25.25m\n",
      "       203           1.2347           24.99m\n",
      "       204           1.2342           24.73m\n",
      "       205           1.2338           24.48m\n",
      "       206           1.2333           24.21m\n",
      "       207           1.2329           23.95m\n",
      "       208           1.2324           23.68m\n",
      "       209           1.2320           23.42m\n",
      "       210           1.2315           23.15m\n",
      "       211           1.2311           22.89m\n",
      "       212           1.2306           22.62m\n",
      "       213           1.2302           22.36m\n",
      "       214           1.2297           22.10m\n",
      "       215           1.2293           21.83m\n",
      "       216           1.2288           21.57m\n",
      "       217           1.2284           21.30m\n",
      "       218           1.2279           21.04m\n",
      "       219           1.2275           20.78m\n",
      "       220           1.2270           20.52m\n",
      "       221           1.2266           20.25m\n",
      "       222           1.2261           19.99m\n",
      "       223           1.2257           19.73m\n",
      "       224           1.2252           19.47m\n",
      "       225           1.2248           19.21m\n",
      "       226           1.2244           18.95m\n",
      "       227           1.2239           18.69m\n",
      "       228           1.2235           18.43m\n",
      "       229           1.2230           18.17m\n",
      "       230           1.2226           17.91m\n",
      "       231           1.2222           17.65m\n",
      "       232           1.2217           17.39m\n",
      "       233           1.2213           17.13m\n",
      "       234           1.2208           16.86m\n",
      "       235           1.2204           16.60m\n",
      "       236           1.2200           16.34m\n",
      "       237           1.2195           16.07m\n",
      "       238           1.2191           15.81m\n",
      "       239           1.2187           15.55m\n",
      "       240           1.2182           15.29m\n",
      "       241           1.2178           15.02m\n",
      "       242           1.2174           14.76m\n",
      "       243           1.2169           14.50m\n",
      "       244           1.2165           14.24m\n",
      "       245           1.2161           13.98m\n",
      "       246           1.2157           13.72m\n",
      "       247           1.2152           13.46m\n",
      "       248           1.2148           13.20m\n",
      "       249           1.2144           12.94m\n",
      "       250           1.2140           12.68m\n",
      "       251           1.2135           12.43m\n",
      "       252           1.2131           12.17m\n",
      "       253           1.2127           11.92m\n",
      "       254           1.2123           11.66m\n",
      "       255           1.2118           11.40m\n",
      "       256           1.2114           11.15m\n",
      "       257           1.2110           10.89m\n",
      "       258           1.2106           10.63m\n",
      "       259           1.2101           10.38m\n",
      "       260           1.2097           10.12m\n",
      "       261           1.2093            9.86m\n",
      "       262           1.2089            9.61m\n",
      "       263           1.2085            9.35m\n",
      "       264           1.2081            9.09m\n",
      "       265           1.2076            8.84m\n",
      "       266           1.2072            8.58m\n",
      "       267           1.2068            8.33m\n",
      "       268           1.2064            8.07m\n",
      "       269           1.2060            7.82m\n",
      "       270           1.2056            7.56m\n",
      "       271           1.2051            7.31m\n",
      "       272           1.2047            7.05m\n",
      "       273           1.2043            6.80m\n",
      "       274           1.2039            6.54m\n",
      "       275           1.2035            6.29m\n",
      "       276           1.2031            6.04m\n",
      "       277           1.2027            5.78m\n",
      "       278           1.2023            5.53m\n",
      "       279           1.2019            5.28m\n",
      "       280           1.2015            5.02m\n",
      "       281           1.2010            4.77m\n",
      "       282           1.2006            4.52m\n",
      "       283           1.2002            4.26m\n",
      "       284           1.1998            4.01m\n",
      "       285           1.1994            3.76m\n",
      "       286           1.1990            3.51m\n",
      "       287           1.1986            3.26m\n",
      "       288           1.1982            3.00m\n",
      "       289           1.1978            2.75m\n",
      "       290           1.1974            2.50m\n",
      "       291           1.1970            2.25m\n",
      "       292           1.1966            2.00m\n",
      "       293           1.1962            1.75m\n",
      "       294           1.1958            1.50m\n",
      "       295           1.1954            1.25m\n",
      "       296           1.1950            1.00m\n",
      "       297           1.1946           45.00s\n",
      "       298           1.1942           30.00s\n",
      "       299           1.1938           15.00s\n",
      "       300           1.1934            0.00s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80     89814\n",
      "           1       0.81      0.79      0.80     90186\n",
      "\n",
      "    accuracy                           0.80    180000\n",
      "   macro avg       0.80      0.80      0.80    180000\n",
      "weighted avg       0.80      0.80      0.80    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "df_train_2 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_2.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_2[feature_columns], df_train_2['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "gbt_model = GradientBoostingClassifier(init=AdaBoostClassifier(), learning_rate=0.001, n_estimators= 300, verbose=4).fit(x_train, y_train)\n",
    "\n",
    "y_pred = gbt_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "gbt_model.fit(x_test, y_test)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'gbt.pkl'), 'wb') as f:\n",
    "    pickle.dump(gbt_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 209901, number of negative: 210099\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.916580\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.489503\n",
      "[LightGBM] [Debug] init for col-wise cost 0.123866 seconds, init for row-wise cost 0.490855 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.561478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 46845\n",
      "[LightGBM] [Info] Number of data points in the train set: 420000, number of used features: 312\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499764 -> initscore=-0.000943\n",
      "[LightGBM] [Info] Start training from score -0.000943\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93     89901\n",
      "           1       0.93      0.93      0.93     90099\n",
      "\n",
      "    accuracy                           0.93    180000\n",
      "   macro avg       0.93      0.93      0.93    180000\n",
      "weighted avg       0.93      0.93      0.93    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "df_train_2 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_2.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_2[feature_columns], df_train_2['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(verbose=4, n_estimators=100).fit(x_train, y_train)\n",
    "\n",
    "y_pred = lgb_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "lgb.fit(x_test, y_test)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'lgb.pkl'), 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
