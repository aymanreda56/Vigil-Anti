{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from helpers import *\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swak\\AppData\\Local\\Temp\\ipykernel_18416\\52725572.py:1: DtypeWarning: Columns (228,302,307,309,316) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r'Dataset_Big.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMPORT_TABLE_size</th>\n",
       "      <th>strings_printabledist_31</th>\n",
       "      <th>strings_printabledist_88</th>\n",
       "      <th>.tls_vsize</th>\n",
       "      <th>subsystem</th>\n",
       "      <th>minor_image_version</th>\n",
       "      <th>RESOURCE_TABLE_virtual_address</th>\n",
       "      <th>.idata_vsize</th>\n",
       "      <th>.textbss_size</th>\n",
       "      <th>Dbgcore.dll</th>\n",
       "      <th>...</th>\n",
       "      <th>.idat_size</th>\n",
       "      <th>Libcrypto_num_funcs</th>\n",
       "      <th>mbedTLS</th>\n",
       "      <th>.txt_entropy</th>\n",
       "      <th>.UPX_props_len</th>\n",
       "      <th>CryptoAPI_num_funcs</th>\n",
       "      <th>.idat_props_len</th>\n",
       "      <th>.idat_vsize</th>\n",
       "      <th>.txt_size</th>\n",
       "      <th>.txt_vsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>858</td>\n",
       "      <td>965</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>0</td>\n",
       "      <td>352256</td>\n",
       "      <td>2370</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420</td>\n",
       "      <td>48</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>20512</td>\n",
       "      <td>483328</td>\n",
       "      <td>8192</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>116</td>\n",
       "      <td>152</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>0</td>\n",
       "      <td>90112</td>\n",
       "      <td>2242</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2370</td>\n",
       "      <td>613</td>\n",
       "      <td>653</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>0</td>\n",
       "      <td>65536</td>\n",
       "      <td>2384</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140</td>\n",
       "      <td>288</td>\n",
       "      <td>323</td>\n",
       "      <td>8</td>\n",
       "      <td>WINDOWS_GUI</td>\n",
       "      <td>0</td>\n",
       "      <td>49152</td>\n",
       "      <td>2384</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   IMPORT_TABLE_size  strings_printabledist_31  strings_printabledist_88   \n",
       "0                200                       858                       965  \\\n",
       "1                420                        48                       170   \n",
       "2                216                       116                       152   \n",
       "3               2370                       613                       653   \n",
       "4                140                       288                       323   \n",
       "\n",
       "   .tls_vsize    subsystem  minor_image_version   \n",
       "0           8  WINDOWS_GUI                    0  \\\n",
       "1           8  WINDOWS_GUI                20512   \n",
       "2           8  WINDOWS_GUI                    0   \n",
       "3           8  WINDOWS_GUI                    0   \n",
       "4           8  WINDOWS_GUI                    0   \n",
       "\n",
       "   RESOURCE_TABLE_virtual_address  .idata_vsize  .textbss_size  Dbgcore.dll   \n",
       "0                          352256          2370              0         True  \\\n",
       "1                          483328          8192              0         True   \n",
       "2                           90112          2242              0         True   \n",
       "3                           65536          2384              0         True   \n",
       "4                           49152          2384              0         True   \n",
       "\n",
       "   ...  .idat_size  Libcrypto_num_funcs  mbedTLS  .txt_entropy   \n",
       "0  ...         0.0                  0.0        0           0.0  \\\n",
       "1  ...         0.0                  0.0        0           0.0   \n",
       "2  ...         0.0                  0.0        0           0.0   \n",
       "3  ...         0.0                  0.0        0           0.0   \n",
       "4  ...         0.0                  0.0        0           0.0   \n",
       "\n",
       "   .UPX_props_len  CryptoAPI_num_funcs  .idat_props_len  .idat_vsize   \n",
       "0             0.0                  0.0              0.0          0.0  \\\n",
       "1             0.0                  0.0              0.0          0.0   \n",
       "2             0.0                  0.0              0.0          0.0   \n",
       "3             0.0                  0.0              0.0          0.0   \n",
       "4             0.0                  0.0              0.0          0.0   \n",
       "\n",
       "   .txt_size  .txt_vsize  \n",
       "0        0.0         0.0  \n",
       "1        0.0         0.0  \n",
       "2        0.0         0.0  \n",
       "3        0.0         0.0  \n",
       "4        0.0         0.0  \n",
       "\n",
       "[5 rows x 324 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'Dataset_Big.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of Features: 323\n"
     ]
    }
   ],
   "source": [
    "# Save our feature list\n",
    "feature_columns = list(df.columns)\n",
    "feature_columns.pop(feature_columns.index('label'))\n",
    "print(f\"total number of Features: {len(feature_columns)}\")\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'features.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_columns, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discard -1 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'suspicious_imports.txt'), 'r') as f:\n",
    "        sus_imports = f.readlines()\n",
    "sus_imports = [re.sub(r'\\n', '', i) for i in sus_imports]\n",
    "boolean_columns = sus_imports + []\n",
    "categorical_columns = [\"subsystem\", \"magic\", \"machine\"]\n",
    "\n",
    "array_of_Label_Encoders = []\n",
    "for col in categorical_columns:\n",
    "    new_LE = LabelEncoder().fit(df[col])\n",
    "    df[col] = new_LE.transform(df[col])\n",
    "    array_of_Label_Encoders.append(new_LE)\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "        if col in boolean_columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "            df[col].fillna(False)\n",
    "            continue\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'enc.pkl'), 'wb') as f:\n",
    "    pickle.dump(array_of_Label_Encoders, f)\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(),'models','enc.pkl'), 'rb') as f:\n",
    "        array_of_Label_Encoders = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     90046\n",
      "           1       0.97      0.95      0.96     89954\n",
      "\n",
      "    accuracy                           0.96    180000\n",
      "   macro avg       0.96      0.96      0.96    180000\n",
      "weighted avg       0.96      0.96      0.96    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_train_2 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_2.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_2[feature_columns], df_train_2['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "rf_model = RandomForestClassifier().fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'rf.pkl'), 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (batch_norm1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense3): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x.float())\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = self.batch_norm2(x.float())\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = self.batch_norm3(x.float())\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "# Define the testing function\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Get the predicted labels\n",
    "            #preds = torch.round(torch.sigmoid(outputs))\n",
    "            _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "            \n",
    "            # Collect the labels and predictions\n",
    "            all_labels += list(labels.numpy().reshape((-1,1)))\n",
    "            all_preds += list(preds.numpy().reshape((-1,1)))\n",
    "    \n",
    "    return np.asarray(all_labels), np.asarray(all_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.9038, Train Accuracy = 0.6314, Val Accuracy = 0.6866\n",
      "Epoch 2: Loss = 1.8401, Train Accuracy = 0.6937, Val Accuracy = 0.6981\n",
      "Epoch 3: Loss = 1.8305, Train Accuracy = 0.6972, Val Accuracy = 0.7006\n",
      "Epoch 4: Loss = 1.8249, Train Accuracy = 0.7056, Val Accuracy = 0.7170\n",
      "Epoch 5: Loss = 1.8194, Train Accuracy = 0.7180, Val Accuracy = 0.7240\n",
      "Epoch 6: Loss = 1.8141, Train Accuracy = 0.7319, Val Accuracy = 0.7353\n",
      "Epoch 7: Loss = 1.8091, Train Accuracy = 0.7431, Val Accuracy = 0.7479\n",
      "Epoch 8: Loss = 1.8051, Train Accuracy = 0.7525, Val Accuracy = 0.7580\n",
      "Epoch 9: Loss = 1.7996, Train Accuracy = 0.7696, Val Accuracy = 0.7833\n",
      "Epoch 10: Loss = 1.7863, Train Accuracy = 0.8025, Val Accuracy = 0.8096\n",
      "Epoch 11: Loss = 1.7779, Train Accuracy = 0.8216, Val Accuracy = 0.8282\n",
      "Epoch 12: Loss = 1.7717, Train Accuracy = 0.8345, Val Accuracy = 0.8365\n",
      "Epoch 13: Loss = 1.7692, Train Accuracy = 0.8399, Val Accuracy = 0.8384\n",
      "Epoch 14: Loss = 1.7675, Train Accuracy = 0.8442, Val Accuracy = 0.8427\n",
      "Epoch 15: Loss = 1.7659, Train Accuracy = 0.8483, Val Accuracy = 0.8483\n",
      "Epoch 16: Loss = 1.7639, Train Accuracy = 0.8548, Val Accuracy = 0.8564\n",
      "Epoch 17: Loss = 1.7614, Train Accuracy = 0.8619, Val Accuracy = 0.8598\n",
      "Epoch 18: Loss = 1.7595, Train Accuracy = 0.8654, Val Accuracy = 0.8616\n",
      "Epoch 19: Loss = 1.7582, Train Accuracy = 0.8686, Val Accuracy = 0.8660\n",
      "Epoch 20: Loss = 1.7569, Train Accuracy = 0.8718, Val Accuracy = 0.8664\n",
      "Epoch 21: Loss = 1.7555, Train Accuracy = 0.8753, Val Accuracy = 0.8713\n",
      "Epoch 22: Loss = 1.7545, Train Accuracy = 0.8777, Val Accuracy = 0.8753\n",
      "Epoch 23: Loss = 1.7536, Train Accuracy = 0.8802, Val Accuracy = 0.8746\n",
      "Epoch 24: Loss = 1.7526, Train Accuracy = 0.8833, Val Accuracy = 0.8798\n",
      "Epoch 25: Loss = 1.7515, Train Accuracy = 0.8858, Val Accuracy = 0.8809\n",
      "Epoch 26: Loss = 1.7509, Train Accuracy = 0.8880, Val Accuracy = 0.8825\n",
      "Epoch 27: Loss = 1.7501, Train Accuracy = 0.8900, Val Accuracy = 0.8836\n",
      "Epoch 28: Loss = 1.7493, Train Accuracy = 0.8923, Val Accuracy = 0.8845\n",
      "Epoch 29: Loss = 1.7485, Train Accuracy = 0.8944, Val Accuracy = 0.8880\n",
      "Epoch 30: Loss = 1.7478, Train Accuracy = 0.8963, Val Accuracy = 0.8899\n",
      "Epoch 31: Loss = 1.7472, Train Accuracy = 0.8979, Val Accuracy = 0.8909\n",
      "Epoch 32: Loss = 1.7466, Train Accuracy = 0.8993, Val Accuracy = 0.8918\n",
      "Epoch 33: Loss = 1.7460, Train Accuracy = 0.9013, Val Accuracy = 0.8914\n",
      "Epoch 34: Loss = 1.7455, Train Accuracy = 0.9023, Val Accuracy = 0.8938\n",
      "Epoch 35: Loss = 1.7445, Train Accuracy = 0.9051, Val Accuracy = 0.8955\n",
      "Epoch 36: Loss = 1.7440, Train Accuracy = 0.9064, Val Accuracy = 0.8960\n",
      "Epoch 37: Loss = 1.7436, Train Accuracy = 0.9073, Val Accuracy = 0.8976\n",
      "Epoch 38: Loss = 1.7430, Train Accuracy = 0.9085, Val Accuracy = 0.8987\n",
      "Epoch 39: Loss = 1.7425, Train Accuracy = 0.9099, Val Accuracy = 0.8985\n",
      "Epoch 40: Loss = 1.7421, Train Accuracy = 0.9103, Val Accuracy = 0.9010\n",
      "Epoch 41: Loss = 1.7416, Train Accuracy = 0.9115, Val Accuracy = 0.9008\n",
      "Epoch 42: Loss = 1.7412, Train Accuracy = 0.9129, Val Accuracy = 0.9021\n",
      "Epoch 43: Loss = 1.7408, Train Accuracy = 0.9138, Val Accuracy = 0.9024\n",
      "Epoch 44: Loss = 1.7404, Train Accuracy = 0.9151, Val Accuracy = 0.9024\n",
      "Epoch 45: Loss = 1.7400, Train Accuracy = 0.9158, Val Accuracy = 0.9033\n",
      "Epoch 46: Loss = 1.7396, Train Accuracy = 0.9164, Val Accuracy = 0.9029\n",
      "Epoch 47: Loss = 1.7392, Train Accuracy = 0.9173, Val Accuracy = 0.9044\n",
      "Epoch 48: Loss = 1.7390, Train Accuracy = 0.9180, Val Accuracy = 0.9045\n",
      "Epoch 49: Loss = 1.7388, Train Accuracy = 0.9187, Val Accuracy = 0.9051\n",
      "Epoch 50: Loss = 1.7384, Train Accuracy = 0.9193, Val Accuracy = 0.9062\n",
      "Epoch 51: Loss = 1.7381, Train Accuracy = 0.9203, Val Accuracy = 0.9063\n",
      "Epoch 52: Loss = 1.7378, Train Accuracy = 0.9207, Val Accuracy = 0.9068\n",
      "Epoch 53: Loss = 1.7375, Train Accuracy = 0.9216, Val Accuracy = 0.9054\n",
      "Epoch 54: Loss = 1.7373, Train Accuracy = 0.9218, Val Accuracy = 0.9074\n",
      "Epoch 55: Loss = 1.7371, Train Accuracy = 0.9223, Val Accuracy = 0.9084\n",
      "Epoch 56: Loss = 1.7369, Train Accuracy = 0.9230, Val Accuracy = 0.9088\n",
      "Epoch 57: Loss = 1.7367, Train Accuracy = 0.9232, Val Accuracy = 0.9080\n",
      "Epoch 58: Loss = 1.7364, Train Accuracy = 0.9240, Val Accuracy = 0.9086\n",
      "Epoch 59: Loss = 1.7360, Train Accuracy = 0.9249, Val Accuracy = 0.9088\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)#, weight_decay=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(130):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), 'trained_model.pt')\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN1.pkl'), 'wb') as f:\n",
    "     pickle.dump(net, f)\n",
    "    \n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net, test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN2\n",
    "### same as base NN model, but without any batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.dense2 = nn.Linear(512, 128)\n",
    "        self.dense3 = nn.Linear(128, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64)).type(torch.float)\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64)).type(torch.float)\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)#, weight_decay=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(130):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN2.pkl'), 'wb') as f:\n",
    "     pickle.dump(net, f)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net, test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN3\n",
    "### same as base model, but with Leaky Relu instead of the tanh activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (batch_norm1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (lrelu1): LeakyReLU(negative_slope=0.01)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (lrelu2): LeakyReLU(negative_slope=0.01)\n",
      "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense3): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (lrelu3): LeakyReLU(negative_slope=0.01)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.lrelu1 = nn.LeakyReLU()\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 128)\n",
    "        self.lrelu2 = nn.LeakyReLU()\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, 8)\n",
    "        self.lrelu3 = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x.float())\n",
    "        x = self.lrelu1(self.dense1(x))\n",
    "        x = self.batch_norm2(x.float())\n",
    "        x = self.lrelu2(self.dense2(x))\n",
    "        x = self.batch_norm3(x.float())\n",
    "        x = self.lrelu3(self.dense3(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.6818, Train Accuracy = 0.6489, Val Accuracy = 0.7792\n",
      "Epoch 2: Loss = 1.4998, Train Accuracy = 0.7909, Val Accuracy = 0.8000\n",
      "Epoch 3: Loss = 1.4754, Train Accuracy = 0.8058, Val Accuracy = 0.8087\n",
      "Epoch 4: Loss = 1.4629, Train Accuracy = 0.8158, Val Accuracy = 0.8168\n",
      "Epoch 5: Loss = 1.4547, Train Accuracy = 0.8223, Val Accuracy = 0.8196\n",
      "Epoch 6: Loss = 1.4471, Train Accuracy = 0.8269, Val Accuracy = 0.8232\n",
      "Epoch 7: Loss = 1.4420, Train Accuracy = 0.8305, Val Accuracy = 0.8238\n",
      "Epoch 8: Loss = 1.4382, Train Accuracy = 0.8336, Val Accuracy = 0.8266\n",
      "Epoch 9: Loss = 1.4364, Train Accuracy = 0.8350, Val Accuracy = 0.8292\n",
      "Epoch 10: Loss = 1.4334, Train Accuracy = 0.8376, Val Accuracy = 0.8319\n",
      "Epoch 11: Loss = 1.4301, Train Accuracy = 0.8407, Val Accuracy = 0.8339\n",
      "Epoch 12: Loss = 1.4284, Train Accuracy = 0.8423, Val Accuracy = 0.8375\n",
      "Epoch 13: Loss = 1.4262, Train Accuracy = 0.8443, Val Accuracy = 0.8378\n",
      "Epoch 14: Loss = 1.4246, Train Accuracy = 0.8461, Val Accuracy = 0.8406\n",
      "Epoch 15: Loss = 1.4138, Train Accuracy = 0.8594, Val Accuracy = 0.8606\n",
      "Epoch 16: Loss = 1.4035, Train Accuracy = 0.8709, Val Accuracy = 0.8625\n",
      "Epoch 17: Loss = 1.4025, Train Accuracy = 0.8718, Val Accuracy = 0.8628\n",
      "Epoch 18: Loss = 1.3998, Train Accuracy = 0.8744, Val Accuracy = 0.8659\n",
      "Epoch 19: Loss = 1.3981, Train Accuracy = 0.8765, Val Accuracy = 0.8673\n",
      "Epoch 20: Loss = 1.3964, Train Accuracy = 0.8776, Val Accuracy = 0.8673\n",
      "Epoch 21: Loss = 1.3950, Train Accuracy = 0.8792, Val Accuracy = 0.8671\n",
      "Epoch 22: Loss = 1.3928, Train Accuracy = 0.8813, Val Accuracy = 0.8704\n",
      "Epoch 23: Loss = 1.3914, Train Accuracy = 0.8828, Val Accuracy = 0.8726\n",
      "Epoch 24: Loss = 1.3889, Train Accuracy = 0.8854, Val Accuracy = 0.8749\n",
      "Epoch 25: Loss = 1.3873, Train Accuracy = 0.8870, Val Accuracy = 0.8749\n",
      "Epoch 26: Loss = 1.3863, Train Accuracy = 0.8882, Val Accuracy = 0.8765\n",
      "Epoch 27: Loss = 1.3850, Train Accuracy = 0.8893, Val Accuracy = 0.8767\n",
      "Epoch 28: Loss = 1.3834, Train Accuracy = 0.8911, Val Accuracy = 0.8782\n",
      "Epoch 29: Loss = 1.3826, Train Accuracy = 0.8918, Val Accuracy = 0.8797\n",
      "Epoch 30: Loss = 1.3812, Train Accuracy = 0.8932, Val Accuracy = 0.8792\n",
      "Epoch 31: Loss = 1.3797, Train Accuracy = 0.8948, Val Accuracy = 0.8796\n",
      "Epoch 32: Loss = 1.3786, Train Accuracy = 0.8958, Val Accuracy = 0.8822\n",
      "Epoch 33: Loss = 1.3778, Train Accuracy = 0.8966, Val Accuracy = 0.8812\n",
      "Epoch 34: Loss = 1.3766, Train Accuracy = 0.8979, Val Accuracy = 0.8828\n",
      "Epoch 35: Loss = 1.3753, Train Accuracy = 0.8992, Val Accuracy = 0.8855\n",
      "Epoch 36: Loss = 1.3751, Train Accuracy = 0.8993, Val Accuracy = 0.8853\n",
      "Epoch 37: Loss = 1.3739, Train Accuracy = 0.9006, Val Accuracy = 0.8864\n",
      "Epoch 38: Loss = 1.3725, Train Accuracy = 0.9021, Val Accuracy = 0.8851\n",
      "Epoch 39: Loss = 1.3718, Train Accuracy = 0.9029, Val Accuracy = 0.8867\n",
      "Epoch 40: Loss = 1.3715, Train Accuracy = 0.9030, Val Accuracy = 0.8867\n",
      "Epoch 41: Loss = 1.3705, Train Accuracy = 0.9041, Val Accuracy = 0.8883\n",
      "Epoch 42: Loss = 1.3694, Train Accuracy = 0.9050, Val Accuracy = 0.8878\n",
      "Epoch 43: Loss = 1.3688, Train Accuracy = 0.9058, Val Accuracy = 0.8867\n",
      "Epoch 44: Loss = 1.3680, Train Accuracy = 0.9069, Val Accuracy = 0.8900\n",
      "Epoch 45: Loss = 1.3674, Train Accuracy = 0.9072, Val Accuracy = 0.8902\n",
      "Epoch 46: Loss = 1.3663, Train Accuracy = 0.9084, Val Accuracy = 0.8907\n",
      "Epoch 47: Loss = 1.3656, Train Accuracy = 0.9091, Val Accuracy = 0.8912\n",
      "Epoch 48: Loss = 1.3655, Train Accuracy = 0.9092, Val Accuracy = 0.8896\n",
      "Epoch 49: Loss = 1.3639, Train Accuracy = 0.9108, Val Accuracy = 0.8922\n",
      "Epoch 50: Loss = 1.3636, Train Accuracy = 0.9112, Val Accuracy = 0.8906\n",
      "Epoch 51: Loss = 1.3634, Train Accuracy = 0.9112, Val Accuracy = 0.8930\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\ClassWork\\anti_virus\\Vigil-Anti\\Source\\EXEs\\classifier.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\ClassWork\\anti_virus\\Vigil-Anti\\Source\\EXEs\\classifier.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm1(x\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlrelu1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm2(x\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlrelu2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense2(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)#, weight_decay=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(130):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), 'trained_model.pt')\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN3.pkl'), 'wb') as f:\n",
    "     pickle.dump(net, f)\n",
    "\n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net, test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN 4\n",
    "### same base model but with an extra dense layer and weight decay = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (batch_norm1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (batch_norm3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batch_norm4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense4): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 512)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.dense3 = nn.Linear(512, 128)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(128)\n",
    "        self.dense4 = nn.Linear(128, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x.float())\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = self.batch_norm2(x.float())\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = self.batch_norm3(x.float())\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = self.batch_norm4(x.float())\n",
    "        x = torch.tanh(self.dense4(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.8411, Train Accuracy = 0.6763, Val Accuracy = 0.7346\n",
      "Epoch 2: Loss = 1.7978, Train Accuracy = 0.7681, Val Accuracy = 0.7936\n",
      "Epoch 3: Loss = 1.7753, Train Accuracy = 0.8268, Val Accuracy = 0.8448\n",
      "Epoch 4: Loss = 1.7620, Train Accuracy = 0.8612, Val Accuracy = 0.8633\n",
      "Epoch 5: Loss = 1.7537, Train Accuracy = 0.8795, Val Accuracy = 0.8751\n",
      "Epoch 6: Loss = 1.7498, Train Accuracy = 0.8888, Val Accuracy = 0.8839\n",
      "Epoch 7: Loss = 1.7464, Train Accuracy = 0.8964, Val Accuracy = 0.8886\n",
      "Epoch 8: Loss = 1.7440, Train Accuracy = 0.9023, Val Accuracy = 0.8943\n",
      "Epoch 9: Loss = 1.7421, Train Accuracy = 0.9071, Val Accuracy = 0.8924\n",
      "Epoch 10: Loss = 1.7406, Train Accuracy = 0.9109, Val Accuracy = 0.9022\n",
      "Epoch 11: Loss = 1.7396, Train Accuracy = 0.9132, Val Accuracy = 0.9027\n",
      "Epoch 12: Loss = 1.7382, Train Accuracy = 0.9167, Val Accuracy = 0.9062\n",
      "Epoch 13: Loss = 1.7373, Train Accuracy = 0.9189, Val Accuracy = 0.9061\n",
      "Epoch 14: Loss = 1.7363, Train Accuracy = 0.9215, Val Accuracy = 0.9079\n",
      "Epoch 15: Loss = 1.7356, Train Accuracy = 0.9232, Val Accuracy = 0.9085\n",
      "Epoch 16: Loss = 1.7349, Train Accuracy = 0.9250, Val Accuracy = 0.9086\n",
      "Epoch 17: Loss = 1.7343, Train Accuracy = 0.9262, Val Accuracy = 0.9141\n",
      "Epoch 18: Loss = 1.7337, Train Accuracy = 0.9277, Val Accuracy = 0.9124\n",
      "Epoch 19: Loss = 1.7331, Train Accuracy = 0.9289, Val Accuracy = 0.9140\n",
      "Epoch 20: Loss = 1.7325, Train Accuracy = 0.9307, Val Accuracy = 0.9162\n",
      "Epoch 21: Loss = 1.7318, Train Accuracy = 0.9317, Val Accuracy = 0.9151\n",
      "Epoch 22: Loss = 1.7314, Train Accuracy = 0.9333, Val Accuracy = 0.9182\n",
      "Epoch 23: Loss = 1.7309, Train Accuracy = 0.9345, Val Accuracy = 0.9184\n",
      "Epoch 24: Loss = 1.7306, Train Accuracy = 0.9348, Val Accuracy = 0.9194\n",
      "Epoch 25: Loss = 1.7302, Train Accuracy = 0.9359, Val Accuracy = 0.9187\n",
      "Epoch 26: Loss = 1.7297, Train Accuracy = 0.9372, Val Accuracy = 0.9185\n",
      "Epoch 27: Loss = 1.7296, Train Accuracy = 0.9372, Val Accuracy = 0.9210\n",
      "Epoch 28: Loss = 1.7291, Train Accuracy = 0.9382, Val Accuracy = 0.9207\n",
      "Epoch 29: Loss = 1.7289, Train Accuracy = 0.9389, Val Accuracy = 0.9221\n",
      "Epoch 30: Loss = 1.7286, Train Accuracy = 0.9401, Val Accuracy = 0.9220\n",
      "Epoch 31: Loss = 1.7284, Train Accuracy = 0.9408, Val Accuracy = 0.9222\n",
      "Epoch 32: Loss = 1.7280, Train Accuracy = 0.9416, Val Accuracy = 0.9210\n",
      "Epoch 33: Loss = 1.7280, Train Accuracy = 0.9414, Val Accuracy = 0.9227\n",
      "Epoch 34: Loss = 1.7276, Train Accuracy = 0.9424, Val Accuracy = 0.9228\n",
      "Epoch 35: Loss = 1.7275, Train Accuracy = 0.9424, Val Accuracy = 0.9245\n",
      "Epoch 36: Loss = 1.7270, Train Accuracy = 0.9437, Val Accuracy = 0.9242\n",
      "Epoch 37: Loss = 1.7270, Train Accuracy = 0.9436, Val Accuracy = 0.9254\n",
      "Epoch 38: Loss = 1.7268, Train Accuracy = 0.9441, Val Accuracy = 0.9248\n",
      "Epoch 39: Loss = 1.7264, Train Accuracy = 0.9456, Val Accuracy = 0.9253\n",
      "Epoch 40: Loss = 1.7265, Train Accuracy = 0.9448, Val Accuracy = 0.9254\n",
      "Epoch 41: Loss = 1.7261, Train Accuracy = 0.9461, Val Accuracy = 0.9243\n",
      "Epoch 42: Loss = 1.7261, Train Accuracy = 0.9458, Val Accuracy = 0.9244\n",
      "Epoch 43: Loss = 1.7258, Train Accuracy = 0.9466, Val Accuracy = 0.9256\n",
      "Epoch 44: Loss = 1.7255, Train Accuracy = 0.9470, Val Accuracy = 0.9264\n",
      "Epoch 45: Loss = 1.7254, Train Accuracy = 0.9476, Val Accuracy = 0.9244\n",
      "Epoch 46: Loss = 1.7255, Train Accuracy = 0.9475, Val Accuracy = 0.9263\n",
      "Epoch 47: Loss = 1.7254, Train Accuracy = 0.9475, Val Accuracy = 0.9253\n",
      "Epoch 48: Loss = 1.7250, Train Accuracy = 0.9483, Val Accuracy = 0.9266\n",
      "Epoch 49: Loss = 1.7248, Train Accuracy = 0.9487, Val Accuracy = 0.9255\n",
      "Epoch 50: Loss = 1.7248, Train Accuracy = 0.9489, Val Accuracy = 0.9259\n",
      "Epoch 51: Loss = 1.7246, Train Accuracy = 0.9494, Val Accuracy = 0.9259\n",
      "Epoch 52: Loss = 1.7243, Train Accuracy = 0.9501, Val Accuracy = 0.9281\n",
      "Epoch 53: Loss = 1.7243, Train Accuracy = 0.9506, Val Accuracy = 0.9251\n",
      "Epoch 54: Loss = 1.7243, Train Accuracy = 0.9499, Val Accuracy = 0.9283\n",
      "Epoch 55: Loss = 1.7242, Train Accuracy = 0.9505, Val Accuracy = 0.9278\n",
      "Epoch 56: Loss = 1.7242, Train Accuracy = 0.9506, Val Accuracy = 0.9266\n",
      "Epoch 57: Loss = 1.7239, Train Accuracy = 0.9508, Val Accuracy = 0.9273\n",
      "Epoch 58: Loss = 1.7238, Train Accuracy = 0.9513, Val Accuracy = 0.9289\n",
      "Epoch 59: Loss = 1.7237, Train Accuracy = 0.9516, Val Accuracy = 0.9278\n",
      "Epoch 60: Loss = 1.7235, Train Accuracy = 0.9519, Val Accuracy = 0.9276\n",
      "Epoch 61: Loss = 1.7235, Train Accuracy = 0.9520, Val Accuracy = 0.9276\n",
      "Epoch 62: Loss = 1.7232, Train Accuracy = 0.9527, Val Accuracy = 0.9263\n",
      "Epoch 63: Loss = 1.7233, Train Accuracy = 0.9524, Val Accuracy = 0.9272\n",
      "Epoch 64: Loss = 1.7231, Train Accuracy = 0.9531, Val Accuracy = 0.9274\n",
      "Epoch 65: Loss = 1.7231, Train Accuracy = 0.9532, Val Accuracy = 0.9279\n",
      "Epoch 66: Loss = 1.7231, Train Accuracy = 0.9533, Val Accuracy = 0.9270\n",
      "Epoch 67: Loss = 1.7228, Train Accuracy = 0.9536, Val Accuracy = 0.9264\n",
      "Epoch 68: Loss = 1.7230, Train Accuracy = 0.9531, Val Accuracy = 0.9280\n",
      "Epoch 69: Loss = 1.7227, Train Accuracy = 0.9540, Val Accuracy = 0.9284\n",
      "Epoch 70: Loss = 1.7227, Train Accuracy = 0.9539, Val Accuracy = 0.9293\n",
      "Epoch 71: Loss = 1.7226, Train Accuracy = 0.9539, Val Accuracy = 0.9290\n",
      "Epoch 72: Loss = 1.7223, Train Accuracy = 0.9549, Val Accuracy = 0.9293\n",
      "Epoch 73: Loss = 1.7223, Train Accuracy = 0.9546, Val Accuracy = 0.9279\n",
      "Epoch 74: Loss = 1.7224, Train Accuracy = 0.9545, Val Accuracy = 0.9279\n",
      "Epoch 75: Loss = 1.7220, Train Accuracy = 0.9553, Val Accuracy = 0.9282\n",
      "Epoch 76: Loss = 1.7220, Train Accuracy = 0.9557, Val Accuracy = 0.9287\n",
      "Epoch 77: Loss = 1.7219, Train Accuracy = 0.9560, Val Accuracy = 0.9286\n",
      "Epoch 78: Loss = 1.7218, Train Accuracy = 0.9563, Val Accuracy = 0.9285\n",
      "Epoch 79: Loss = 1.7217, Train Accuracy = 0.9567, Val Accuracy = 0.9272\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\ClassWork\\anti_virus\\Vigil-Anti\\Source\\EXEs\\classifier.ipynb Cell 21\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X33sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X33sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m running_corrects \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# Counter for correct predictions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X33sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X33sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# Zero the gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X33sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ClassWork/anti_virus/Vigil-Anti/Source/EXEs/classifier.ipynb#X33sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.000001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(130):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), 'trained_model.pt')\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN4.pkl'), 'wb') as f:\n",
    "     pickle.dump(net, f)\n",
    "\n",
    "\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net, test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN 5\n",
    "### same as NN 4 but with bigger learning rate, and with weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_train, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_val = torch.from_numpy(np.asarray(x_val, dtype=np.int64))\n",
    "y_val = torch.from_numpy(np.asarray(y_val, dtype=bool)).type(torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features).to(device=device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.000001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(130):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs.to(device=device))\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy on the training set\n",
    "    train_accuracy = running_corrects / len(dataset)\n",
    "     \n",
    "    # Calculate the validation accuracy\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = val_corrects / val_total\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Train Accuracy = {train_accuracy:.4f}, Val Accuracy = {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), 'trained_model.pt')\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'NN4.pkl'), 'wb') as f:\n",
    "     pickle.dump(net, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
