{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from helpers import *\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sha256': '2ef9a92ee6c955364564b0df75ee3753473014b2ba162b9df90afe6df9dbb256', 'md5': '7e39aeea7bc21d16b8652516a150b282', 'appeared': '2018-01', 'label': 1, 'avclass': 'sivis', 'histogram': [60782, 5895, 2020, 1487, 2075, 1367, 1145, 856, 2037, 725, 2027, 716, 1418, 903, 672, 1014, 1605, 652, 702, 691, 1048, 927, 641, 599, 795, 636, 598, 598, 677, 629, 597, 571, 8564, 738, 921, 600, 1253, 835, 645, 565, 1015, 919, 958, 868, 917, 784, 1435, 1307, 1470, 1081, 903, 1380, 913, 914, 872, 823, 1013, 1048, 1001, 1289, 1063, 1261, 792, 771, 1852, 3074, 928, 1346, 1238, 1786, 1036, 857, 1028, 1149, 902, 749, 1003, 1101, 1014, 883, 2012, 1152, 1374, 1468, 1242, 1374, 1312, 1447, 975, 848, 716, 1067, 940, 1566, 1298, 1468, 897, 3196, 1406, 2574, 2206, 5376, 2771, 1455, 2052, 2923, 1401, 908, 2522, 1562, 3768, 3473, 2336, 813, 3879, 2968, 5270, 2441, 1323, 1398, 1176, 1245, 843, 944, 984, 1172, 878, 851, 1168, 1116, 1029, 2612, 900, 1471, 827, 767, 953, 1479, 908, 4228, 772, 1342, 753, 719, 828, 761, 745, 667, 729, 686, 713, 639, 737, 713, 666, 672, 705, 725, 678, 685, 830, 987, 702, 760, 765, 719, 679, 645, 780, 748, 680, 685, 716, 685, 621, 669, 816, 653, 658, 659, 678, 658, 618, 628, 903, 654, 659, 629, 654, 667, 612, 740, 1437, 848, 819, 1296, 1546, 564, 831, 996, 689, 721, 643, 583, 3203, 570, 617, 603, 719, 594, 601, 548, 576, 541, 548, 547, 731, 557, 553, 589, 663, 543, 610, 562, 710, 552, 583, 580, 599, 812, 518, 600, 1907, 648, 515, 828, 993, 545, 527, 577, 841, 550, 571, 629, 667, 565, 750, 723, 1097, 631, 546, 581, 864, 605, 859, 4201], 'byteentropy': [24434, 11, 18, 5, 9, 8, 35, 21, 3, 4, 8, 4, 2, 3, 7, 4, 3921, 32, 41, 22, 21, 4, 15, 7, 22, 0, 0, 4, 0, 0, 4, 3, 7561, 18, 51, 73, 51, 40, 104, 71, 116, 13, 25, 9, 10, 19, 13, 18, 5508, 58, 71, 53, 75, 49, 86, 58, 39, 15, 9, 16, 20, 9, 10, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11511, 101, 126, 342, 292, 209, 2287, 1039, 128, 18, 10, 39, 105, 49, 55, 73, 10708, 79, 164, 153, 171, 123, 5559, 3045, 47, 46, 77, 66, 45, 98, 46, 53, 6544, 90, 493, 123, 406, 153, 2676, 1645, 16, 13, 10, 9, 18, 9, 23, 60, 7743, 288, 14879, 1670, 2485, 2220, 19097, 10241, 86, 94, 104, 72, 78, 97, 92, 146, 12536, 518, 4286, 917, 2292, 1775, 8007, 4676, 521, 112, 150, 156, 203, 139, 214, 362, 9395, 855, 1791, 1529, 2377, 1044, 4336, 2572, 453, 250, 210, 235, 342, 224, 236, 775, 11736, 1103, 1073, 5260, 1820, 1455, 2484, 2296, 1637, 698, 741, 691, 1121, 684, 784, 1233, 3145, 294, 554, 638, 544, 577, 973, 1039, 700, 334, 308, 203, 212, 131, 246, 342, 29295, 4407, 2827, 3128, 8848, 10365, 5312, 5061, 15488, 874, 1533, 1870, 11498, 1445, 5335, 9450, 6830, 2164, 3620, 5074, 6207, 7724, 9480, 9261, 8770, 6310, 6425, 4435, 4128, 2301, 2210, 3125, 16372, 13900, 14628, 14176, 14254, 14749, 16462, 15966, 14060, 13914, 13731, 13957, 14147, 13756, 13708, 13644], 'strings': {'numstrings': 3863, 'avlength': 17.643541289153507, 'printabledist': [7399, 89, 373, 105, 116, 114, 101, 94, 430, 406, 359, 173, 325, 243, 829, 797, 479, 350, 283, 236, 214, 214, 236, 226, 213, 241, 345, 498, 258, 415, 188, 164, 198, 422, 274, 724, 394, 386, 311, 243, 304, 550, 138, 198, 352, 261, 343, 294, 469, 208, 517, 646, 526, 290, 261, 452, 242, 212, 174, 254, 252, 237, 176, 550, 210, 2508, 825, 1746, 1558, 4547, 941, 883, 1064, 2222, 317, 347, 1798, 938, 3013, 2686, 1543, 219, 3052, 2222, 3465, 1030, 653, 651, 457, 573, 234, 307, 201, 236, 146, 194], 'printables': 68157, 'entropy': 5.683163905800377, 'paths': 6, 'urls': 22, 'registry': 0, 'MZ': 7}, 'general': {'size': 349811, 'vsize': 28672, 'has_debug': 0, 'exports': 0, 'imports': 55, 'has_relocations': 0, 'has_resources': 1, 'has_signature': 0, 'has_tls': 0, 'symbols': 0}, 'header': {'coff': {'timestamp': 1301832471, 'machine': 'I386', 'characteristics': ['CHARA_32BIT_MACHINE', 'RELOCS_STRIPPED', 'EXECUTABLE_IMAGE', 'LINE_NUMS_STRIPPED', 'LOCAL_SYMS_STRIPPED']}, 'optional': {'subsystem': 'WINDOWS_GUI', 'dll_characteristics': [], 'magic': 'PE32', 'major_image_version': 0, 'minor_image_version': 0, 'major_linker_version': 2, 'minor_linker_version': 50, 'major_operating_system_version': 4, 'minor_operating_system_version': 0, 'major_subsystem_version': 4, 'minor_subsystem_version': 0, 'sizeof_code': 8704, 'sizeof_headers': 1024, 'sizeof_heap_commit': 4096}}, 'section': {'entry': '.code', 'sections': [{'name': '.code', 'size': 2048, 'entropy': 5.322405635106704, 'vsize': 1841, 'props': ['CNT_CODE', 'MEM_EXECUTE', 'MEM_READ']}, {'name': '.text', 'size': 6656, 'entropy': 6.191552965438026, 'vsize': 6552, 'props': ['CNT_CODE', 'MEM_EXECUTE', 'MEM_READ']}, {'name': '.rdata', 'size': 512, 'entropy': 1.7695459925589745, 'vsize': 28, 'props': ['CNT_INITIALIZED_DATA', 'MEM_READ']}, {'name': '.data', 'size': 2048, 'entropy': 4.516154676301226, 'vsize': 1960, 'props': ['CNT_INITIALIZED_DATA', 'MEM_READ', 'MEM_WRITE']}, {'name': '.rsrc', 'size': 1024, 'entropy': 5.06744528418907, 'vsize': 700, 'props': ['CNT_INITIALIZED_DATA', 'MEM_READ']}]}, 'imports': {'MSVCRT.dll': ['memset', 'memcpy', '_stricmp', 'strncmp', '_strnicmp', 'strcmp', 'memmove', 'strlen', 'strcpy', 'strcat', 'strncpy'], 'KERNEL32.dll': ['GetModuleHandleA', 'HeapCreate', 'HeapDestroy', 'ExitProcess', 'GetCurrentThreadId', 'GetTickCount', 'HeapAlloc', 'HeapFree', 'WriteFile', 'CloseHandle', 'CreateFileA', 'GetFileSize', 'ReadFile', 'SetFilePointer', 'InitializeCriticalSection', 'GetModuleFileNameA', 'GetCurrentProcess', 'DuplicateHandle', 'CreatePipe', 'GetStdHandle', 'CreateProcessA', 'WaitForSingleObject', 'EnterCriticalSection', 'LeaveCriticalSection', 'GetCurrentProcessId', 'GetDriveTypeA', 'FindFirstFileA', 'FindClose', 'GetFileAttributesA', 'CreateDirectoryA', 'GetLastError', 'FindNextFileA', 'SetFileAttributesA', 'HeapReAlloc'], 'COMCTL32.DLL': ['InitCommonControls'], 'USER32.DLL': ['MessageBoxA', 'GetWindowThreadProcessId', 'IsWindowVisible', 'IsWindowEnabled', 'GetForegroundWindow', 'EnableWindow', 'EnumWindows'], 'SHELL32.DLL': ['ShellExecuteExA'], 'OLE32.DLL': ['CoInitialize']}, 'exports': [], 'datadirectories': [{'name': 'EXPORT_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'IMPORT_TABLE', 'size': 140, 'virtual_address': 20604}, {'name': 'RESOURCE_TABLE', 'size': 700, 'virtual_address': 24576}, {'name': 'EXCEPTION_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'CERTIFICATE_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'BASE_RELOCATION_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'DEBUG', 'size': 0, 'virtual_address': 0}, {'name': 'ARCHITECTURE', 'size': 0, 'virtual_address': 0}, {'name': 'GLOBAL_PTR', 'size': 0, 'virtual_address': 0}, {'name': 'TLS_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'LOAD_CONFIG_TABLE', 'size': 0, 'virtual_address': 0}, {'name': 'BOUND_IMPORT', 'size': 0, 'virtual_address': 0}, {'name': 'IAT', 'size': 244, 'virtual_address': 20988}, {'name': 'DELAY_IMPORT_DESCRIPTOR', 'size': 0, 'virtual_address': 0}, {'name': 'CLR_RUNTIME_HEADER', 'size': 0, 'virtual_address': 0}]}\n"
     ]
    }
   ],
   "source": [
    "json_file = r\"D:\\ClassWork\\anti_virus\\Vigil-Anti\\EXE_Dataset\\ember2018\\train_features_1.jsonl\"\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    json_ds_list = list(f)\n",
    "\n",
    "DataSet = []\n",
    "for i,ds in enumerate(json_ds_list):\n",
    "    #if( i > 12000):\n",
    "    #    break\n",
    "    DataSet.append(json.loads(ds))\n",
    "\n",
    "# to free some of the precious memory\n",
    "del json_ds_list\n",
    "\n",
    "print (DataSet[0])\n",
    "\n",
    "simple_ds = DataSet[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the unique section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall_sectionNames = set()\\nfor ds_obj in DataSet:\\n    for dic_elm in ds_obj[\\'section\\'][\\'sections\\']:\\n        all_sectionNames.add(dic_elm[\\'name\\'])\\n\\nwith open(\\'sectionNames.txt\\', \\'w\\') as f:\\n    f.write(\\'\\n\\'.join(all_sectionNames))\\n\\ncorrect_sec_names = []\\nfor n in all_sectionNames:\\n    if(n and n[0] == \".\"):\\n        correct_sec_names.append(n)\\n\\nwith open(\\'sectionNames_correct.txt\\', \\'w\\') as f:\\n    f.write(\\'\\n\\'.join(correct_sec_names))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "all_sectionNames = set()\n",
    "for ds_obj in DataSet:\n",
    "    for dic_elm in ds_obj['section']['sections']:\n",
    "        all_sectionNames.add(dic_elm['name'])\n",
    "\n",
    "with open('sectionNames.txt', 'w') as f:\n",
    "    f.write('\\n'.join(all_sectionNames))\n",
    "\n",
    "correct_sec_names = []\n",
    "for n in all_sectionNames:\n",
    "    if(n and n[0] == \".\"):\n",
    "        correct_sec_names.append(n)\n",
    "\n",
    "with open('sectionNames_correct.txt', 'w') as f:\n",
    "    f.write('\\n'.join(correct_sec_names))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spoiler: there are lots of malicious section names\n",
    "#### so I just extracted the most common and correct section names and then wrote them into \"common_section_names.txt\"\n",
    "#### any other section names will be considered \"UNKNOWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the most common section names\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'common_section_names.txt'), 'r') as f:\n",
    "    Common_section_names = f.readlines()\n",
    "\n",
    "Common_section_names = [re.sub(r'\\n', '', i) for i in Common_section_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore all the possible imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tqdm import tqdm\\nall_imports = set()\\nfor obj in tqdm(DataSet):\\n    import_DLL_dict = obj['imports']\\n    DLL_list = list(import_DLL_dict.keys())\\n    for elm in DLL_list:\\n        if(elm.endswith('.dll')):\\n            all_imports.add(elm)\\n    #all_imports = set(all_imports)\\n\\nwith open('all_imports_cleansed.txt', 'w') as f:\\n    f.write('\\n'.join(all_imports))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "all_imports = set()\n",
    "for obj in tqdm(DataSet):\n",
    "    import_DLL_dict = obj['imports']\n",
    "    DLL_list = list(import_DLL_dict.keys())\n",
    "    for elm in DLL_list:\n",
    "        if(elm.endswith('.dll')):\n",
    "            all_imports.add(elm)\n",
    "    #all_imports = set(all_imports)\n",
    "\n",
    "with open('all_imports_cleansed.txt', 'w') as f:\n",
    "    f.write('\\n'.join(all_imports))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Same problem with DLL imports, there are numerous different DLLs\n",
    "### and I cannot really filter all of them, so I will just grab the most common DLLs that are associated with most malwares\n",
    "### and another feature which will be the number of imported DLLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's just cleanse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleansing the dataset:   1%|          | 77/12001 [00:00<00:16, 740.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleansing the dataset:  62%|██████▏   | 7474/12001 [00:07<00:03, 1278.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inside handle_DLL_imports()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cleansing the dataset: 100%|██████████| 12001/12001 [00:11<00:00, 1033.39it/s]\n"
     ]
    }
   ],
   "source": [
    "new_Dataset = []\n",
    "\n",
    "for simple_ds in tqdm(DataSet, desc='cleansing the dataset'):\n",
    "    try:\n",
    "        # add reduced features of byteentropy distribution\n",
    "        simple_ds.update(Interpret_Histogram(simple_ds['byteentropy'], 'byteentropy'))\n",
    "\n",
    "        # add reduced features of byte histogram distribution\n",
    "        simple_ds.update(Interpret_Histogram(simple_ds['histogram'], 'bytehistogram'))\n",
    "\n",
    "        # reduce strings field\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'strings', normalize_names=True, delete_field=True)\n",
    "\n",
    "        # flatten the strings printables distribution field\n",
    "        simple_ds = flatten_strings_printable_distribution(simple_ds, delete_field=True)\n",
    "\n",
    "        # reduce general field\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'general', normalize_names=True, delete_field=True)\n",
    "\n",
    "        # reduce header field\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'header', normalize_names=True, delete_field=True)\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'header_optional', normalize_names=False, delete_field=True)\n",
    "        simple_ds = extract_subfields_from_fields(simple_ds, 'header_coff', normalize_names=False, delete_field=True)\n",
    "\n",
    "\n",
    "        # handle data directories field\n",
    "        simple_ds = handle_data_directories_field(simple_ds)\n",
    "\n",
    "\n",
    "        # handle sections fields\n",
    "        simple_ds = handle_section_names(simple_ds, Common_section_names, delete_field=True)\n",
    "\n",
    "        # handle imports fields\n",
    "        simple_ds = handle_DLL_imports(simple_ds, delete_field=False)\n",
    "\n",
    "        # Remove the useless columns for now (they are not entirely useless but they will make the training process very complex for me :(( )\n",
    "        useless_columns = ['sha256'\n",
    "            ,'md5'\n",
    "            ,'appeared'\n",
    "            ,'avclass'\n",
    "            ,'histogram'\n",
    "            ,'byteentropy'\n",
    "            ,'imports'\n",
    "            ,'exports'\n",
    "            ,'dll_characteristics'\n",
    "            ,'characteristics']\n",
    "\n",
    "        for useless_col in useless_columns:\n",
    "            del simple_ds[useless_col]\n",
    "        \n",
    "        new_Dataset.append(simple_ds)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Finally, free the original dataset from our precious memory\n",
    "del DataSet\n",
    "\n",
    "#print(simple_ds)\n",
    "\n",
    "with open('lol.json', 'w') as f:\n",
    "     json.dump(new_Dataset[5], f, indent=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's prepare our Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= pd.DataFrame()\n",
    "# i = 0\n",
    "# for dic in new_Dataset:\n",
    "#     df = pd.concat([df, pd.DataFrame([0]*len(df.columns))], axis=0)\n",
    "#     for k in dic.keys():\n",
    "#         if k in df.columns:\n",
    "#             try:\n",
    "#                 df.loc[i, k] = dic[k]\n",
    "#             except:\n",
    "#                 print(k)\n",
    "#                 print(df)\n",
    "#         else:\n",
    "#             dummy_list = pd.DataFrame([0]*len(df) if len(df) > 0 else [0])\n",
    "#             df.insert(0, k, dummy_list)\n",
    "#             #print(df.columns)\n",
    "#             df.loc[i, k] = dic[k]\n",
    "    \n",
    "#     #print(df.head())\n",
    "#     i+=1\n",
    "\n",
    "\n",
    "# df.fillna(0)\n",
    "# print(df)\n",
    "\n",
    "# df.to_csv('lol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "constructing a pandas dataframe...: 100%|██████████| 12001/12001 [09:28<00:00, 21.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.code_size</th>\n",
       "      <th>.code_entropy</th>\n",
       "      <th>.code_vsize</th>\n",
       "      <th>.code_props_len</th>\n",
       "      <th>.text_size</th>\n",
       "      <th>.text_entropy</th>\n",
       "      <th>.text_vsize</th>\n",
       "      <th>.text_props_len</th>\n",
       "      <th>.rdata_size</th>\n",
       "      <th>.rdata_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>Dbgcore.dll_num_funcs</th>\n",
       "      <th>ncrypt.dll_num_funcs</th>\n",
       "      <th>.UPX_size</th>\n",
       "      <th>.UPX_entropy</th>\n",
       "      <th>.UPX_vsize</th>\n",
       "      <th>.UPX_props_len</th>\n",
       "      <th>.upx_size</th>\n",
       "      <th>.upx_entropy</th>\n",
       "      <th>.upx_vsize</th>\n",
       "      <th>.upx_props_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>1.200100e+04</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>1.200100e+04</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>1.200100e+04</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "      <td>12001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>258.213482</td>\n",
       "      <td>0.062253</td>\n",
       "      <td>254.584618</td>\n",
       "      <td>0.035414</td>\n",
       "      <td>3.731573e+05</td>\n",
       "      <td>4.920905</td>\n",
       "      <td>3.912547e+05</td>\n",
       "      <td>2.649029</td>\n",
       "      <td>7.284497e+04</td>\n",
       "      <td>2.851109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>21.502208</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>21.502208</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>1.407883</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>1.706524</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3889.317044</td>\n",
       "      <td>0.584017</td>\n",
       "      <td>3868.117829</td>\n",
       "      <td>0.328881</td>\n",
       "      <td>1.491375e+06</td>\n",
       "      <td>2.762939</td>\n",
       "      <td>1.537916e+06</td>\n",
       "      <td>2.155105</td>\n",
       "      <td>5.994435e+05</td>\n",
       "      <td>2.690136</td>\n",
       "      <td>...</td>\n",
       "      <td>1.041154</td>\n",
       "      <td>0.068914</td>\n",
       "      <td>2355.547030</td>\n",
       "      <td>0.072054</td>\n",
       "      <td>2355.547030</td>\n",
       "      <td>0.036513</td>\n",
       "      <td>154.232246</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>186.948177</td>\n",
       "      <td>0.045642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.560000e+03</td>\n",
       "      <td>4.594207</td>\n",
       "      <td>2.084000e+03</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.963200e+04</td>\n",
       "      <td>6.338444</td>\n",
       "      <td>7.232000e+04</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.120000e+02</td>\n",
       "      <td>2.299104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.621440e+05</td>\n",
       "      <td>6.640903</td>\n",
       "      <td>2.750240e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.867200e+04</td>\n",
       "      <td>5.262078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>168448.000000</td>\n",
       "      <td>7.469426</td>\n",
       "      <td>168242.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.211226e+07</td>\n",
       "      <td>7.999931</td>\n",
       "      <td>6.211208e+07</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.489267e+07</td>\n",
       "      <td>7.999782</td>\n",
       "      <td>...</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>258048.000000</td>\n",
       "      <td>7.893430</td>\n",
       "      <td>258048.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16896.000000</td>\n",
       "      <td>4.561308</td>\n",
       "      <td>20480.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          .code_size  .code_entropy    .code_vsize  .code_props_len   \n",
       "count   12001.000000   12001.000000   12001.000000     12001.000000  \\\n",
       "mean      258.213482       0.062253     254.584618         0.035414   \n",
       "std      3889.317044       0.584017    3868.117829         0.328881   \n",
       "min         0.000000       0.000000       0.000000         0.000000   \n",
       "25%         0.000000       0.000000       0.000000         0.000000   \n",
       "50%         0.000000       0.000000       0.000000         0.000000   \n",
       "75%         0.000000       0.000000       0.000000         0.000000   \n",
       "max    168448.000000       7.469426  168242.000000         6.000000   \n",
       "\n",
       "         .text_size  .text_entropy   .text_vsize  .text_props_len   \n",
       "count  1.200100e+04   12001.000000  1.200100e+04     12001.000000  \\\n",
       "mean   3.731573e+05       4.920905  3.912547e+05         2.649029   \n",
       "std    1.491375e+06       2.762939  1.537916e+06         2.155105   \n",
       "min    0.000000e+00       0.000000  0.000000e+00         0.000000   \n",
       "25%    2.560000e+03       4.594207  2.084000e+03         3.000000   \n",
       "50%    6.963200e+04       6.338444  7.232000e+04         3.000000   \n",
       "75%    2.621440e+05       6.640903  2.750240e+05         3.000000   \n",
       "max    6.211226e+07       7.999931  6.211208e+07        17.000000   \n",
       "\n",
       "        .rdata_size  .rdata_entropy  ...  Dbgcore.dll_num_funcs   \n",
       "count  1.200100e+04    12001.000000  ...           12001.000000  \\\n",
       "mean   7.284497e+04        2.851109  ...               0.012832   \n",
       "std    5.994435e+05        2.690136  ...               1.041154   \n",
       "min    0.000000e+00        0.000000  ...               0.000000   \n",
       "25%    0.000000e+00        0.000000  ...               0.000000   \n",
       "50%    5.120000e+02        2.299104  ...               0.000000   \n",
       "75%    2.867200e+04        5.262078  ...               0.000000   \n",
       "max    4.489267e+07        7.999782  ...             101.000000   \n",
       "\n",
       "       ncrypt.dll_num_funcs      .UPX_size  .UPX_entropy     .UPX_vsize   \n",
       "count          12001.000000   12001.000000  12001.000000   12001.000000  \\\n",
       "mean               0.000917      21.502208      0.000658      21.502208   \n",
       "std                0.068914    2355.547030      0.072054    2355.547030   \n",
       "min                0.000000       0.000000      0.000000       0.000000   \n",
       "25%                0.000000       0.000000      0.000000       0.000000   \n",
       "50%                0.000000       0.000000      0.000000       0.000000   \n",
       "75%                0.000000       0.000000      0.000000       0.000000   \n",
       "max                7.000000  258048.000000      7.893430  258048.000000   \n",
       "\n",
       "       .UPX_props_len     .upx_size  .upx_entropy    .upx_vsize   \n",
       "count    12001.000000  12001.000000  12001.000000  12001.000000  \\\n",
       "mean         0.000333      1.407883      0.000380      1.706524   \n",
       "std          0.036513    154.232246      0.041637    186.948177   \n",
       "min          0.000000      0.000000      0.000000      0.000000   \n",
       "25%          0.000000      0.000000      0.000000      0.000000   \n",
       "50%          0.000000      0.000000      0.000000      0.000000   \n",
       "75%          0.000000      0.000000      0.000000      0.000000   \n",
       "max          4.000000  16896.000000      4.561308  20480.000000   \n",
       "\n",
       "       .upx_props_len  \n",
       "count    12001.000000  \n",
       "mean         0.000417  \n",
       "std          0.045642  \n",
       "min          0.000000  \n",
       "25%          0.000000  \n",
       "50%          0.000000  \n",
       "75%          0.000000  \n",
       "max          5.000000  \n",
       "\n",
       "[8 rows x 108 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.DataFrame().from_dict(DataSet_Dict)\n",
    "\n",
    "#print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for dictionary_obj in tqdm(new_Dataset, desc=\"constructing a pandas dataframe...\"):\n",
    "    df_row = pd.DataFrame().from_dict(dictionary_obj, orient='index').transpose()\n",
    "    df = pd.concat([df, df_row], axis=0, join='outer',ignore_index=False)\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "df.to_csv('Dataset.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:        int64\n",
      "zero_bytes_byteentropy:        int64\n",
      "full_bytes_byteentropy:        int64\n",
      "mean_of_bytes_byteentropy:        int64\n",
      "standard_dev_byteentropy:        int64\n",
      "total_bytes_byteentropy:        int64\n",
      "mean_of_first_tertile_byteentropy:        int64\n",
      "mean_of_second_tertile_byteentropy:        int64\n",
      "mean_of_third_tertile_byteentropy:        int64\n",
      "zero_bytes_bytehistogram:        int64\n",
      "full_bytes_bytehistogram:        int64\n",
      "mean_of_bytes_bytehistogram:        int64\n",
      "standard_dev_bytehistogram:        int64\n",
      "total_bytes_bytehistogram:        int64\n",
      "mean_of_first_tertile_bytehistogram:        int64\n",
      "mean_of_second_tertile_bytehistogram:        int64\n",
      "mean_of_third_tertile_bytehistogram:        int64\n",
      "strings_numstrings:        int64\n",
      "strings_avlength:        int64\n",
      "strings_printables:        int64\n",
      "strings_entropy:        int64\n",
      "strings_paths:        int64\n",
      "strings_urls:        int64\n",
      "strings_registry:        int64\n",
      "strings_MZ:        int64\n",
      "strings_printabledist_0:        int64\n",
      "strings_printabledist_1:        int64\n",
      "strings_printabledist_2:        int64\n",
      "strings_printabledist_3:        int64\n",
      "strings_printabledist_4:        int64\n",
      "strings_printabledist_5:        int64\n",
      "strings_printabledist_6:        int64\n",
      "strings_printabledist_7:        int64\n",
      "strings_printabledist_8:        int64\n",
      "strings_printabledist_9:        int64\n",
      "strings_printabledist_10:        int64\n",
      "strings_printabledist_11:        int64\n",
      "strings_printabledist_12:        int64\n",
      "strings_printabledist_13:        int64\n",
      "strings_printabledist_14:        int64\n",
      "strings_printabledist_15:        int64\n",
      "strings_printabledist_16:        int64\n",
      "strings_printabledist_17:        int64\n",
      "strings_printabledist_18:        int64\n",
      "strings_printabledist_19:        int64\n",
      "strings_printabledist_20:        int64\n",
      "strings_printabledist_21:        int64\n",
      "strings_printabledist_22:        int64\n",
      "strings_printabledist_23:        int64\n",
      "strings_printabledist_24:        int64\n",
      "strings_printabledist_25:        int64\n",
      "strings_printabledist_26:        int64\n",
      "strings_printabledist_27:        int64\n",
      "strings_printabledist_28:        int64\n",
      "strings_printabledist_29:        int64\n",
      "strings_printabledist_30:        int64\n",
      "strings_printabledist_31:        int64\n",
      "strings_printabledist_32:        int64\n",
      "strings_printabledist_33:        int64\n",
      "strings_printabledist_34:        int64\n",
      "strings_printabledist_35:        int64\n",
      "strings_printabledist_36:        int64\n",
      "strings_printabledist_37:        int64\n",
      "strings_printabledist_38:        int64\n",
      "strings_printabledist_39:        int64\n",
      "strings_printabledist_40:        int64\n",
      "strings_printabledist_41:        int64\n",
      "strings_printabledist_42:        int64\n",
      "strings_printabledist_43:        int64\n",
      "strings_printabledist_44:        int64\n",
      "strings_printabledist_45:        int64\n",
      "strings_printabledist_46:        int64\n",
      "strings_printabledist_47:        int64\n",
      "strings_printabledist_48:        int64\n",
      "strings_printabledist_49:        int64\n",
      "strings_printabledist_50:        int64\n",
      "strings_printabledist_51:        int64\n",
      "strings_printabledist_52:        int64\n",
      "strings_printabledist_53:        int64\n",
      "strings_printabledist_54:        int64\n",
      "strings_printabledist_55:        int64\n",
      "strings_printabledist_56:        int64\n",
      "strings_printabledist_57:        int64\n",
      "strings_printabledist_58:        int64\n",
      "strings_printabledist_59:        int64\n",
      "strings_printabledist_60:        int64\n",
      "strings_printabledist_61:        int64\n",
      "strings_printabledist_62:        int64\n",
      "strings_printabledist_63:        int64\n",
      "strings_printabledist_64:        int64\n",
      "strings_printabledist_65:        int64\n",
      "strings_printabledist_66:        int64\n",
      "strings_printabledist_67:        int64\n",
      "strings_printabledist_68:        int64\n",
      "strings_printabledist_69:        int64\n",
      "strings_printabledist_70:        int64\n",
      "strings_printabledist_71:        int64\n",
      "strings_printabledist_72:        int64\n",
      "strings_printabledist_73:        int64\n",
      "strings_printabledist_74:        int64\n",
      "strings_printabledist_75:        int64\n",
      "strings_printabledist_76:        int64\n",
      "strings_printabledist_77:        int64\n",
      "strings_printabledist_78:        int64\n",
      "strings_printabledist_79:        int64\n",
      "strings_printabledist_80:        int64\n",
      "strings_printabledist_81:        int64\n",
      "strings_printabledist_82:        int64\n",
      "strings_printabledist_83:        int64\n",
      "strings_printabledist_84:        int64\n",
      "strings_printabledist_85:        int64\n",
      "strings_printabledist_86:        int64\n",
      "strings_printabledist_87:        int64\n",
      "strings_printabledist_88:        int64\n",
      "strings_printabledist_89:        int64\n",
      "strings_printabledist_90:        int64\n",
      "strings_printabledist_91:        int64\n",
      "strings_printabledist_92:        int64\n",
      "strings_printabledist_93:        int64\n",
      "strings_printabledist_94:        int64\n",
      "strings_printabledist_95:        int64\n",
      "general_size:        int64\n",
      "general_vsize:        int64\n",
      "general_has_debug:        int64\n",
      "general_exports:        int64\n",
      "general_imports:        int64\n",
      "general_has_relocations:        int64\n",
      "general_has_resources:        int64\n",
      "general_has_signature:        int64\n",
      "general_has_tls:        int64\n",
      "general_symbols:        int64\n",
      "subsystem:        object\n",
      "magic:        object\n",
      "major_image_version:        int64\n",
      "minor_image_version:        int64\n",
      "major_linker_version:        int64\n",
      "minor_linker_version:        int64\n",
      "major_operating_system_version:        int64\n",
      "minor_operating_system_version:        int64\n",
      "major_subsystem_version:        int64\n",
      "minor_subsystem_version:        int64\n",
      "sizeof_code:        int64\n",
      "sizeof_headers:        int64\n",
      "sizeof_heap_commit:        int64\n",
      "timestamp:        int64\n",
      "machine:        object\n",
      "EXPORT_TABLE_size:        int64\n",
      "EXPORT_TABLE_virtual_address:        int64\n",
      "IMPORT_TABLE_size:        int64\n",
      "IMPORT_TABLE_virtual_address:        int64\n",
      "RESOURCE_TABLE_size:        int64\n",
      "RESOURCE_TABLE_virtual_address:        int64\n",
      "EXCEPTION_TABLE_size:        int64\n",
      "EXCEPTION_TABLE_virtual_address:        int64\n",
      "CERTIFICATE_TABLE_size:        int64\n",
      "CERTIFICATE_TABLE_virtual_address:        int64\n",
      "BASE_RELOCATION_TABLE_size:        int64\n",
      "BASE_RELOCATION_TABLE_virtual_address:        int64\n",
      "DEBUG_size:        int64\n",
      "DEBUG_virtual_address:        int64\n",
      "ARCHITECTURE_size:        int64\n",
      "ARCHITECTURE_virtual_address:        int64\n",
      "GLOBAL_PTR_size:        int64\n",
      "GLOBAL_PTR_virtual_address:        int64\n",
      "TLS_TABLE_size:        int64\n",
      "TLS_TABLE_virtual_address:        int64\n",
      "LOAD_CONFIG_TABLE_size:        int64\n",
      "LOAD_CONFIG_TABLE_virtual_address:        int64\n",
      "BOUND_IMPORT_size:        int64\n",
      "BOUND_IMPORT_virtual_address:        int64\n",
      "IAT_size:        int64\n",
      "IAT_virtual_address:        int64\n",
      "DELAY_IMPORT_DESCRIPTOR_size:        int64\n",
      "DELAY_IMPORT_DESCRIPTOR_virtual_address:        int64\n",
      "CLR_RUNTIME_HEADER_size:        int64\n",
      "CLR_RUNTIME_HEADER_virtual_address:        int64\n",
      ".code_size:        int64\n",
      ".code_entropy:        int64\n",
      ".code_vsize:        int64\n",
      ".code_props_len:        int64\n",
      ".text_size:        int64\n",
      ".text_entropy:        int64\n",
      ".text_vsize:        int64\n",
      ".text_props_len:        int64\n",
      ".rdata_size:        int64\n",
      ".rdata_entropy:        int64\n",
      ".rdata_vsize:        int64\n",
      ".rdata_props_len:        int64\n",
      ".data_size:        int64\n",
      ".data_entropy:        int64\n",
      ".data_vsize:        int64\n",
      ".data_props_len:        int64\n",
      ".rsrc_size:        int64\n",
      ".rsrc_entropy:        int64\n",
      ".rsrc_vsize:        int64\n",
      ".rsrc_props_len:        int64\n",
      "Kernel32.dll:        bool\n",
      "Kernel32.dll_num_funcs:        int64\n",
      "User32.dll:        bool\n",
      "User32.dll_num_funcs:        int64\n",
      "USER32.dll:        bool\n",
      "USER32.dll_num_funcs:        int64\n",
      "Shell32.dll:        bool\n",
      "Shell32.dll_num_funcs:        int64\n",
      "Ole32.dll:        bool\n",
      "Ole32.dll_num_funcs:        int64\n",
      "number_of_DLLs:        int64\n",
      "total_num_of_imported_funcs:        int64\n",
      "GDI32.dll:        bool\n",
      "GDI32.dll_num_funcs:        int64\n",
      ".reloc_size:        int64\n",
      ".reloc_entropy:        int64\n",
      ".reloc_vsize:        int64\n",
      ".reloc_props_len:        int64\n",
      "Winmm.dll:        bool\n",
      "Winmm.dll_num_funcs:        int64\n",
      "Wininet.dll:        bool\n",
      "Wininet.dll_num_funcs:        int64\n",
      "Advapi32.dll:        bool\n",
      "Advapi32.dll_num_funcs:        int64\n",
      "UNKNOWN_SECTION_size:        int64\n",
      "UNKNOWN_SECTION_entropy:        int64\n",
      "UNKNOWN_SECTION_vsize:        int64\n",
      "UNKNOWN_SECTION_props_len:        int64\n",
      ".tls_size:        int64\n",
      ".tls_entropy:        int64\n",
      ".tls_vsize:        int64\n",
      ".tls_props_len:        int64\n",
      ".idat_size:        int64\n",
      ".idat_entropy:        int64\n",
      ".idat_vsize:        int64\n",
      ".idat_props_len:        int64\n",
      "OLEAUT32.dll:        bool\n",
      "OLEAUT32.dll_num_funcs:        int64\n",
      "Crypt32.dll:        bool\n",
      "Crypt32.dll_num_funcs:        int64\n",
      "Iphlpapi.dll:        bool\n",
      "Iphlpapi.dll_num_funcs:        int64\n",
      "Rpcrt4.dll:        bool\n",
      "Rpcrt4.dll_num_funcs:        int64\n",
      "Ws2_32.dll:        bool\n",
      "Ws2_32.dll_num_funcs:        int64\n",
      ".idata_size:        int64\n",
      ".idata_entropy:        int64\n",
      ".idata_vsize:        int64\n",
      ".idata_props_len:        int64\n",
      ".pdata_size:        int64\n",
      ".pdata_entropy:        int64\n",
      ".pdata_vsize:        int64\n",
      ".pdata_props_len:        int64\n",
      ".xdata_size:        int64\n",
      ".xdata_entropy:        int64\n",
      ".xdata_vsize:        int64\n",
      ".xdata_props_len:        int64\n",
      ".bss_size:        int64\n",
      ".bss_entropy:        int64\n",
      ".bss_vsize:        int64\n",
      ".bss_props_len:        int64\n",
      ".edata_size:        int64\n",
      ".edata_entropy:        int64\n",
      ".edata_vsize:        int64\n",
      ".edata_props_len:        int64\n",
      ".CRT_size:        int64\n",
      ".CRT_entropy:        int64\n",
      ".CRT_vsize:        int64\n",
      ".CRT_props_len:        int64\n",
      "Msvcr.dll:        bool\n",
      "Msvcr.dll_num_funcs:        int64\n",
      "Psapi.dll:        bool\n",
      "Psapi.dll_num_funcs:        int64\n",
      "Urlmon.dll:        bool\n",
      "Urlmon.dll_num_funcs:        int64\n",
      ".textbss_size:        int64\n",
      ".textbss_entropy:        int64\n",
      ".textbss_vsize:        int64\n",
      ".textbss_props_len:        int64\n",
      "Sensapi.dll:        bool\n",
      "Sensapi.dll_num_funcs:        int64\n",
      "Dbghelp.dll:        bool\n",
      "Dbghelp.dll_num_funcs:        int64\n",
      ".debug_size:        int64\n",
      ".debug_entropy:        int64\n",
      ".debug_vsize:        int64\n",
      ".debug_props_len:        int64\n",
      ".aspack_size:        int64\n",
      ".aspack_entropy:        int64\n",
      ".aspack_vsize:        int64\n",
      ".aspack_props_len:        int64\n",
      "Bcrypt.dll:        bool\n",
      "Bcrypt.dll_num_funcs:        int64\n",
      ".sxdata_size:        int64\n",
      ".sxdata_entropy:        int64\n",
      ".sxdata_vsize:        int64\n",
      ".sxdata_props_len:        int64\n",
      "Dbgcore.dll:        bool\n",
      "Dbgcore.dll_num_funcs:        int64\n",
      "ncrypt.dll:        bool\n",
      "ncrypt.dll_num_funcs:        int64\n",
      ".UPX_size:        int64\n",
      ".UPX_entropy:        int64\n",
      ".UPX_vsize:        int64\n",
      ".UPX_props_len:        int64\n",
      ".upx_size:        int64\n",
      ".upx_entropy:        int64\n",
      ".upx_vsize:        int64\n",
      ".upx_props_len:        int64\n",
      "total number of Features: 305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'suspicious_imports.txt'), 'r') as f:\n",
    "    sus_imports = f.readlines()\n",
    "sus_imports = [re.sub(r'\\n', '', i) for i in sus_imports]\n",
    "\n",
    "boolean_columns = sus_imports + []\n",
    "categorical_columns = [\"subsystem\", \"magic\", \"machine\"]\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in boolean_columns:\n",
    "        df[col] = df[col].astype(bool)\n",
    "        df[col].fillna(False)\n",
    "        continue\n",
    "\n",
    "    if col in categorical_columns:\n",
    "        df[col].replace(0, 'UNKNOWN', inplace=True)\n",
    "        continue\n",
    "    df[col].fillna(0)\n",
    "    df[col] = df[col].astype(np.int64)\n",
    "    df[col].fillna(0)\n",
    "\n",
    "for col in df.columns:\n",
    "    print(f\"{col}:        {df[col].dtype}\")\n",
    "\n",
    "\n",
    "df.to_csv('Dataset_big.csv', index=False)\n",
    "\n",
    "# Save our feature list\n",
    "feature_columns = list(df.columns)\n",
    "feature_columns.pop(feature_columns.index('label'))\n",
    "print(f\"total number of Features: {len(feature_columns)}\")\n",
    "with open(os.path.join(os.getcwd(), 'assets', 'features.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_columns, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the -1 tuples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_train = df.copy()\n",
    "array_of_Label_Encoders = []\n",
    "for col in categorical_columns:\n",
    "    new_LE = LabelEncoder().fit(df_train[col])\n",
    "    df_train[col] = new_LE.transform(df_train[col])\n",
    "    array_of_Label_Encoders.append(new_LE)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'enc.pkl'), 'wb') as f:\n",
    "    pickle.dump(array_of_Label_Encoders, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.01      0.01      1286\n",
      "           1       0.51      1.00      0.68      1334\n",
      "\n",
      "    accuracy                           0.51      2620\n",
      "   macro avg       0.69      0.50      0.34      2620\n",
      "weighted avg       0.69      0.51      0.35      2620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "df_train_1 = df_train.copy()\n",
    "\n",
    "feature_columns = list(df_train_1.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_1[feature_columns], df_train_1['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "svm_model = SVC(kernel='poly', degree= 3, verbose=True).fit(x_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'svm.pkl'), 'wb') as f:\n",
    "    pickle.dump(svm_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      1253\n",
      "           1       0.94      0.91      0.93      1367\n",
      "\n",
      "    accuracy                           0.92      2620\n",
      "   macro avg       0.92      0.92      0.92      2620\n",
      "weighted avg       0.92      0.92      0.92      2620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_train_2 = df.copy()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_train_2[col] = LabelEncoder().fit_transform(df_train_2[col])\n",
    "\n",
    "feature_columns = list(df_train_2.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_2[feature_columns], df_train_2['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "rf_model = RandomForestClassifier().fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'models', 'rf.pkl'), 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (batch_norm1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense1): Linear(in_features=120, out_features=512, bias=True)\n",
      "  (batch_norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (batch_norm3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dense4): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_features = 120):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, 512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dense2 = nn.Linear(512, 128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dense3 = nn.Linear(128, 128)\n",
    "        self.dense4 = nn.Linear(128, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x.float())\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = self.batch_norm2(x.float())\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = self.batch_norm3(x.float())\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = torch.tanh(self.dense4(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet()\n",
    "\n",
    "# Print the network architecture\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swak\\AppData\\Local\\Temp\\ipykernel_15572\\2880260397.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(torch.from_numpy(np.asarray(y_train, dtype=bool)), dtype= torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.9294, Accuracy = 0.7154\n",
      "Epoch 2: Loss = 1.8375, Accuracy = 0.7522\n",
      "Epoch 3: Loss = 1.8035, Accuracy = 0.7969\n",
      "Epoch 4: Loss = 1.7894, Accuracy = 0.8029\n",
      "Epoch 5: Loss = 1.7814, Accuracy = 0.8163\n",
      "Epoch 6: Loss = 1.7752, Accuracy = 0.8262\n",
      "Epoch 7: Loss = 1.7764, Accuracy = 0.8291\n",
      "Epoch 8: Loss = 1.7736, Accuracy = 0.8290\n",
      "Epoch 9: Loss = 1.7719, Accuracy = 0.8355\n",
      "Epoch 10: Loss = 1.7710, Accuracy = 0.8389\n",
      "Epoch 11: Loss = 1.7683, Accuracy = 0.8415\n",
      "Epoch 12: Loss = 1.7689, Accuracy = 0.8415\n",
      "Epoch 13: Loss = 1.7684, Accuracy = 0.8439\n",
      "Epoch 14: Loss = 1.7671, Accuracy = 0.8449\n",
      "Epoch 15: Loss = 1.7653, Accuracy = 0.8542\n",
      "Epoch 16: Loss = 1.7635, Accuracy = 0.8554\n",
      "Epoch 17: Loss = 1.7648, Accuracy = 0.8520\n",
      "Epoch 18: Loss = 1.7604, Accuracy = 0.8632\n",
      "Epoch 19: Loss = 1.7608, Accuracy = 0.8595\n",
      "Epoch 20: Loss = 1.7618, Accuracy = 0.8562\n",
      "Epoch 21: Loss = 1.7576, Accuracy = 0.8694\n",
      "Epoch 22: Loss = 1.7558, Accuracy = 0.8699\n",
      "Epoch 23: Loss = 1.7578, Accuracy = 0.8688\n",
      "Epoch 24: Loss = 1.7559, Accuracy = 0.8739\n",
      "Epoch 25: Loss = 1.7583, Accuracy = 0.8656\n",
      "Epoch 26: Loss = 1.7537, Accuracy = 0.8881\n",
      "Epoch 27: Loss = 1.7538, Accuracy = 0.9018\n",
      "Epoch 28: Loss = 1.7539, Accuracy = 0.8811\n",
      "Epoch 29: Loss = 1.7532, Accuracy = 0.8910\n",
      "Epoch 30: Loss = 1.7510, Accuracy = 0.8893\n",
      "Epoch 31: Loss = 1.7502, Accuracy = 0.8844\n",
      "Epoch 32: Loss = 1.7488, Accuracy = 0.8939\n",
      "Epoch 33: Loss = 1.7493, Accuracy = 0.8992\n",
      "Epoch 34: Loss = 1.7487, Accuracy = 0.9237\n",
      "Epoch 35: Loss = 1.7427, Accuracy = 0.9201\n",
      "Epoch 36: Loss = 1.7403, Accuracy = 0.9159\n",
      "Epoch 37: Loss = 1.7345, Accuracy = 0.9270\n",
      "Epoch 38: Loss = 1.7342, Accuracy = 0.9308\n",
      "Epoch 39: Loss = 1.7341, Accuracy = 0.9305\n",
      "Epoch 40: Loss = 1.7356, Accuracy = 0.9250\n",
      "Epoch 41: Loss = 1.7338, Accuracy = 0.9327\n",
      "Epoch 42: Loss = 1.7336, Accuracy = 0.9291\n",
      "Epoch 43: Loss = 1.7328, Accuracy = 0.9336\n",
      "Epoch 44: Loss = 1.7311, Accuracy = 0.9351\n",
      "Epoch 45: Loss = 1.7311, Accuracy = 0.9342\n",
      "Epoch 46: Loss = 1.7314, Accuracy = 0.9362\n",
      "Epoch 47: Loss = 1.7313, Accuracy = 0.9353\n",
      "Epoch 48: Loss = 1.7286, Accuracy = 0.9434\n",
      "Epoch 49: Loss = 1.7281, Accuracy = 0.9417\n",
      "Epoch 50: Loss = 1.7282, Accuracy = 0.9424\n",
      "Epoch 51: Loss = 1.7327, Accuracy = 0.9330\n",
      "Epoch 52: Loss = 1.7294, Accuracy = 0.9395\n",
      "Epoch 53: Loss = 1.7289, Accuracy = 0.9418\n",
      "Epoch 54: Loss = 1.7279, Accuracy = 0.9437\n",
      "Epoch 55: Loss = 1.7274, Accuracy = 0.9417\n",
      "Epoch 56: Loss = 1.7242, Accuracy = 0.9540\n",
      "Epoch 57: Loss = 1.7270, Accuracy = 0.9482\n",
      "Epoch 58: Loss = 1.7273, Accuracy = 0.9438\n",
      "Epoch 59: Loss = 1.7234, Accuracy = 0.9534\n",
      "Epoch 60: Loss = 1.7241, Accuracy = 0.9522\n",
      "Epoch 61: Loss = 1.7251, Accuracy = 0.9498\n",
      "Epoch 62: Loss = 1.7289, Accuracy = 0.9404\n",
      "Epoch 63: Loss = 1.7241, Accuracy = 0.9539\n",
      "Epoch 64: Loss = 1.7250, Accuracy = 0.9496\n",
      "Epoch 65: Loss = 1.7231, Accuracy = 0.9542\n",
      "Epoch 66: Loss = 1.7231, Accuracy = 0.9548\n",
      "Epoch 67: Loss = 1.7237, Accuracy = 0.9562\n",
      "Epoch 68: Loss = 1.7227, Accuracy = 0.9553\n",
      "Epoch 69: Loss = 1.7244, Accuracy = 0.9528\n",
      "Epoch 70: Loss = 1.7216, Accuracy = 0.9569\n",
      "Epoch 71: Loss = 1.7219, Accuracy = 0.9592\n",
      "Epoch 72: Loss = 1.7207, Accuracy = 0.9598\n",
      "Epoch 73: Loss = 1.7207, Accuracy = 0.9597\n",
      "Epoch 74: Loss = 1.7224, Accuracy = 0.9565\n",
      "Epoch 75: Loss = 1.7233, Accuracy = 0.9553\n",
      "Epoch 76: Loss = 1.7192, Accuracy = 0.9637\n",
      "Epoch 77: Loss = 1.7209, Accuracy = 0.9614\n",
      "Epoch 78: Loss = 1.7217, Accuracy = 0.9579\n",
      "Epoch 79: Loss = 1.7199, Accuracy = 0.9638\n",
      "Epoch 80: Loss = 1.7221, Accuracy = 0.9585\n",
      "Epoch 81: Loss = 1.7216, Accuracy = 0.9576\n",
      "Epoch 82: Loss = 1.7224, Accuracy = 0.9565\n",
      "Epoch 83: Loss = 1.7195, Accuracy = 0.9617\n",
      "Epoch 84: Loss = 1.7220, Accuracy = 0.9609\n",
      "Epoch 85: Loss = 1.7190, Accuracy = 0.9644\n",
      "Epoch 86: Loss = 1.7202, Accuracy = 0.9611\n",
      "Epoch 87: Loss = 1.7202, Accuracy = 0.9601\n",
      "Epoch 88: Loss = 1.7187, Accuracy = 0.9649\n",
      "Epoch 89: Loss = 1.7191, Accuracy = 0.9626\n",
      "Epoch 90: Loss = 1.7208, Accuracy = 0.9623\n",
      "Epoch 91: Loss = 1.7260, Accuracy = 0.9493\n",
      "Epoch 92: Loss = 1.7205, Accuracy = 0.9614\n",
      "Epoch 93: Loss = 1.7200, Accuracy = 0.9629\n",
      "Epoch 94: Loss = 1.7186, Accuracy = 0.9656\n",
      "Epoch 95: Loss = 1.7198, Accuracy = 0.9635\n",
      "Epoch 96: Loss = 1.7214, Accuracy = 0.9615\n",
      "Epoch 97: Loss = 1.7190, Accuracy = 0.9634\n",
      "Epoch 98: Loss = 1.7189, Accuracy = 0.9640\n",
      "Epoch 99: Loss = 1.7176, Accuracy = 0.9667\n",
      "Epoch 100: Loss = 1.7192, Accuracy = 0.9634\n",
      "Epoch 101: Loss = 1.7196, Accuracy = 0.9618\n",
      "Epoch 102: Loss = 1.7178, Accuracy = 0.9666\n",
      "Epoch 103: Loss = 1.7218, Accuracy = 0.9594\n",
      "Epoch 104: Loss = 1.7192, Accuracy = 0.9635\n",
      "Epoch 105: Loss = 1.7184, Accuracy = 0.9658\n",
      "Epoch 106: Loss = 1.7177, Accuracy = 0.9664\n",
      "Epoch 107: Loss = 1.7177, Accuracy = 0.9663\n",
      "Epoch 108: Loss = 1.7187, Accuracy = 0.9630\n",
      "Epoch 109: Loss = 1.7172, Accuracy = 0.9659\n",
      "Epoch 110: Loss = 1.7170, Accuracy = 0.9673\n",
      "Epoch 111: Loss = 1.7162, Accuracy = 0.9699\n",
      "Epoch 112: Loss = 1.7160, Accuracy = 0.9701\n",
      "Epoch 113: Loss = 1.7160, Accuracy = 0.9708\n",
      "Epoch 114: Loss = 1.7198, Accuracy = 0.9635\n",
      "Epoch 115: Loss = 1.7171, Accuracy = 0.9656\n",
      "Epoch 116: Loss = 1.7170, Accuracy = 0.9678\n",
      "Epoch 117: Loss = 1.7217, Accuracy = 0.9582\n",
      "Epoch 118: Loss = 1.7180, Accuracy = 0.9676\n",
      "Epoch 119: Loss = 1.7161, Accuracy = 0.9695\n",
      "Epoch 120: Loss = 1.7160, Accuracy = 0.9714\n",
      "Epoch 121: Loss = 1.7163, Accuracy = 0.9704\n",
      "Epoch 122: Loss = 1.7167, Accuracy = 0.9689\n",
      "Epoch 123: Loss = 1.7171, Accuracy = 0.9685\n",
      "Epoch 124: Loss = 1.7185, Accuracy = 0.9640\n",
      "Epoch 125: Loss = 1.7168, Accuracy = 0.9704\n",
      "Epoch 126: Loss = 1.7181, Accuracy = 0.9655\n",
      "Epoch 127: Loss = 1.7177, Accuracy = 0.9663\n",
      "Epoch 128: Loss = 1.7161, Accuracy = 0.9692\n",
      "Epoch 129: Loss = 1.7173, Accuracy = 0.9681\n",
      "Epoch 130: Loss = 1.7191, Accuracy = 0.9649\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Define a custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe_train, dataframe_labels):\n",
    "        self.data = dataframe_train\n",
    "        self.labels = dataframe_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "# Load the data from a pandas DataFrame\n",
    "#df = pd.read_csv('Dataset_1.csv') \n",
    "\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    df_train_3[col] = LabelEncoder().fit_transform(df_train_3[col]).astype(int)\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "if(re.findall('Unnamed', feature_columns[0], re.IGNORECASE)):\n",
    "    feature_columns.pop(0)\n",
    "    \n",
    "num_features = len(feature_columns)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3.drop(columns=['label']), df_train_3['label'])\n",
    "\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = torch.from_numpy(np.asarray(x_train, dtype=np.int64))\n",
    "#X = torch.tensor(torch.from_numpy(np.asarray(x_train, dtype=np.int64)), dtype=torch.int64)\n",
    "#y = torch.tensor(torch.from_numpy(np.asarray(y_train, dtype=bool)), dtype=torch.int64)\n",
    "y = torch.tensor(torch.from_numpy(np.asarray(y_train, dtype=bool)), dtype= torch.int64)\n",
    "\n",
    "# Create instances of the dataset and data loader\n",
    "dataset = MyDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(130):  # Replace 10 with the desired number of epochs\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0  # Counter for correct predictions\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the predictions and accuracy\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        corrects = torch.sum(preds == labels)  # Count the number of correct predictions\n",
    "        running_corrects += corrects.item()\n",
    "    \n",
    "    # Calculate the running accuracy\n",
    "    accuracy = running_corrects / len(dataset)\n",
    "    \n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'Epoch {epoch+1}: Loss = {running_loss/len(dataloader):.4f}, Accuracy = {accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), 'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91      1023\n",
      "           1       0.92      0.93      0.92      1160\n",
      "\n",
      "    accuracy                           0.92      2183\n",
      "   macro avg       0.92      0.92      0.92      2183\n",
      "weighted avg       0.92      0.92      0.92      2183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the testing function\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Get the predicted labels\n",
    "            #preds = torch.round(torch.sigmoid(outputs))\n",
    "            _, preds = torch.max(outputs, 1)  # Get the predicted labels\n",
    "            \n",
    "            # Collect the labels and predictions\n",
    "            all_labels += list(labels.numpy().reshape((-1,1)))\n",
    "            all_preds += list(preds.numpy().reshape((-1,1)))\n",
    "    \n",
    "    return np.asarray(all_labels), np.asarray(all_preds)\n",
    "\n",
    "# Create an instance of the network\n",
    "net = MyNet(num_features)\n",
    "\n",
    "# Load the trained weights\n",
    "net.load_state_dict(torch.load('trained_model.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# Create the testing dataset and data loader\n",
    "X = torch.from_numpy(np.asarray(x_test, dtype=np.int64))\n",
    "y = torch.from_numpy(np.asarray(y_test, dtype=np.int64))\n",
    "test_dataset = MyDataset(X, y)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "true_labels, predicted_labels = test_model(net, test_dataloader)\n",
    "\n",
    "# Import the necessary libraries for classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['class_0', 'class_1']  # Replace with appropriate class names\n",
    "print(classification_report(true_labels, predicted_labels, labels=np.unique(true_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'bruh\\r\\nCLASSIFICATION IS =============================================================\\r\\n[0]\\r\\n'\n",
      "b'bruh\\r\\nCLASSIFICATION IS =============================================================\\r\\n[1]\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "from subprocess import run\n",
    "Good_file = r\"D:\\win32diskimager-1.0.0-install.exe\"\n",
    "Bad_file = r\"D:\\hackSF\\filtered_dataset\\Win32_EXE\\186\"\n",
    "end_script = r\"D:\\ClassWork\\anti_virus\\Vigil-Anti\\Source\\EXEs\\run.py\"\n",
    "model_path= r\"D:\\ClassWork\\anti_virus\\Vigil-Anti\\Source\\EXEs\\models\\rf.pkl\"\n",
    "\n",
    "result1 = run(['python', end_script, Good_file, model_path], capture_output=True)\n",
    "resutl2 = run(['python', end_script, Bad_file, model_path], capture_output=True)\n",
    "print(result1.stdout)\n",
    "print(resutl2.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks suck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.neural_network import MLPClassifier\\n\\nMLP_classifier = MLPClassifier(hidden_layer_sizes=[120, 120, 30], solver=\\'sgd\\', alpha=1, random_state=1)\\n\\ndf_train_3 = df.copy()\\n\\nfor col in categorical_columns:\\n    df_train_3[col] = LabelEncoder().fit_transform(df_train_3[col])\\n\\nfeature_columns = list(df_train_3.columns)\\nfeature_columns.pop(feature_columns.index(\"label\"))\\n\\nx_train, x_test, y_train, y_test = train_test_split(df_train_3[feature_columns], df_train_3[\\'label\\'], test_size=0.3, shuffle=True)\\n\\nfor i in range(10):\\n    MLP_classifier.fit(x_train, y_train)\\n\\ny_pred = MLP_classifier.predict(x_test)\\nprint(classification_report(y_test, y_pred, zero_division=1))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_classifier = MLPClassifier(hidden_layer_sizes=[120, 120, 30], solver='sgd', alpha=1, random_state=1)\n",
    "\n",
    "df_train_3 = df.copy()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_train_3[col] = LabelEncoder().fit_transform(df_train_3[col])\n",
    "\n",
    "feature_columns = list(df_train_3.columns)\n",
    "feature_columns.pop(feature_columns.index(\"label\"))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train_3[feature_columns], df_train_3['label'], test_size=0.3, shuffle=True)\n",
    "\n",
    "for i in range(10):\n",
    "    MLP_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = MLP_classifier.predict(x_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
